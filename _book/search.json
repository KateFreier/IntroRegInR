[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro Regression In R",
    "section": "",
    "text": "Opening Note\nThis material is a book in progress, written with Portland State University’s Stat 364 course for data science majors in mind. Please email kfreier@pdx.edu with any found errors or comments on these materials.",
    "crumbs": [
      "Opening Note"
    ]
  },
  {
    "objectID": "Chapter1.html",
    "href": "Chapter1.html",
    "title": "1  Visualize the Relationship",
    "section": "",
    "text": "1.1 Scatterplots\nEver wondered how to tell if two things are related—like hours spent studying and the number of snacks consumed? If you haven’t worked with them before, it is now time for you to meet the scatterplot. A scatterplot is a graph that displays pairs of values for two quantitative variables. Each point represents one observation, with its position determined by the values of the two variables—one on the x-axis (horizontal), the other on the y-axis (vertical).\nConsider Figure 1.1 below showing the relationship between vehicle weight and fuel efficiency for 32 cars from 1973-1974 as reported by Motor Trend magazine.\nFigure 1.1: Vehicle weight and fuel efficiency\nThree points have been highlighted with large solid plotting characters. The three points correspond to the data describing the Cadillac Fleetwood, Honda Civic, and Pontiac Firebird.\nwt  mpg\nCadillac Fleetwood 5.250 10.4\nHonda Civic        1.615 30.4\nPontiac Firebird   3.845 19.2\nStarting with the Cadillac, where shoulld that car’s data be noted in the scatterplot of Figure 1.1? Reading across the x-axis for weight, we want the point to be a bit past 5 at 5.25 since the axis is measured in terms of 1,000 lbs. From X=5.25 we then move upward to a Y corresponding to 10.4 mpg. Since 10.4 is one of the smallest mpgs in the dataset we don’t move up much. X=5.25 and Y=10.4 intersect at the location of the solid square in the plot. The solid square marks the Cadillac Fleetwood. The solid circle belongs to the Honda Civic, and the Pontiac Firebird is marked by the solid triangle.\nBy showing every observed combination of your two quantitative variables, a scatter plot provides a quick view of the spread of each variable separately and a view of how they relate to each other. Looking at Figure 1.1 you can easily see the Cadillac Fleetwood had one of the lowest fuel efficiency levels and that the most efficient cars from the data achieved approximately 35 mpg. The figure also shows you that the weight of vehicles ranges from about 1,500 lbs up to about 5,500 lbs and that there seem to be a lot of cars right around 3,500 lbs, but hardly any in the 4,000 to 5,000 lb range.\nThe general downward slope of points going from the upper left corner down to the lower right corner shows the relationship between vehicle weight and fuel efficiency is exactly as we’d expect: heavier cars don’t get as many miles on a gallon of gas as lighter cars. This direction of the relationship, assuming weight influences fuel efficiency, is why weight was plotted on the x-axis as the independent variable and miles per gallon was plotted on the y-axis as the dependent variable.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualize the Relationship</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#scatterplots",
    "href": "Chapter1.html#scatterplots",
    "title": "1  Visualize the Relationship",
    "section": "",
    "text": "X-axis: Usually the independent variable (the one you think is doing the influencing).\nY-axis: Usually the dependent variable (the one you think is being influenced).\n\n\n\n\n\n\n\n\n\nIn R\nThe scatterplot shown in Figure 1.1 was completed using simple base R graphics and the mtcars data contained in the datasets library and included in the default install of R. Try the code for yourself:\n\nplot(mtcars$wt, mtcars$mpg, \n     xlab=\"Weight (1000 lbs)\", ylab=\"Mile Per Gallon\", \n     main=\"Vehicle Weight and Fuel Efficiency\")\n\npoints(mtcars$wt[c(15,19,25)], mtcars$mpg[c(15,19,25)], \n       pch=c(15,16,17), cex=1.5)\n\nidentify(mtcars$wt, mtcars$mpg, rownames(mtcars))\n\nThat last line of code, the identify function, allows you to generate labels for points in your scatterplot by clicking on points of interest. When you are done labeling points, hit the stop button or the esc key on your keyboard.\n\n\nOn Your Own\n\nConsider the cars dataset built into R via the datasets library. This dataframe does not provide information about different models of car like the mtcars dataset seen in Figure 1, instead it shows 50 measurements from an experiment that wanted to explore the stopping distance of a car traveling at various speeds. There are two columns in the data frame: speed, measured in miles per hour, and stopping distance, measured in feet.\n\nWhich one should be the X in your scatterplot and which should be the Y? Why?\nCreate your scatterplot. Be sure to include relevant axis labels.\nWhat can you learn from your scatterplot about the speeds used in the experiment?\nWhat can you learn from your scatterplot about the stopping distances that were observed in the experiment?\nWhat relationship between stopping distance and speed do you see in the scatterplot? Explain.\nBased on your scatterplot, would a stopping distance of 60 ft be unusual for a car traveling at 10 mph? What about for a car traveling 20 mph? Explain.\n\nUse the help menu to learn about the USArrests data frame in R.\n\n?USArrests\n\n\nCreate a scatterplot with the rate of arrest for assault as the independent variable and the rate of arrest for murder as the dependent variable.\nUsing your scatterplot, what is your estimate for the highest murder arrest rate in the US? Does it pair with the highest arrest rate for assault?\nFrom your scatterplot, what is more common: a murder arrest rate higher than 10 per 100,000 or lower than 10 per 100,000?\nWhat would you guess is the arrest rate for murder in a state with an assault arrest rate of 100 per 100,000? Explain.\nWhat does your scatterplot indicate is the relationship between a state’s arrest rates for assault and murder?\nUsing the identify function, what state corresponds to the point at X of about 240 and Y of about 6? What two states have the highest arrest rates for assault? Which state has the highest rate of arrest for murder?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualize the Relationship</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#advanced-scatterplotting",
    "href": "Chapter1.html#advanced-scatterplotting",
    "title": "1  Visualize the Relationship",
    "section": "1.2 Advanced Scatterplotting",
    "text": "1.2 Advanced Scatterplotting\n\n1.2.1 Jittering\nScatterplots are essential tools for visualizing relationships between two quantitative variables. However, when multiple data points share the same or very similar values, they can overlap on the plot, making it difficult to assess true distributions and where the density of the data lies. This phenomenon is known as overplotting.\nJittering is a technique used to address overplotting by adding a small amount of random noise to the position of each point. This noise is usually applied to one or both axes, causing points that would otherwise overlap to appear side by side.\nBenefits of Jittering Include\n\nRevealing Hidden Data Density\nWithout jittering, overlapping points may appear as a single point, obscuring the actual number of observations at that location. Jittering spreads these points apart, making clusters and concentrations more visible.\nImproving Data Interpretation\nBy making individual points distinguishable, jittering allows viewers to better estimate the frequency of specific values and identify patterns or outliers that would otherwise be hidden.\nEnhance Visual Clarity\nJittering reduces visual clutter caused by overplotting, resulting in a clearer and more informative plot. This is especially useful for categorical or discrete variables, where many data points may share identical values.\n\nThough jittering includes adding noise to data, the amount of jitter is typically small relative to the scale of the data being jittered to ensure that the overall structure and trends of the data remain intact. This preserves the interpretability of the scatterplot while providing a more accurate representation of the data distributions.\nJittering is particularly helpful when one or both axes represent categorical or discrete variables, as these often lead to many overlapping points. For example, consider Figure 1.2 showing data from the same mtcars library seen in Figure 1.1. On the left you see the number of engine cylinders in each car plotted against the vehicle weight. As you might have suspected, the heavier cars tend to have more cylinders. Since cylinder count is discrete, the scatterplot of the raw data has a vertical stripe look to it as all values are equal to four, six, or eight. By adding small random noise to the cylinder count for the graph on the right of Figure 1.2, it becomes easier to see each vehicle in the plot. Particularly in the case of eight cylinders, several points in the 3,200 to 4,000 lb range are now visible where before the points were stacked on top of each other.\n\n\n\n\n\n\n\n\nFigure 1.2: Vehicle Engine Size and Weight\n\n\n\n\n\n\n\n1.2.2 Adding a Third Variable\nAs we’ve seen, scatterplots are great at showing two variables - but what about three? Three dimensional scatterplots are possible with today’s technology and R will happily generate them. Below you’ll find a screenshot of a 3D scatterplot showing the weight, mpg, and cylinder count from cars in the mtcars data frame. If you run the code below for yourself, you’ll generate this plot in a form that allows you to click and drag the plotting box to rotate into different viewing angles.\n\n\n\n\n\n\nFigure 1.3: 3D Scatterplot Example\n\n\n\n\nlibrary(rgl)\nplot3d(mtcars$wt, mtcars$mpg, mtcars$cyl)\n\nThis 3D scatterplot functionality is certainly fun to play with, and it can be useful as you explore data relationships, but it has clear limitations. When working with stationary forms of communication like printed paper, a rotating scatterplot just doesn’t work. Putting an animated 3D scatterplot into presentation slides can work, but it often just creates a distraction.\nIncorporating information about your third variable into your plotting character is often a better choice especially if one of your variables is discrete or categorical. Figure 1.4 illustrates how color of plotting character can be used for a third variable that is either discrete or continuous. On the left of Figure 1.4 you see Figure 1.1 redone with color coded dots indicating the cylinder count of the engine. On the right you see color coding indicating the horsepower of the engine.\nA few things to keep in mind when using color:\n\nA non-trivial segment of the population has some form of color blindness. The inability to clearly distinguish between red and green is the most common form of color handicap so avoid creating plots that rely on distinguishing red dots from green ones.\nIf your work is going to be printed in black and white, you’ll want to use colors that will still be distinguishable in grey scale. Using color combined with plotting character changes is a good strategy for keeping your work clear when printed without color.\ncolorbrewer2.org is a good source for information on simple color schemes that are color-blind friendly and grey scale safe.\nIf using a continuous color scale, consult resources on Viridis (the scale used on the right side of Figure 1.4) or other color scales designed with color blindness in mind. R provides good information to consider here: https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Discrete Color\n\n\n\n\n\n\n\n\n\n\n\n(b) Continuous Color\n\n\n\n\n\n\n\nFigure 1.4: Color for third variable\n\n\n\nPlotting character differences don’t usually pop to our eyes quite as clearly as color, but sometimes they’re the better choice. Consider the case of a plot that has points for both male and female patients - using “M” and “F” as your plotting characters quickly distinguishes between the two groups and does so in a way that makes a legend less essential for understanding. Our brains are well trained to spot letter and numeric characters so you can use that to your advantage - think of “Y” and “N” for yes/no situations, “G” and “B” for good/bad, or even “1”, “2”, “3”, “4” if you have groups that are readily identified by a number like 1st, 2nd, 3rd, or 4th year of school. Figure 1.5 shows you vehicle weight and fuel efficiency with plotting character indicating if the vehicle transmission is manual (M) or automatic (A). Avoid letter combos like “O” and “C” for open/closed though - some letters can look the same too easily especially if partially obscured by other points.\n\n\n\n\n\n\n\n\n\n\n\n(a) By Cylinder Count\n\n\n\n\n\n\n\n\n\n\n\n(b) By Transmission\n\n\n\n\n\n\n\nFigure 1.5: Plotting character options\n\n\n\n\n\n1.2.3 Scatterplot Matrix\nWhen your data has more than two columns and you want to get a quick idea of all possible two-variable relationship, a scatterplot matrix is your answer. A scatterplot matrix (sometimes called a “pairs plot”) is a grid of scatterplots that visualizes relationships between each pair of numerical variables in a dataset. Each cell in the matrix displays a scatterplot comparing a pair of variables, with one variable plotted along the x-axis and the other along the y-axis.\nFigure 1.6 shows a scatterplot matrix of five quantitative variables from the mtcars data frame: MPG, engine displacement, horsepower, rear axle ratio, and weight. Because of the way it is formatted, every pair of variables is plotted twice. This allows you to see engine displacement on the X-axis with MPG on the Y-axis in the top row, second position and the reverse in the first position of the second row.\n\n\n\n\n\n\n\n\nFigure 1.6: Scatterplot Matrix Example\n\n\n\n\n\nA scatterplot matrix allows for a comprehensive, at-a-glance view of how all variables in your data frame relate to one another, making it a valuable tool in exploratory data analysis. You can quickly see from Figure 1.6 which pairs are most strongly related to each other, which have positive upward-sloping trends, and which are downward-sloping.\n\n\nIn R\nTo jitter a variable for your plot, you can use the jitter function. The jitter function can work with simply the variable you wish to jitter as the only input, or can take additional arguments to specify the amount of jitter you want to apply. Consider each of the jittered graphs below in Figure 1.7 showing a the number of cylinders in a car’s engine plotted against the horsepower of the vehicle. The graphs were created using the shown code.\nplot(jitter(mtcars$cyl), mtcars$hp)\nplot(jitter(mtcars$cyl, .5), mtcars$hp)\nplot(jitter(mtcars$cyl, 1.5), mtcars$hp)\n\n\n\n\n\n\n\n\n\n\n\n(a) Default jitter\n\n\n\n\n\n\n\n\n\n\n\n(b) Half Jitter\n\n\n\n\n\n\n\n\n\n\n\n(c) 1.5x Jitter\n\n\n\n\n\n\n\nFigure 1.7: Three jitter sizes\n\n\n\nAdding color or changing your plotting characters can be done inside your plot function call using the col and pch inputs. Specifying col is for color, and pch is for plotting character. As you can see below, sometimes you might get a little creative with arithmetic to pull out the numbered plotting characters you’re after. The numbers one through ten can pull out colors for you as well, or you can specify color names as done here, or even hex code color references.\npar(mar=c(4,4,1,1))\nplot(mtcars$wt, mtcars$hp, col=c(\"steelblue\", \"tomato\")[mtcars$am+1], pch=16)\nlegend(\"bottomright\", c(\"Automatic\", \"Manual\"), \n       col=c(\"steelblue\", \"tomato\"), pch=16)\n\nplot(mtcars$wt, mtcars$hp, pch=(mtcars$am+5)+mtcars$am*9)\nlegend(\"bottomright\", c(\"Automatic\", \"Manual\"), pch=c(5, 15))\n\n\n\n\n\n\n\n\n\n\n\n(a) Transmission type by color\n\n\n\n\n\n\n\n\n\n\n\n(b) Transmission type by plotting character\n\n\n\n\n\n\n\nFigure 1.8: Transmission\n\n\n\nThe continuous color approach is best done going beyond the base R plot command. Options include using the plt function in the tinyplot library or switching to ggplot graphics and the ggplot2 library. Both of these options are shown in Figure 1.9.\nlibrary(tinyplot)\nplt(mtcars$wt, mtcars$mpg, by=mtcars$hp, \n    xlab=\"Weight (1,000 lbs)\", ylab=\"MPG\", pch=16)\n\nlibrary(ggplot2)\nggplot(data=mtcars, aes(x=wt, y=mpg, color=hp))+\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n(a) tinyplot\n\n\n\n\n\n\n\n\n\n\n\n(b) ggplot\n\n\n\n\n\n\n\nFigure 1.9: Two R plotting packages options\n\n\n\nCreating a scatterplot matrix is very straightforward provided your data is in a data frame. Using the plot command on a data frame will automatically create a scatterplot matrix. If you want only a subset of the columns, specify the columns you want in square brackets.\n\nplot(mtcars[,c(1,4,6)])\n\n\n\n\n\n\n\n\n\n\nOn Your Own\n\nConsider the trees dataset built into R via the datasets library. This dataframe gives the girth, height, and volume of Black Cherry trees.\n\nCreate a scatter plot matrix of the tree data. Based on your plot, if you want to estimate a tree’s volume, what is more helpful to know: the tree’s girth or the tree’s height? Explain your choice.\nCreate a scatter plot of tree height and volume. Use plotting character 16, a solid circle for trees with a girth greater than 13 and an empty triangle for trees with a girth less than 13. Include a legend. What do you see of interest in this plot?\n\nConsider the swiss dataset built into R via the datasets library. Use the help menu to learn about this data covering fertility and socioeconomic indicators over 47 provinces in 1888.\n\nCreate a scatterplot matrix of all columns in the the swiss data frame. Which included socioeconomic factors, if any, are positively related to fertility rate?\nUsing your scatterplot matrix from part a, which socioeconomic factors, if any, are negatively related to fertility rate?\nCreate a scatterplot with the percent of males in agricultural occupations on the X-axis and fertility rate on the Y-axis. Use plotting character 16, the solid circle. For provinces with a population that is less than 50% Catholic, make the circles orchid. For provinces with a population that is over 50% Catholic, make the circles royalblue3. Do you see any interesting relationships in your graph? Explain.\nCreate a scatterplot with the percent of males in agricultural occupations on the X-axis and the rate of education beyond primary school on the Y-axis. Use plotting character 16, the solid circle and color it using a continuous color scale for percent Catholic. Do you see any interesting relationships in your graph? Explain.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualize the Relationship</span>"
    ]
  },
  {
    "objectID": "Chapter2.html",
    "href": "Chapter2.html",
    "title": "2  Add a Line",
    "section": "",
    "text": "2.1 Describing the Relationship with a Line\nWith a scatterplot in front of you and a linear trend evident the obvious next step is to draw a line on your plot. We begin with a simple method for fitting a line to a scatterplot that is easy to compute in four steps:\nThat’s it. You’re cutting the data in half from left to right along the X-axis, then finding the median X and Y on each side to create two points, then connecting the points with a line. What does that look like? Figure 2.1 below walks through the process as applied to the cars speed and stopping distance data.\nFirst, the data is split into a left and right half based on the median vehicle speed, shown with the vertical grey dashed line. In the second plot, the \\(25^{th}\\) and \\(75^{th}\\) percentiles of speed are marked with green vertical lines and the median stopping distances for the two halves are marked with horizontal blue lines. Orchid dots mark the intersection of the medians. These two key dots are then joined with a line for the third graph on the right.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Add a Line</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#describing-the-relationship-with-a-line",
    "href": "Chapter2.html#describing-the-relationship-with-a-line",
    "title": "2  Add a Line",
    "section": "",
    "text": "Calculate the median, 1st quartile, and 3rd quartile, of your X variable.\nCalculate the median Y value of points with a corresponding X value less than the median you found in step 1.\nCalculate the median Y value of points with a corresponding X value greater than the median you found in step 1.\nAdd the points corresponding to the 1st quartile of X paired with the median calculated in step 2 and the 3rd quartile of X paired with the median calculated in step 3 to your scatterplot. Draw a line connecting those two points.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Find median X, divide\n\n\n\n\n\n\n\n\n\n\n\n(b) Median X & Y each side\n\n\n\n\n\n\n\n\n\n\n\n(c) Connect two points\n\n\n\n\n\n\n\nFigure 2.1: Median-Median Line Steps\n\n\n\n\n\nIn R\nA big part of the beauty of this method for drawing a line on a scatterplot is that it is simple. The ideas involved are nothing beyond quartiles and the algebra of connecting two points with a line. To find quartiles in R you’ll use the quantile function and base R arithmetic will get your through the rest of it.\nHere’s the code to fit the line to the cars data frame shown in Figure 2.1.\n\n# make initial scatterplot to begin\nplot(cars$speed, cars$dist, xlab=\"Speed\", ylab=\"Stopping Distance\")\n\n# step 1: calculate the median, 1st quartile and 3rd quartile of X, weight\nquartx&lt;-quantile(cars$speed, c(.5, .25, .75))\n\n# step 2: calculate the median Y of points to the left of the median X\nmed_y_left&lt;-median(cars$dist[cars$speed&lt;=quartx[1]])\n\n# step 3: calculate the median Y of points to the right of the median X\nmed_y_right&lt;-median(cars$dist[cars$speed&gt;=quartx[1]])\n\n# step 4: Add points and line connecting them\npoints(quartx[2:3], c(med_y_left, med_y_right), pch=16, col=2)\n\nslope&lt;-(med_y_right-med_y_left)/(quartx[3]-quartx[2])\nintercept&lt;-med_y_left-(slope*quartx[2])\nabline(intercept, slope, col=\"orchid3\", lwd=3)\n\n\n\nOn Your Own\n\nCreate a scatterplot of vehicle weight and horsepower from the mtcars data frame.\n\nOverlay your plot with a median-median line in red.\nInterpret the median-median line in context.\n\nExplore the variability in the median-median line approach through the following steps\n\nWrite a function that takes in an X and a Y, then generates the slope and intercept for the median-median line.\nUse your function on the cars data frame to identify the median-median line describing the relationship between vehicle speed and stopping distance.\nSample the rows of the cars data frame with replacement to create a new data frame with speed and stopping distance that is the same size as the original. Square brackets and sample(1:nrows(cars), replace=TRUE) will help you out with this. Once you have your new data frame, apply your median-median line function. How did the intercept and slope change?\nRepeat the steps of part c 200 times in a loop, saving the new intercept and slope each time. Then generate histograms of your 200 intercepts and 200 slopes. How variable is the median-median line from one data sample to the next?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Add a Line</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#sec-resid",
    "href": "Chapter2.html#sec-resid",
    "title": "2  Add a Line",
    "section": "2.2 Residuals",
    "text": "2.2 Residuals\nLet’s consider a relatively small data set. The bac data from the openintro (Çetinkaya-Rundel et al. 2024) R library gives data from 16 student volunteers at Ohio State University who drank beer and then had their blood alcohol content measured.\nFigure 2.2 shows the 16 data points from this study (one from each student) with the beer consumption level jittered for clarity. The red line is the result when using the Tukey median-median line method to describe the relationship between blood alcohol content (BAC) and beer consumption.\nObviously this red fit line does not exactly pass through every point. From the scatterplot it is clear that while BAC and beer consumption amounts are related, the relationship is not perfect and no straight line could perfectly pass through all 16 points. You’ll also see that each point has a thin line drawn vertically connecting it to the red line. The lengths of these lines are considered the error of the red line fit. These errors have a special name: residuals.\n\n\n\n\n\n\n\n\nFigure 2.2: Residual diagram\n\n\n\n\n\nA residual is the difference between the actual observed value of the dependent variable (\\(y\\)) and the value predicted by the linear fit (\\(\\hat{y}\\)).\nMathematically, it is expressed as: \\(Residual=y-\\hat{y}\\) or specific to the \\(i^{th}\\) observation, \\(r_i=y_i-\\hat{y}_i\\), where \\(y_i\\) is the observed value and \\(\\hat{y}_i\\) is the value predicted from the line.\nKey points about residuals:\n\nA positive residual means the observed value is above the predicted value (the model underestimated).\nA negative residual means the observed value is below the predicted value (the model overestimated).\nThe residuals represent the vertical distances between the data points and the regression line on a scatterplot. The errors are parallel to the y-axis, not projections meeting the linear fit at a right-angle.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Add a Line</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#sec-lsr",
    "href": "Chapter2.html#sec-lsr",
    "title": "2  Add a Line",
    "section": "2.3 Least-Squares Regression Line",
    "text": "2.3 Least-Squares Regression Line\nWith the idea of a residual established, we can move beyond the simple median-median line and define a new line of fit: the least-squares regression line. With this approach our fit line is defined as the line that minimizes the sum of the squared residuals.\nWhy the sum of squared residuals and not simply the sum of residuals? Recall that residuals can be both positive and negative. Errors from points above the line and errors from points below the line would cancel each other out in a simple sum of residuals.\nWhy not the sum of the absolute value of residuals? Well… calculus. Think back to your calculus classes. What was more straight-forward: finding the minimum of a smooth continuous polynomial function or finding the minimum of a function with a kink in it caused by an absolute value?\nYou can also see the squaring as a nice feature as it puts an added penalty whenever your line is far off. Big residuals squared become even bigger errors in the sum you’re minimizing.\nSome notation for all this is needed now. Our goal is to model the relationship between a single independent variable \\(x\\) and a dependent variable \\(y\\) with a line in the form:\n\\[y=β_0+β_1x+ε\\]\nWhere\n\n\\(y=\\) Dependent (response) variable\n\\(x=\\) Independent (predictor) variable\n\\(β_0=\\) Intercept (value of \\(y\\) when \\(x=0\\))\n\\(β_1=\\) Slope (change in \\(y\\) for a one-unit change in \\(x\\))\n\\(ε=\\) Error term that captures deviations from the line (residuals)\n\nMinimizing the sum of squared errors means minimizing:\n\\[\n\\sum (y_i-(β_0+β_1x_i))^2\n\\]\n\\[\n=\\sum (y_i-β_0-β_1x_i)^2\n\\]\nBy differentiating with respect to \\(β_0\\) and then separately differentiating with respect to \\(β_1\\) , setting each to 0 and solving for the minimums you can find that the least-squares estimate for \\(β_0\\) and \\(β_1\\) are:\n\\[\nβ_1=\\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2}\n\\]\n\\[β_0=\\bar{y}-β_1\\bar{x}\\]\nwhere \\(\\bar{x}\\) and \\(\\bar{y}\\) are the sample means of \\(x\\) and \\(y\\), respectively.\nFigure 2.3 shows the least-squares regression line added to the plot from Figure 2.2. The red line is the Median-Median fit of the data, the dashed blue line shows the least-squares regression fit.\n\n\n\n\n\n\n\n\nFigure 2.3: Median-Median and Least-Squares Lines\n\n\n\n\n\nNow a question: If you swap your dependent and independent variables and fit a line to X~Y instead, will you get the same linear equation just solved for X instead of solved for Y? Nope. When BAC is fit as a function of beer consumption you get that \\(BAC=-0.0127+0.0180\\times beers\\). If instead you fit beers as a function of BAC, then, \\(beers=1.5288+44.5252\\times BAC\\) which when solved for BAC comes to \\(BAC=-0.0343-0.0225\\times temperature\\). Both of these are shown on Figure 2.4 to show how different the results are:\n\n\n\n\n\n\n\n\nFigure 2.4\n\n\n\n\n\nWhy are they so different? It is all because of how residuals are defined. Remember our least-squares regression fit is designed to minimizes the sum of squared residuals and residuals are defined as the error measured parallel to the y-axis. When you change which term is y, you change the error measure that matters.\nIt is worth noting that every regression line will pass through the point \\((\\bar{x}, \\bar{y})\\) and because of that, the point at which the two lines intersect is, and always will be, \\((\\bar{x}, \\bar{y})\\). With this data, that means \\((4.812, 0.07375)\\).\n\nIn R\nLeast-squares linear regression is a foundational technique in statistics and data science. Some consider it the first step you’ll take into the world of machine learning. As such, it is built into base R and those formulas above are not ones you’ll need to memorize. R will do all the heavy lifting. Here’s the code to fit the blue regression line in Figure 2.3 above and pull out the slope and intercept.\n\nblue_model&lt;-lm(bac~beers, data=bac)\nblue_model$coef\n\n(Intercept)       beers \n-0.01270060  0.01796376 \n\n\nYou can even draw the regression line on your scatterplot using the model object directly with the abline function. Here’s the code to create the plot and insert the line in the color blue with a line width of three:\n\nplot(bac$beers, bac$bac)\nabline(blue_model, col=4, lwd=3)\n\n\n\nOn Your Own\n\nCreate a scatterplot of vehicle weight and horsepower from the mtcars data frame.\n\nOverlay your plot with a least-squares regression line in red.\nWhat is the equation of your fitted line?\n\nCreate a scatterplot of vehicle speed and stopping distance from the cars data frame.\n\nOverlay your plot with a least-squares regression line in the color of your choice.\nWhat is the equation of your fitted line?\n\nFit a least-squares model predicting a newborn’s father’s age as a function of the babies’ mother’s age using the births14 data frame in the openintro library.\n\nWhat is the equation of your fitted line?\nMake a scatterplot of the data with your fitted line superimposed on top.\nNow fit the model predicting the mother’s age as a function of father’s age. What is the equation of the fitted line?\nAdjust your new model by solving for father’s age and add the new model to your scatterplot.\nConfirm the point of intersection is at \\((\\bar{x}, \\bar{y})\\).\n\nExplore the variability in the least-squares regression line approach through the following steps a. Identify the slope and intercept of the fitted regression line predicting car stopping distance as a function of speed from the cars data frame. (yes, this is the same as #2 above) b. Sample the rows of the cars data frame with replacement to create a new data frame with speed and stopping distance that is the same size as the original. Square brackets and sample(1:nrows(cars), replace=TRUE) will help you out with this. Once you have your new data frame, find the new least-squares slope and intercept. How did the line coefficients change? c. Repeat the steps of part b 200 times in a loop, saving the new intercept and slope each time. Then generate histograms of your 200 intercepts and 200 slopes. How variable is your regression line from one data sample to the next? d. Plot car speed and stopping distance in a scatterplot and overlay 10 of your fitted regression lines from part c. When you look at the 10 lines, how do they differ from eachother? Do they tend to be parallel? Is the variability in line placement the same across the range of x, or are there areas of x where the lines are more or less consistent?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Add a Line</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#sstotal-ssregression-and-r2",
    "href": "Chapter2.html#sstotal-ssregression-and-r2",
    "title": "2  Add a Line",
    "section": "2.4 SSTotal, SSRegression, and \\(R^2\\)",
    "text": "2.4 SSTotal, SSRegression, and \\(R^2\\)\nIn Section 2.2 we discussed residuals as being the measure from each data point to the fitted line. There are two other important metrics in regression to be aware of.\nBelow in Figure 2.5 is the same student alcohol data with the blue least-squares line fit earlier. The horizontal grey dashed line is at 0.07375, the mean blood alcohol level across the 16 students.\n\n\n\n\n\n\nFigure 2.5: SST, SSE, and SSR\n\n\n\nThe zoomed-in section of Figure 2.5 shows each of the three key components to be aware of in regression modeling drawn out for the point corresponding to our biggest drinker as an example.\n\nAs you know from Section 2.2, the thin black line is the residual; the difference between the fitted regression model in blue, and the actual observed point. By design, our least-squares fit in blue minimizes the sum of the squared lengths of the black residual lines for all points. The sum of the squared errors is known as SSE, SSError, or SSResidual.\nThe green dot-dashed line is the distance from the observed point to the observed mean Y. The sum of these distances squared is known as Sum of Squares Total, SSTotal, or SST. This is a measure of the total variability of Y in your data set. The idea of the sum of squared distances to the mean as a measure of variability is not new: recall standard deviation from your intro stats course. By definition, the standard deviation of Y (a sample) is:\\[\\sqrt{\\frac{\\sum(y-\\bar{y})^2}{n-1}}\\]\nwith the sum of squared distances to the mean right there in the numerator.\nThe third line shown dotted in orchid is the distance from the mean of Y to the regression line fit.\nIf beer consumption were unknown, our best guess for \\(y_1\\) would be 0.07375, the overall average that corresponds to the horizontal grey line. However, using beer consumption and our fitted model, the best guess for \\(y_1\\) is .14897, the fitted value corresponding to nine consumed beers calculated using the intercept and slope found earlier: \\(-0.0127+(0.01796*9)=0.1489\\).\nThe orchid dotted line shows this difference between the 0.1489 and 0.07375, and can be thought of as the value added by the linear model. It is graphically how much closer we are to the actual value of \\(y_i\\) because we used the linear relationship to X in our estimation. The sum of squares of these differences attributed to the regression model is known as Sum Squares Regression, SSReg, or SSR.\n\n\nIn summary:\n\\[\nSSError=SSE=\\sum(y_i-\\hat{y}_i)^2\n\\]\n\\[\nSSTotal=SST=\\sum(y_i-\\bar{y})^2\n\\]\n\\[\nSSRegression=SSR=\\sum(\\bar{y}_i-\\hat{y}_i)^2\n\\]\nClearly from the visual in Figure 2.5 these three Sum of Squares measurements are closely related. In fact\\[\nSSTotal=SSError+SSRegression\n\\]Typically this relationship is relied on when calculating SSRegression as SSTotal and SSError are easier to calculate directly.\nTo avoid potential confusion, throughout this book SSR will refer only to SSRegression and SSE will be used for the Sum of Squared Residuals. Be aware though that some sources may use SSR to refer to SSRegression or SSResiduals.\nAll of this leads us to \\(R^2\\), the coefficient of determination. \\[R^2=1-\\frac{SSError}{SSTotal}=\\frac{SSReg}{SSTotal}\\]\nPut into words, this means \\(R^2\\) is the proportion of variability in Y that has been explained by your linear regression model. Note that \\(R^2\\) must always be positive and between 0 and 1, inclusive. The \\(R^2\\) value also has nothing to do with the direction of the relationship between X and Y, only the strength of that relationship. The slope may be upward or downward sloping, \\(R^2\\) only cares how closely the points follow the line.\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(R^2=.99\\)\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(R^2=.68\\)\n\n\n\n\n\n\n\n\n\n\n\n(c) \\(R^2=.22\\)\n\n\n\n\n\n\n\nFigure 2.6: \\(R^2\\), Coefficient of Determination\n\n\n\nConsider the three plots in Figure 2.6. On the left where the points closely follow the fitted red regression line, SSE is small, meaning SST and SSR are close to equal in value and \\(R^2\\) is near 1 at 0.99. In the middle, the regression line does a fair job of describing the variability in Y so SSR is larger than SSE and the \\(R^2\\) is 0.68. The downward slope of this middle graph is irrelevant. On the far right, the residuals are larger relative to the total variability of Y, so \\(\\frac{SSE}{SST}\\) is larger than in the other graphs making \\(1-\\frac{SSE}{SST}\\) smaller and \\(R^2\\) is only 0.22.\nWhat constitutes a “good” \\(R^2\\) varies by discipline. If your data is related to engineering and is the X-Y relationship is related to a law of physics for instance, \\(R^2\\) should be very nearly 1. If however you are working in the social sciences and the variability in the X-Y relationship is about people being different from each other, then an \\(R^2\\) of 0.4 might be super exciting. Context matters when evaluating \\(R^2\\).\n\nIn R\nCode below calculates the SST and SSE from the least-squares line predicting BAC using beer consumption. SSR is calculated two ways; by subtracting SSE from SST, and directly using the mean of crawling age and model fitted values.\n\nblue_model&lt;-lm(bac~beers, data=bac)\n\n# calculate mean BAC\nmean_bac&lt;-mean(bac$bac)\n\n# calculate fitted values for BAC\nmodel_fit&lt;-blue_model$coef[1]+(blue_model$coef[2]*bac$beers)\n\n# renaming just to make later calculations clear\nbac_obs&lt;-bac$bac\n\n# calculate SST\nSSTotal&lt;-sum((bac_obs-mean_bac)^2)\nSSTotal\n\n[1] 0.029225\n\n# calculate SSE\nSSError&lt;-sum((bac_obs-model_fit)^2)\nSSError\n\n[1] 0.005849655\n\n# calculate SSR, two ways\nSSReg&lt;-SSTotal-SSError\nSSReg_long&lt;-sum((model_fit-mean_bac)^2)\n\nc(SSReg, SSReg_long)\n\n[1] 0.02337535 0.02337535\n\n\nNote the two approaches to SSR produced the same result. There was an easier way though. Just as $coef after your model name will retreive the fitted model coeffiecents, there are similarly intuitive ways to quickly pull the fitted values and the residuals:\n\nsummary(blue_model$fitted.values)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.005263 0.041191 0.077118 0.073750 0.099573 0.148973 \n\nsummary(blue_model$residuals)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.027118 -0.017350  0.001773  0.000000  0.008623  0.041027 \n\n\nAnd here are three ways to calculate \\(R^2\\), the coefficient of determination.\n\nopt1&lt;-1-(SSError/SSTotal)\n\nopt2&lt;-SSReg/SSTotal\n\nopt3&lt;-summary(blue_model)$r.square\n\nc(opt1, opt2, opt3)\n\n[1] 0.7998407 0.7998407 0.7998407\n\n\n\n\nOn Your Own\n\nFit a least-squares model predicting vehicle hwy_mpg as a function of vehicle weight using the cars04 data frame in the openintro library.\n\nWhat is the equation of your fitted line?\nMake a scatterplot of the data with your fitted line superimposed on top.\nFind SST, SSE, and SSR.\nFind \\(R^2\\).\n\nFit a least-squares model predicting total SAT score as a function of high school GPA using the satgpa data frame in the openintro library.\n\nWhat is the equation of your fitted line?\nMake a scatterplot of the data with your fitted line superimposed on top.\nFind SST, SSE, and SSR.\nFind \\(R^2\\).\n\nFit a least-squares model predicting a newborn’s father’s age as a function of the babies’ mother’s age using the births14 data frame in the openintro library.\n\nWhat is the equation of your fitted line?\nMake a scatterplot of the data with your fitted line superimposed on top.\nFind SST, SSE, and SSR.\nFind \\(R^2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Add a Line</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#using-regression-models",
    "href": "Chapter2.html#using-regression-models",
    "title": "2  Add a Line",
    "section": "2.5 Using Regression Models",
    "text": "2.5 Using Regression Models\nGreat! We have a line! Now what? The two most common uses for a fitted regression line are:\n\nUsing the slope as a descriptor of the relationship between X and Y\nUsing the fitted line to predict future observations\n\nContinuing our work with the student alcohol data, recall the fit: \\[bac=-0.0127+(0.01796\\times beers)\\]\nThe positive slope tells us that average BAC goes up as beer consumption goes up. Not surprising at all. Specifically, with a value of \\(0.01796\\), the model tells us the average BAC goes up by 0.01796 with every additional can of beer.\nUsing the full equation, we can also estimate that if someone drinks 4 beers, their blood alcohol level will likely be about 0.059 because \\(-0.0127+(0.01796\\times 4)=0.05914\\). Similarly, a student who consumes seven beers likely has a BAC of about 0.113.\nOne important caveat for using your equation for estimation: extrapolation beyond the bounds of your observed data is not a good idea. What does that mean? Well think of a student with remarkably bad judgement who drinks 12 cans of beer. Our data doesn’t actually include any students drinking more than nine beers so by using our model to estimate the BAC at 12 beers we are making the assumption that the linear trend continues beyond the scope of our plot. Maybe it does and this student has a BAC just a bit over 0.2, but maybe it doesn’t and beers 10, 11, and 12 have a different impact than beers 1, 2, and 3, on increasing BAC. Estimations near the middle of our observed X range are where we feel the most confident; estimations beyond the range of observed X should be made with caution.\n\nIn R\nYou already know that model_name$coef will get you your model coefficients. From there you can get predicted values using your line with a little algebra. There’s another way though; one that is particularly useful when you have more than one prediction to make. Here is how you can use the predict.lm function.\n\n# create a data frame with the x-values you want a prediction for, making \n#   sure the column name matches the x name used when you created your model\n\nnew_data&lt;-data.frame(beers=c(2,3,6.5))\n\n# get your predicted values\npredict.lm(blue_model, new_data)\n\n         1          2          3 \n0.02322692 0.04119068 0.10406385 \n\n\n\n\nOn Your Own\n\nFit a least-squares model predicting vehicle hwy_mpg as a function of vehicle weight using the cars04 data frame in the openintro library.\n\nWhat is the equation of your fitted line?\nHow do you expect hwy_mpg to change with a 100 lb increase in vehicle weight?\nWhat highway gas mileage would you expect for a car that weighs 3,802 lbs?\nWhat highway gas mileage would you expect for a car that weighs 1,700 lbs?\nWhich estimate, d or e, do you feel better about? Why?\n\n\nFit a least-squares model predicting total SAT score as a function of high school GPA using the satgpa data frame in the openintro library.\n\nWhat is the equation of your fitted line?\nIf Anne has a high school GPA of 3.3 and and Clark has a GPA of 3.8, how do you think Clark’s and Anne’s SAT scores compare? Explain with estimates of each SAT score using your model.\n\nFit a least-squares model predicting a newborn’s father’s age as a function of the babies’ mother’s age using the births14 data frame in the openintro library.\n\nWhat is the equation of your fitted line?\nFor each additional year of age for the mother, how much older do you anticipate the father to be?\nGrace’s Mom was 29 when she was born, how old would you guess her father was?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Add a Line</span>"
    ]
  },
  {
    "objectID": "Chapter2.html#matrix-notation",
    "href": "Chapter2.html#matrix-notation",
    "title": "2  Add a Line",
    "section": "2.6 Matrix Notation",
    "text": "2.6 Matrix Notation\nSection 2.3 expressed the form of our fitted model in the notation of a simple linear function (\\(y=β_0+β_1x+ε\\)) but that isn’t the only option. Our model could also be expressed using matrix notation as:\n\\[\nY=Xβ+ε\n\\]\nWhere\n\n\\(Y\\) is an \\(n\\times 1\\) column matrix containing your \\(n\\) observed outcomes\n\\(X\\) is an \\(n\\times 2\\) matrix with the first column full of 1s to correspond to the intercept term of the linear model and the second column containing the \\(x\\) values for your observed outcomes\n\\(β\\) is \\(2\\times 1\\) containing \\(β_0\\) and \\(β_1\\) in separate rows\n\\(ε\\) is an \\(n\\times 1\\) column matrix of the error terms\n\nSo with the student alcohol data data:\n\\[\nY=\\begin{bmatrix}\n    0.1 \\\\\n    0.03 \\\\\n0.19\\\\\n    \\vdots \\\\\n0.05\\\\\n    \\end{bmatrix}\nand \\space\nX=\\begin{bmatrix}\n    1 & 5 \\\\\n    1 & 2\\\\\n1 & 9 \\\\\n    \\vdots & \\vdots \\\\\n    1 & 4 \\\\\n\\end{bmatrix}\n\\]\nTo estimate \\(β\\), the \\(\\hatβ\\) is calculated by:\n\\[\n\\hatβ=(X^TX)^{-1}X^TY\n\\]\nWhich means your estimate for \\(β\\) is dependent of \\(X^TX\\) having an inverse. No inverse, no \\(\\hat{β}\\).\nPutting this \\(\\hatβ\\) back in our model equation, this means\n\\[\\hat{Y}=X\\hatβ\\] \\[\\hat{Y}=X(X^TX)^{-1}X^TY\\]\nWe then define \\(X(X^TX)^{-1}X^T\\) as \\(H\\), the “hat matrix” because \\(HY=\\hat{Y}\\). H “puts a hat” on Y. The hat matrix will play a role later on in chapter XXXXXXX.\n\n\nIn R\nYou won’t do it regularly, but it’s not a bad exercise to work through fitting a linear model with matrix algebra rather than the lm shortcut offered by R.\n\nY_mat&lt;-matrix(bac$bac, ncol=1)\nX_mat&lt;-matrix(c(rep(1, 16), bac$beers), ncol=2)\n\nXTX&lt;-t(X_mat)%*%X_mat\nXTX_inv&lt;-solve(XTX)\nXTX_invXT&lt;-XTX_inv%*%t(X_mat)\nbeta_hat&lt;-XTX_invXT%*%Y_mat\n\nbeta_hat\n\n            [,1]\n[1,] -0.01270060\n[2,]  0.01796376\n\nlm_mod&lt;-lm(bac~beers, data=bac)\nlm_mod$coef\n\n(Intercept)       beers \n-0.01270060  0.01796376 \n\n\nYay! They match, just as expected.\n\n\nOn Your Own\n\nFit a least-squares model predicting total SAT score as a function of high school GPA using the satgpa data frame in the openintro library.\n\nWhat is the \\((X^TX)^{-1}\\) matrix in the interim calculation of \\(\\hatβ\\)?\nConfirm the model fit using lm matches the fit using matrix algebra.\nWhat is the value in the 1,1 position of the hat matrix?\n\nNo data now, just linear algebra:\n\nWhat are the dimensions of the hat matrix?\nWhat does \\(HH\\) equal?\n\n\n\n\n\n\nÇetinkaya-Rundel, Mine, David Diez, Andrew Bray, Albert Y. Kim, Ben Baumer, Chester Ismay, Nick Paterno, and Christopher Barr. 2024. “Openintro: Datasets and Supplemental Functions from ’OpenIntro’ Textbooks and Labs.” https://doi.org/10.32614/CRAN.package.openintro.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Add a Line</span>"
    ]
  },
  {
    "objectID": "Chapter3.html",
    "href": "Chapter3.html",
    "title": "3  How sure are you?",
    "section": "",
    "text": "3.1 Inference for Model Coefficients\nEvery confidence interval you learned in intro statistics has the same basic format:\\[\npoint\\: estimate \\pm multiplier\\times standard \\: error\n\\] Similarly, the hypothesis tests you’ve learned for means and proportions have test statistics in the general format of:\n\\[\n\\frac{observed \\: value - hypothesized\\: value}{standard\\: error}\n\\]\nBoth of these basic structures will continue in the realm of regression analysis.\nYou can now fit a regression model, but in many ways that’s equivalent to being able to calculate a sample mean. The least-squares fit gives you a way to describe your sample data well, but the line alone doesn’t tell you all you need to make statements about the population your sample came from. Just as you learned how to put a confidence interval on a sample mean in intro stat, you’ll now put confidence intervals on your fitted model coefficients. Confidence intervals allow you to quantify how much uncertainty surrounds the estimates of the intercept \\((β_0)\\) and slope \\((β_1)\\) coefficients.\nConsider the cats data frame available in the MASS library for R. This data frame includes the body weight (in kg) and heart weight (in grams) for 144 cats. The least-squares fit for the first 50 cats in Figure 3.1.\nBut what if a different sample of 50 cats had been selected? How different might the \\(\\hat{β_0}\\) and \\(\\hat{β_1}\\) be? Figure 3.1 (b) shows five different regression lines fit on five different samples of size 50 from the cat data superimposed on a scatterplot of all 144 cats in the data frame.\nAs you see, each sample produces a slightly different line. The five intercepts range from -2.164 up to 1.141, and the five slopes range from 3.4268 up to 4.8005. Clearly which random sample you look at matters, and when this variability exists we need a confidence intervals to communicate this uncertainty, and hypothesis testing is needed to evaluate hypotheses.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How sure are you?</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#inference-for-model-coefficients",
    "href": "Chapter3.html#inference-for-model-coefficients",
    "title": "3  How sure are you?",
    "section": "",
    "text": "(a) Fit of first 50 cats\n\n\n\n\n\n\n\n\n\n\n\n(b) 5 fits from 5 samples of 50\n\n\n\n\n\n\n\nFigure 3.1: Cat Body and Heart Weight\n\n\n\n\n\n3.1.1 Confidence Intervals\n\nIntercept \\(β_0\\)\n\nThe point estimate for the intercept in your linear model describing the relationship between X and Y is simply your \\(\\hat{β_0}\\).\nThe multiplier used in your confidence interval will come from a t-distribution with \\(n-2\\) degrees of freedom. Why \\(n-2\\)? You started with n data points (and n degrees of freedom), and then you lose one degree of freedom for each parameter you estimate. You estimated two parameters: \\(β_0\\) and \\(β_1\\), hence \\(n-2\\).\nThe standard error for \\(\\hat{β_0}\\) is \\(SE_{\\hat{β_0}}=S_R\\sqrt{\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x-\\bar{x})^2}}\\) where \\(S_R=\\sqrt\\frac{\\sum(y_i-\\hat{y_i})^2}{n-2}\\), the standard deviation of the residuals.\n\n\nExample with cat heart weight model from Figure 3.1 (a)\n\nPoint estimate is simply \\(\\hat{β_0}=2.4368\\).\nWith \\(n=50\\) there are \\(50-2=48\\) degrees of freedom, so for a 95% confidence interval the multiplier will be \\(t_{.025, 48}=2.010635\\).\nUsing the components \\(\\bar{x}=2.34\\), \\(\\sum(x-\\bar{x})^2=3.76\\), and \\(S_R=1.2007\\), we can calculate \\(SE_{\\hat{β_0}}=1.2007 \\sqrt{\\frac{1}{50}+\\frac{2.34^2}{3.76}}=1.4588\\).\nInterval: \\(2.4368\\pm(2.010635\\times 1.4588)=(-0.4963, 5.3699)\\)\nWith this, we are 95% confident that the interval going from \\(-0.4963\\) to \\(5.3699\\) captures the intercept for the linear model describing heart weight as a function of body weight in the full population of cats.\n\n\n\nSlope \\(β_1\\)\n\nThe point estimate for the intercept in your linear model describing the relationship between X and Y is simply your \\(\\hat{β_0}\\).\nThe multiplier used in your confidence interval will come from a t-distribution with \\(n-2\\) degrees of freedom. Why \\(n-2\\)? You started with n data points (and n degrees of freedom), and then you lose one degree of freedom for each parameter you estimate. You estimated two parameters: \\(β_0\\) and \\(β_1\\), hence \\(n-2\\).\nThe standard error for \\(\\hat{β_1}\\) is \\(SE_{\\hat{β_1}}=\\frac{S_R}{\\sqrt{\\sum(x-\\bar{x})^2}}\\) with \\(S_R\\) the same as before.\n\nExample with cat heart weight model from Figure 3.1 (a)\n\nPoint estimate is simply \\(\\hat{β_1}=2.8527\\).\nWith \\(n=50\\) there are \\(50-2=48\\) degrees of freedom, so for a 95% confidence interval the multiplier will be \\(t_{.025, 48}=2.010635\\).\nUsing the components \\(\\sum(x-\\bar{x})^2=3.76\\), and \\(S_R=1.2007\\), we can calculate \\(SE_{\\hat{β_1}}=\\frac{1.2007}{\\sqrt{3.76}}=0.6192\\)\nInterval: \\(2.8527\\pm(2.010635\\times 0.6192)=(1.6077, 4.0977)\\)\nWith this, we are 95% confident that the interval going from \\(1.6077\\) to \\(4.0977\\) captures the slope for the linear model describing heart weight as a function of body weight in the full population of cats.\n\n\n\n\n3.1.2 Hypothesis tests\nHypothesis tests for regression coefficients most often test the null hypothesis that \\(β_0=0\\) or that \\(β_1=0\\), but there is nothing stopping you from testing any value of interest for \\(β_0\\) or \\(β_1\\).\n\nIntercept \\(β_0\\)\nTo test \\(H_0:β_0=b_0\\) the test statistic is \\(t=\\frac{\\hat{β_0}-b_0}{SE_{β_0}}\\) with \\(SE_{\\hat{β_0}}\\) as defined earlier. The test statistic follows a t distribution with \\(n-2\\) degrees of freedom.\nExample with cat heart weight model from Figure 3.1 (a):\n\n\\(H_0:β_0=0\\) corresponding to testing if the line passes through the origin.\nTest statistic \\(t=\\frac{\\hat{β_0}-0}{SE_{β_0}}=\\frac{2.4368-0}{1.4588}=1.670\\).\nBeing a two-tailed test, the p-value is the area under the \\(t_{df=48}\\) distribution curve to the right of 1.670 and to the left of -1.670. This comes out to \\(p=0.1014\\), indicating there is not evidence the intercept differs from 0.\n\n\n\nSlope \\(β_1\\)\nAs with intercept, to test \\(H_1:β_1=b_1\\) the test statistic is \\(t=\\frac{\\hat{β_1}-b_1}{SE_{β_1}}\\) with \\(SE_{\\hat{β_1}}\\) as defined earlier and the test statistic follows a t distribution with \\(n-2\\) degrees of freedom.\nExample with cat heart weight model from Figure 3.1 (a):\n\n\\(H_0:β_1=0\\) corresponding to testing if the line is horizontal meaning knowing a cat’s body weight gives no meaningful insight into it’s heart weight.\nTest statistic \\(t=\\frac{\\hat{β_1}-0}{SE_{β_1}}=\\frac{2.8527-0}{0.6192}=4.607\\).\nBeing a two-tailed test, the p-value is the area under the \\(t_{df=48}\\) distribution curve to the right of 4.607 and to the left of -4.607. This comes out to \\(p&lt;0.0001\\), indicating there is strong evidence the slope differs from 0.\n\n\n\n\nIn R\n\nConfidence Intervals\nThere are a couple ways you can get your confidence intervals using R. Option one is to use the summary command to get the various elements you need for the intervals:\n\nmod_cat&lt;-lm(Hwt~Bwt, data=cats[1:50,])\n\nsummary(mod_cat)\n\n\nCall:\nlm(formula = Hwt ~ Bwt, data = cats[1:50, ])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.98316 -0.79264 -0.00526  0.86316  2.28737 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.4368     1.4588   1.670    0.101    \nBwt           2.8527     0.6192   4.607 3.03e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.201 on 48 degrees of freedom\nMultiple R-squared:  0.3066,    Adjusted R-squared:  0.2922 \nF-statistic: 21.23 on 1 and 48 DF,  p-value: 3.029e-05\n\n\nThere is lots of great information in this summary output. It begins with the Call section that shows you the function call you made that created the model object in the first place. Next is the Residuals section that gives you the five number summary for the model residuals providing a snapshot of the model errors. The Coefficients section has the output you care about most. This table gives you the \\(\\hat{β_0}\\) and \\(\\hat{β_1}\\) estimates along with their standard errors. Highlighted below as an example is the detail for the intercept. The estimate for \\(\\hat{β_0}\\) of \\(2.4368\\) is in the Estimate column, and the standard error of \\(\\hat{β_0}\\) , \\(1.4588\\) appears along side it in the Std. Error column.\n\n\n\n\n\nGetting your t multiplier then is as simple as using the qt function:\n\nqt(.025, df=48)\n\n[1] -2.010635\n\n\nAlso helpful: the “Residual standard error” given in the summary output below the Coefficients table is the \\(S_R\\) used in the standard error formulas.\nIf you want to have R do even more of the work for you, there’s the confint function. This provides you confidence intervals on both the intercept and slope as easily as:\n\nconfint(mod_cat, level=0.95)\n\n                 2.5 %   97.5 %\n(Intercept) -0.4963738 5.369927\nBwt          1.6076963 4.097623\n\n\n\n\nHypothesis Tests\nThe summary of your regression model Coefficients table not only gives the point estimate and standard error, but also the test statistic and p-value for the test of whether \\(β_i=0\\). Below is the cat model summary table highlighted to show the details for the test of \\(β_1=0\\).\n\n\n\n\n\nThe third column gives the t test statistic, \\(4.607\\), and the fourth column provides the p-value for the test. To the right of the p-value column R provides a code indicating the outcome of test. If the p-value is below 0.001 as is the case with slope here, three astrisk marks are shown. A p-value between 0.01 and 0.001 gets two asterisks, between 0.01 and 0.05, a single asterisk, a single “.” appears if the p-value is between 0.05 and 0.1, and the area is blank if the p-value is greater than 0.1. This is why no symbol is seen in the first row corresponding to the intercept.\n\n\n\nOn Your Own\n\nConsider the bac data frame from the openintro library. Fit a regression model predicting blood alcohol content as a linear function of beer consumption.\n\nInterpret your model.\nComplete and interpret a 95% confidence interval for the intercept.\nComplete and interpret a 95% confidence interval for the slope. What does the intercept mean in the context of this data?\nIs there significant evidence at the \\(α=0.05\\) level that the intercept is different from zero?\nIs there significant evidence at the \\(α=0.05\\) level that the slope is greater than 0.015?\n\nConsider the mtcars data frame from the datasets library. Fit a linear regression model predicting mpg as a function of vehicle weight in 1,000s of lbs.\n\nInterpret your model.\nComplete and interpret a 95% confidence interval for the intercept. What would the intercept mean in this context?\nComplete and interpret a 95% confidence interval for the slope.\nIs there significant evidence at the \\(α=0.05\\) level that a car’s fuel efficiency drops by an average of more than 5 mpg for every extra 1,000 lbs weight?\n\nConsider the record race times for Scottish hill races provided in the hills data in the MASS library. Fit a linear model predicting the record race time in minutes as a function of the race distance length in miles.\n\nInterpret your model.\nWhat does the intercept mean in this context? Complete and interpret a 90% confidence interval for intercept.\nComplete and interpret a 90% confidence interval for the slope.\nIs there significant evidence at the \\(α=0.01\\) level that an additional mile of race distances increases the record winning time by more than 7 minutes?\n\nConsider our cats heart and body weight data from the MASS library.\n\nFit a model predicting cat heart weight as a function of body weight using only the Male cats. Interpret your result.\nFit a model predicting cat heart weight as a function of body weight using only the Female cats. Interpret your result.\nComplete 90% confidence intervals for the slopes of each model. Discuss your results. Is there evidence the relationship between heart weight and body weight varies by cat sex?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How sure are you?</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#confidence-intervals-for-baryx",
    "href": "Chapter3.html#confidence-intervals-for-baryx",
    "title": "3  How sure are you?",
    "section": "3.2 Confidence Intervals for \\(\\bar{Y}|X\\)",
    "text": "3.2 Confidence Intervals for \\(\\bar{Y}|X\\)\nIn addition to confidence intervals for regression coefficients, you can also create confidence intervals for the mean Y at a given value of X. The point estimate that serves as the center of your confidence interval is simply the predicted value of Y given X based on your model fit and the multiplier is again a t distribution multiplier with \\(n-2\\) degrees of freedom.\nThe standard error of \\(\\bar{Y}\\) when \\(X=x_*\\) is\n\\[\nSE_{\\bar{Y}|x_*}=S_R\\sqrt{\\frac{1}{n}+\\frac{(x_*-\\bar{x})^2}{\\sum{(x_i-\\bar{x})^2}}}\n\\]\nThe \\(S_R\\) is the same one seen in section 3.1 and reflects the variability of residual error observed with the model fit. The \\((x_*-\\bar{x})^2\\) component is entirely new though. If you look back at Figure 2, you’ll see the five lines are closest together near the center of our collection of X values. On the outer edges, the lines are farther apart. This means we are much more confident in our prediction of Y given X in the middle of the X range than we are on the edges of the observed X range. That is what the \\((x_*-\\bar{x})^2\\) component is capturing because \\(x_*\\) is the value of X for which you are predicting Y. When \\(x_*\\) is close to \\(\\bar{x}\\), this term will quite small and only the \\(\\frac{1}{n}\\) in the square-root will really matter. When \\(x_*\\) is far from \\(\\bar{x}\\), this term will be large and the full \\(SE_{(\\bar{Y}|X)}\\) will increase.\nSo using our cat body and heart weight data model on the first 50 cats, if we are interested in the mean heart weight of cats with a body weight of 2.3 lbs, the steps to complete a 98% confidence interval would look like:\n\\[\n\\hat{Y}|(X=2.3)\\pm t_{\\frac{0.02}{2}, 48}\\times SE_{(\\bar{Y}|X=2.3)}\n\\]\n\\[\n\\hat{β_0}+(\\hat{β_1}\\times2.3)\\pm 2.4066 \\times 1.2007\\sqrt {\\frac{1}{50}+\\frac{(2.3-2.34)^2}{3.76}}\n\\]\n\\[\n8.9979\\pm2.4066\\times0.1716\n\\]\n\\[\n(8.5849, 9.4109)\n\\]\nThe interval for the mean heart weight of cats with a weight of 2.9 lbs however would be:\n\\[\n\\hat{β_0}+(\\hat{β_1}\\times2.9)\\pm 2.4066 \\times 1.2007\\sqrt {\\frac{1}{50}+\\frac{(2.9-2.34)^2}{3.76}}\n\\]\n\\[\n10.7095\\pm2.4066\\times0.3861\n\\]\n\\[\n(9.7803, 11.6387)\n\\]\nBecause 2.9 lbs is farther from the mean X of 2.34, the \\(SE_{\\bar{Y}|X=2.9}&gt;SE_{\\bar{Y}|X=2.3}\\) resulting in a wider interval.\n\nIn R\nConfidence intervals for \\(\\bar{Y}|X=x_*\\) are available through the same predict.lm function seen in Chapter 2 for calculating predicted values with your model. The usage begins the same as with finding predicted values: you input your fitted model object and then a data frame with the \\(x_*\\) values of interest. All that’s needed beyond that is specifying interval=\"confidence\". By default the confidence level is .95, but any value can be specified with a level= input as shown in the example below.\n\nmod_cat&lt;-lm(Hwt~Bwt, data=cats[1:50,])\n\nnew_cats&lt;-data.frame(Bwt=c(2.3, 2.9))\npredict.lm(mod_cat, newdata=new_cats, interval=\"confidence\", level=.98)\n\n        fit      lwr      upr\n1  8.997894 8.584937  9.41085\n2 10.709489 9.780337 11.63864\n\n\nThese result match what was found in the step-by-step calculations above.\n\n\nOn Your Own\n\nConsider the bac data frame from the openintro library.\n\nFit and interpret a regression model predicting blood alcohol content as a linear function of beer consumption.\nWhat do you estimate for the mean blood alcohol level when 1, 3, 5, or 7 beers are consumed?\nWhat is the t-multiplier you would use for a 98% confidence interval for the mean blood alcohol level after 1 beer?\nWould a 95% confidence interval for the mean blood alcohol level after 1 beer be wider, narrower, or the same width as a 98% confidence interval for blood alcohol after 4 beers? Explain.\nComplete 98% confidence intervals for the mean blood alcohol levels after 1, 3, 5, and 7 cans of beer. Write out an interpretation for the 3-beer interval.\n\nConsider the mtcars data frame from the datasets library.\n\nFit and interpret a linear regression model predicting mpg as a function of vehicle weight in 1,000s of lbs.\nComplete and interpret a 95% confidence interval for the mean mpg of cars weight 2,000 lbs, 2,500 lbs, 3,000 lbs, and 3,500 lbs.\nHow wide are each of your intervals from part b? Why is the 2,000 lb interval so much wider than the 3,000 lb interval? Explain.\nComplete and interpret a 98% confidence interval for the mean mpg of cars weight 3,000 lbs. How does this interval compare to the 3,000 lb interval from part b.\n\nConsider the speed and stopping distance data in the cars data frame.\n\nFit and interpret a linear regression model predicting stopping distance as a function of speed.\nComplete and interpret 90% confidence intervals for the mean stopping distance of cars traveling at 20 and 40 miles per hour.\nHow wide are each of your intervals from part b? Are you equally confident in each of them? Why or why not?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How sure are you?</span>"
    ]
  },
  {
    "objectID": "Chapter3.html#prediction-intervals-for-y",
    "href": "Chapter3.html#prediction-intervals-for-y",
    "title": "3  How sure are you?",
    "section": "3.3 Prediction Intervals for \\(Y\\)",
    "text": "3.3 Prediction Intervals for \\(Y\\)\nThere is a new type of interval in regression modeling that is a little different than those you’ve learned up until now. Your intro stats class likely considered confidence intervals for a mean, a proportion, the difference between two means, or the difference between two proportions. In section 3.1 we looked at confidence intervals for intercept and slope for our regression fit. Section 3.2 was confidence intervals for the mean \\(Y\\) at a given \\(X=x_*\\). What all of these have in common is that the interval is all about the estimate of an unknown population parameter.\nThere exists a true population mean, and you’re using your sample data to create an interval likely to capture that population parameter because measuring the entire population to calculate it directly is not feasible. Just as there exists a true slope for the relationship between X and Y for the entire population - you can’t measure the full population so you use your sample to develop an interval likely to include the population slope parameter.\nThis new type of interval, a prediction interval, isn’t about estimating an unknown population parameter. The goal with a prediction interval is to predict the next Y observation at a set \\(X=x_*\\). As such, prediction intervals are quite a bit wider than the corresponding confidence interval at \\(X=x∗\\) because single observations have quite a bit more variability than means. How much more variability? Well… look back at Figure 1. What measure do we have that describes the variability of individual points around the fitted regression line? The \\(S_R\\) term. That’s why while the standard error of \\(\\bar{Y}\\) at a given X looks like this:\n\\[\nSE_{\\bar{Y}|x_*}=S_R\\sqrt{\\frac{1}{n}+\\frac{(x_*-\\bar{x})^2}{\\sum{(x_i-\\bar{x})^2}}}\n\\]\nThe standard error of the next Y at a given X takes on this form:\n\\[\nSE_{y|x_*}=S_R\\sqrt{1+ \\frac{1}{n}+\\frac{(x_*-\\bar{x})^2}{\\sum{(x_i-\\bar{x})^2}}}\n\\]\nThe two are nearly identical, but with the addition of a simple \\(1+\\) we’ve added in the \\(S_R\\) one more time to account for the variability of individual points.\nThe general form of your prediction interval is exactly as with the confidence interval: \\[\npoint\\space estimate \\pm multiplier\\times standard \\space error\n\\]\nso if we want a 90% prediction interval for the heart weight of a cat with a body weight of 2.3, that is achieved through\n\\[\nE(Y|(X=2.3))\\pm t_{\\frac{0.1}{2}, 48}\\times SE_{(y|X=2.3)}\n\\]\n\\[\n\\hat{β_0}+(\\hat{β_1}\\times2.3)\\pm 1.6772 \\times 1.2007\\sqrt {1+\\frac{1}{50}+\\frac{(2.3-2.34)^2}{3.76}}\n\\]\n\\[\n8.9979\\pm1.6772\\times1.2129\n\\]\n\\[\n(6.9636, 11.0322)\n\\]\nNote that little \\(1+\\) addition has a big impact. Even though this is a 90% level interval, it is much much wider than the 98% confidence interval calculated in section 3.2 for the same \\(X=2.3\\). That’s because while that one was about \\(\\bar{Y}\\) at \\(X=2.3\\), this is about a single observation. So we can now say we are 90% confident the interval ranging from 6.9636 to 11.0322 grams includes the heart weight of Fluffy, a cat with a body weight of 2.3lbs.\n\nIn R\nFor prediction intervals you’re using the predict.lm function again only now specifying interval=\"prediction\".\n\nmod_cat&lt;-lm(Hwt~Bwt, data=cats[1:50,])\n\nnew_cats&lt;-data.frame(Bwt=c(2.3, 2.9))\npredict.lm(mod_cat, newdata=new_cats, interval=\"prediction\", level=.9)\n\n        fit      lwr      upr\n1  8.997894 6.963668 11.03212\n2 10.709489 8.594171 12.82481\n\n\n\n\nOn Your Own\n\nConsider the bac data frame from the openintro library.\n\nFit and interpret a regression model blood alcohol content as a linear function of the number of cans of beer consumed.\nWhat do you estimate for the blood alcohol content when 2, 4, 6, or 8 beers are consumed?\nWhat is the t-multiplier you would use for a 90% prediction interval for blood alcohol when 4 beers are consumed?\nComplete 90% confidence intervals for blood alcohol level with 2, 4, 6, or 8 beers are consumed. Write out an interpretation for the 4-beer interval.\nWould a 98% confidence interval for blood alcohol content when 2 beers are consumed be wider, narrower, or the same width as a 98% confidence interval for when 4 beers are consumed? Explain.\n\nConsider the mtcars data frame from the datasets library.\n\nFit and interpret a linear regression model predicting mpg as a function of vehicle weight in 1,000s of lbs.\nComplete and interpret a 95% prediction interval for the mpg of three new cars coming out soon with weights of 2,200 lbs, 2,700 lbs, and 3,300 lbs.\nHow wide are each of your intervals from part b? Why is the 2,200 lb interval so much wider than the 3,300 lb interval? Explain.\nComplete and interpret a 95% confidence interval for the mean mpg of cars weighing 3,300 lbs. How does this interval compare to the 3,300 lb interval from part b? Explain the difference.\n\nConsider the speed and stopping distance data in the cars data frame.\n\nFit and interpret a linear regression model predicting stopping distance as a function of speed.\nComplete and interpret 90% prediction intervals for the stopping distance of cars traveling at 15, 25, 35, and 45 miles per hour.\nHow wide are each of your intervals from part b? How would the width of an interval for a car traveling at 10mph compare to the widths seen in b?\nAre you equally confident in each of your intervals from part b? Why or why not?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>How sure are you?</span>"
    ]
  },
  {
    "objectID": "Chapter4.html",
    "href": "Chapter4.html",
    "title": "4  Assumptions Were Made",
    "section": "",
    "text": "4.1 Linearity\nPerhaps the most fundamental assumption you are making when you fit a linear regression model is that a line is a good model. This is why before even fitting your model you should start with a scatterplot. If your X and Y don’t look linearly related in a scatterplot it should be obvious that fitting a line is a bad idea.\nBut sometimes departure from linearity is subtle and your initial scatterplot can be misleading. Consider the case of the forbes data in the MASS library plotted below in Figure 4.1. This dataset contains 17 observations exploring the relationship between barometric pressure and the boiling point of water.\nFigure 4.1: Barometric pressure and boiling point, fitted\nThe fitted regression line looks pretty good at first glance, but let’s consider a new plot: the fitted vs. residual diagnostic scatter plot. In Figure 4.2 you can see the original data with the same fitted line as before along with a scatterplot of the model’s fitted values plotted against residuals. A red reference line corresponding to a residual equal to 0 is included.\nHmm… the fitted vs. residual plot is a giant arc (with one outlier). From the fitted vs. residual plot it becomes much more clear that as we scan from left to right along the blue fit line our data points begin below the fit line, then there’s a string of points above it (with the exception of an oddity at pressure=26.6), and then the last points are below the line again. This means that there is subtle curvature to the relationship between pressure and boiling point.\nIdeally your fitted vs. residual plot should be random scatter centered around zero. Without linearity, your line tends to over-estimate Y in some parts of X and under-estimate Y in others. If your linear model error isn’t consistent throughout the range of X, the inference methods of Chapter 3 fall apart and aren’t valid.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assumptions Were Made</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#linearity",
    "href": "Chapter4.html#linearity",
    "title": "4  Assumptions Were Made",
    "section": "",
    "text": "(a) Fitted Model\n\n\n\n\n\n\n\n\n\n\n\n(b) Fitted vs. Residuals\n\n\n\n\n\n\n\nFigure 4.2: Boiling model with residuals\n\n\n\n\n\n\nIn R\nCreating the fitted vs. residual diagnostic plot is straight-forward in R and makes use of elements stored in the lm output object:\n\n# fit the model\nmod_pressure&lt;-lm(bp~pres, data=forbes)\n\n# plot fitted values and residuals\nplot(mod_pressure$fitted, mod_pressure$residual, \n     xlab=\"Fitted Value\", ylab=\"Residual\", pch=16)\n\n# add horizontal reference line\nabline(h=0, col=2, lwd=3)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assumptions Were Made</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#homoskedasticity",
    "href": "Chapter4.html#homoskedasticity",
    "title": "4  Assumptions Were Made",
    "section": "4.2 Homoskedasticity",
    "text": "4.2 Homoskedasticity\nNot only do you want your model to consistently have an average residual of zero, you want the spread around zero to be the same across all values of X. The term for this idea of same spread is homoskedasticity.\nConsider the cpus data frame in the MASS library of R. This data includes a variety of performance measures and characteristics of CPUs (computer Central Processing Units). The left side of Figure 4.3 shows a plot of the estimated CPU performance based on a prediction model by Ein-Dor and Feldmesser and the published performance based on an industry benchmark. Part b of Figure 4.3 shows the fitted vs. residual plot for the fitted model predicting published performance based on the Ein-Dor/Feldmesser estimate.\n\n\n\n\n\n\n\n\n\n\n\n(a) Data with fit\n\n\n\n\n\n\n\n\n\n\n\n(b) Fitted vs. Residuals\n\n\n\n\n\n\n\n\n\n\n\n(c) Log(Fitted) vs. Residuals\n\n\n\n\n\n\n\nFigure 4.3: CPU Model and Residuals\n\n\n\nThis fitted vs. residual plot doesn’t show curvature like we saw with the boiling point data, but it doesn’t exactly look like a random cloud either. Figure 4.3 c stretches out the x-axis using a log scale so you can better picture what is going on with the residuals as we move from low performing to high performing CPUs. The CPUs on the lower end of our scale with log fitted performance below 2.0 have residuals with a noticeably tighter clump around zero than the more powerful CPUs with log fitted performance above 2.0.\nRecall from the inference equations in Chapter 3, a single \\(S_R\\) is used to represent the variability of residuals throughout the model. If \\(S_R\\) is not constant, those inference equations are no longer valid.\nBeyond evaluating constant variance with a fitted vs. residual scatterplot, there is also a formal hypothesis test you can perform to check if the homoskedasticity assumption is reasonable. The F-test considers the null hypothesis that the ratio of two variances is equal to 1, vs the alternative that the ratio is different from 1. The test statistic is simply:\n\\[\nF=\\frac{\\sigma^2_1}{\\sigma^2_2}\n\\] where \\(\\sigma^2_1\\) is the variance from group 1, and \\(\\sigma^2_2\\) is the variance of the second group. The degrees of freedom for the test are \\(n_1-1\\) and \\(n_2-1\\), where \\(n_1\\) and \\(n_2\\) are the two group sample sizes.\nPerforming this test on the CPU performance data and splitting the data so group 1 includes fitted values &lt;50 and group 2 has fitted values &lt;=50 yields:\n\\[\nF=\\frac{166.859}{3098.414}=0.05385\n\\]\nwith \\(100-1=99\\) and \\(109-1=108\\) degrees of freedom. As shown below this means the p-value for the test is very very small meaning there is strong evidence the ratio of variances is not 1 and homoskedasticity is not present. The opposite of homoskedasticity is heteroskedasticity.\n\n\n\n\n\n\n\n\n\n\nIn R\nThe new element here is the variance test. R makes this easy to do with the var.test function.\n\nmod_cpu&lt;-lm(perf~estperf, data=cpus)\n\ngroup&lt;-mod_cpu$fitted&lt;50\n\nvar.test(mod_cpu$residuals[group==TRUE], mod_cpu$residuals[group==FALSE])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assumptions Were Made</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#normality",
    "href": "Chapter4.html#normality",
    "title": "4  Assumptions Were Made",
    "section": "4.3 Normality",
    "text": "4.3 Normality\nThinking back to the inference procedures of Chapter 3, recall that the t-distribution plays a role. That is because it is assumed that the residuals are approximately Normally distributed. Looking again at the cat heart weight and body weight data and model, Figure 4.4 shows the fitted model, the fitted vs. residual plot, and a histogram of the model residuals.\n\n\n\n\n\n\n\n\n\n\n\n(a) Data with fit\n\n\n\n\n\n\n\n\n\n\n\n(b) Fitted vs. Residuals\n\n\n\n\n\n\n\n\n\n\n\n(c) Residual Hisogram\n\n\n\n\n\n\n\nFigure 4.4: Cat model and residuals\n\n\n\nA line looks like a reasonable approach looking at the random scatter of the fitted vs. residual plot, and there isn’t cause to suspect heteroskedasticiy either (an F-test comparing the variance of the first 50% of fitted values to the second 50% of values has a p-value of 0.1012 ). The histogram of residuals looks fairly Normal, but there’s more to do to check.\nFirst is a residual Quantile-Quantile plot, usually simply referred to as a QQ plot. This plot is a scatterplot looking at the quantiles of your residuals as observed vs the quantiles you would have if your data was perfectly normally distributed. Figure 4.5 shows three example QQ plots for three different distributions shapes.\n\n\n\n\n\n\n\n\n\n\n\n(a) Normal\n\n\n\n\n\n\n\n\n\n\n\n(b) Uniform\n\n\n\n\n\n\n\n\n\n\n\n(c) Skewed\n\n\n\n\n\n\n\nFigure 4.5: Three QQ-Plot examples\n\n\n\nIn a), you see the ideal case where the data and the theoretical Normal align and follow the red reference line. Figure 4.5 b) shows what your QQ plot will look like if your residuals are distributed in a way that has more weight on both tails than is expected with a Normal. Along the middle of 6B the reference line is fairly closely followed, but left side is above the line and the right side is below the line. On the far right, Figure 4.5 c shows what a QQ plot looks like with right-skewed data. The right-skew makes for points in the QQ plot that stretch high above the reference line on the right side of the plot. If left-skewed, the plot would show a trail of points far below the line on the left side.\nIn the case of the cat data, the QQ plot of the residuals is as shown in Figure 7. Though not perfect, the points follow the reference line fairly well.\n\n\n\n\n\n\n\n\nFigure 4.6: Cat Model Residuals\n\n\n\n\n\nA formal test of Normality can be performed using the Shapiro-Wilk test. Details of how the test works are beyond our scope here, but the execution of the test is easy with software such as R. The null hypothesis in the Shapiro-Wilk test is that the sample data comes from a Normal distribution. Therefore, if the Shapiro-Wilk p-value is small, that indicates the Null is likely false, and therefore your assumption about Normal residuals is likely false. In our cat model, the Shapiro-Wilk test on the residuals yields a p-value of 0.1046, supporting our conclusion that assuming Normality is reasonable based on the histogram and QQ plots.\n\nIn R\nQQ plots require two lines of code to create the visualization. The first line makes the scatterplot and the second overlays the reference line. The col and lwd options in the qqline function are optional, but can be helpful to make your reference line stand out a bit.\n\nqqnorm(mod_cat$residuals)\nqqline(mod_cat$residuals, col=2, lwd=3)\n\nThe Shapiro-Wilk test for Normality is even easier to execute in R than the F test for homoskedasticity.\n\nshapiro.test(mod_cat$residuals)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assumptions Were Made</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#independence",
    "href": "Chapter4.html#independence",
    "title": "4  Assumptions Were Made",
    "section": "4.4 Independence",
    "text": "4.4 Independence\nThe final big assumption that was made in fitter the regression model is related to independence. Does each point in your scatterplot represent an observation that is independent from all the others?\nOften this is an assumption that requires little more than pausing to think about how the data was collected. Does each point come from a different cat/cpu/person/car… whatever your study subject is? That’s a good indication you are working with independent data. One plot that is often worthwhile when considering independence is simply a look at your residuals in order from first collected to last collected.\nLike a good fitted vs. residual plot, a plot of residuals ordered by collection time should be a random scatter centered at zero. If, however, your data collector got better at their job over time, or equipment changed as it was used, or experimental conditions changed in a way you weren’t aware of, or… some other weirdness… you might see a pattern or drift in the plot indicating a lack of independence.\nFigure 4.7 shows the residuals from a)the CPU performance model b) the model predicting blood alcohol level based on beer consumption from Chapter 2.\n\n\n\n\n\n\n\n\n\n\n\n(a) BAC Residuals\n\n\n\n\n\n\n\n\n\n\n\n(b) CPU Residuals\n\n\n\n\n\n\n\nFigure 4.7: Residuals in order\n\n\n\nThe plot of BAC residuals might look like it has a hint of a downward slope, but much of that is caused by students 1 and 3 having two of three largest residuals. If point 3 was removed, your eye wouldn’t question this as a random jumble and we’re looking for patterns stronger than a single point’s influence.\nThe CPU residuals look random at first glance, but on a closer inspection you’ll notice that there seem to be some clumping behavior. For example right around the index of 100, there are six points all together with unusually large residuals. Later on around 155, there’s a clump of more than 10 points largely on top of each other forming a downward sloping tightly knit cluster.\nThankfully you need not rely on your eyes catching deviations from random scatter to properly assess independence; a formal test for correlated residuals exists. The Durbin-Watson test has the null hypothesis that the correlation of successive residuals (known as the autocorrelation of residuals) is zero against the alternative that it is non-zero. This means that small p-values from the Durbin-Watson test indicate the assumption of independence may not be valid.\nThe p-value for the Durbin-Watson test applied to the BAC model is 0.6152 and for the CPU model the Durbin-Watson p-value is 10^{-4}. This indicates that while our independence assumption for the student blood alcohol level data is likely sound, the same assumption for the CPU data is in doubt.\nWhy might that be? It seems unlikely that one published CPU performance compared to estimated performance would influence another one. The problem lies in our data’s ordering. The CPU data frame is not listed in the order of measurement, but rather is listed by alphabetical order of the CPU name. What’s happening here is that the performance estimate does a better job on CPUs from some manufacturers than others and so the errors turn up in clusters based on maker. Just because your Durbin-Watson test indicates significant autocorrelation doesn’t mean trouble - it just means there is structure to your data order you maybe weren’t aware of.\n\nIn R\nThe plot of residuals from first to last is a simple plot command. The Durbin-Watson test is available in the lmtest library in R and takes in a lm model object as the key input. Note that putting your residuals into the dwtest function won’t work - you need to input the model object.\n\n# fit the model\nmod_bac&lt;-lm(bac~beers, data=bac)\n\n# plot the residuals, add in a reference line if you like\nplot(mod_bac$residuals)\nabline(h=0, col=2, lwd=3)\n\n# load the lmtest library\nlibrary(lmtest)\n\n# run the Durbin-Watson test\ndwtest(mod_bac)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assumptions Were Made</span>"
    ]
  },
  {
    "objectID": "Chapter4.html#all-assumptions-on-your-own",
    "href": "Chapter4.html#all-assumptions-on-your-own",
    "title": "4  Assumptions Were Made",
    "section": "All Assumptions, On Your Own",
    "text": "All Assumptions, On Your Own\nNothing says your model can only break one assumption at a time. So you check them all…\n\nUsing the bac dataframe in the openintro library, fit a model predicting blood alcohol level as a linear function of beer consumption. Evaluate:\n\nIs the linearity assumption valid? Provide evidence and explain.\nIs the homoskedasticity assumption valid? Provide evidence and explain.\nIs the Normality assumption valid? Provide evidence and explain.\n\nCreate a model estimating an orange tree’s age using it’s circumference using the Orange data in the datasets library.\n\nIs the linearity assumption valid? Provide evidence and explain.\nIs the homoskedasticity assumption valid? Provide evidence and explain.\nIs the Normality assumption valid? Provide evidence and explain.\nIs the independence assumption valid? Provide evidence and explain.\n\nCreate a model estimating a Gentoo penguin’s body mass as a linear function of flipper length using data from the penguins data frame from the datasets library. Be sure to limit your model to only include the Gentoo species.\n\nIs the linearity assumption valid? Provide evidence and explain.\nIs the homoskedasticity assumption valid? Provide evidence and explain.\nIs the Normality assumption valid? Provide evidence and explain.\nIs the independence assumption valid? Provide evidence and explain.\n\nThe MASS library contains the anorexia data frame that includes the pre and post treatment weights of anorexia patients. Fit a model predicting a patient’s post-CBT treatment weight based on their pre-CBT treatment weight.\n\nIs the linearity assumption valid? Provide evidence and explain.\nIs the homoskedasticity assumption valid? Provide evidence and explain.\nIs the Normality assumption valid? Provide evidence and explain.\nIs the independence assumption valid? Provide evidence and explain.\n\nConsider the hills data frame in the MASS library. The data frame contains the record winning times for 35 Scottish hill races along with their distances and elevation climbs. Fit two models predicting a race’s record time 1) as a function of the race’s distance and 2) as a function of the race’s elevation climb.\n\nIs the linearity assumption valid for either model? Provide evidence and explain.\nIs the homoskedasticity assumption valid for either model? Provide evidence and explain.\nIs the Normality assumption valid for either model? Provide evidence and explain.\nIs the independence assumption valid for either model? Provide evidence and explain.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Assumptions Were Made</span>"
    ]
  },
  {
    "objectID": "Chapter5.html",
    "href": "Chapter5.html",
    "title": "5  Make it Work",
    "section": "",
    "text": "5.1 Simple Transformations\nJust because an assumption or two isn’t met, that doesn’t mean you can’t use regression modeling. It just means you have to work a little harder at it.\nWe know from the work in ?sec-chap4 that the relationship between barometric pressure and boiling point is not quite linear. Figure 5.1 shows the fitted regression line and the resulting fitted vs. residual plot with clear curvature. So what do we do about plots that aren’t linear? We make them linear.\nThe idea is this: rather than model boiling point as a function of pressure directly, could we model boiling point as a function of the square-root of pressure? Or log(pressure)? There are four main transformations to consider when trying to make non-linear data more linear: Squares, square-roots, inverses, and logs. Trial and error plus a little exploration might lead you beyond those four, but those are always good places to start.\nFigure 5.2 shows the resulting fitted vs. residual plot for each of these transformations applied to pressure. Axis labels are removed because we’re focused only on the shape of the scattered points here and how closely they follow the red line.\nFigure 5.2 (a) shows that results from modeling boiling point as a function of \\(pressure^2\\) is an even more pronounced parabola than what was originally seen in Figure 5.1; and Figure 5.2 (c) shows that when boiling point is modeled as a function of \\(\\frac{1}{pressure}\\) the parabola flips to become concave up, but does not flatten considerably. From part (b) it is clear that modeling using \\(\\sqrt{pressure}\\) is a step in the right direction, but it is the model using \\(log(pressure)\\) in (d) that is the clear winner of these four.\nThis means that rather than\n\\[boiling\\space point=β_0+β_1pressure+ε\\]\nour model will take the form of\n\\[boiling\\space point=β_0+β_1log(pressure)+ε\\]\nso a plot of our linear fit becomes what is shown in Figure 5.3. The estimated coefficients of the transformed model come out to be\\(\\hat{β_0}=\\) 47.8638 and \\(\\hat{β_1}=\\) 48.2467. So now to estimate the boiling point when \\(pressure=28\\) we’d have\n\\[\n47.8638+48.2467\\times log(28)=208.6317\n\\]\nFinding a transformation that makes your linear model work can be a challenge. Maybe your data needs \\(\\frac{1}{x^3}\\) or some other less obvious approach - that’s fine. Just start by looking at the shape of the curve you see in your data and consider what function will reverse that shape to create a line. At least one of squares, square-roots, inverses, and logs will usually show some promise and then you can trial and error your way from there to find \\(x^4\\) actually does a better job than \\(x^2\\).\nIt is to your advantage though to use a simpler transformation when possible. Why? Interpretation and communication. It’s pretty straight-forward to tell someone that boiling point is linearly related to the log of barometric pressure, but to try explaining that the square-root of boiling point is linearly related the inverse log of barometric pressure.\nWe’ve transformed X here, but you can also transform Y. Transforming X is generally preferable, because 1)every confidence interval or prediction interval would need to be either transformed back to original units, or interpreted in the revised transformed scale and 2) later we’ll be working with models that include more than one X and transforming Y will impact the relationships for all X at once rather than allowing you to tweak each separately.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make it Work</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#sec-simple-transformations",
    "href": "Chapter5.html#sec-simple-transformations",
    "title": "5  Make it Work",
    "section": "",
    "text": "(a) Data and model fit\n\n\n\n\n\n\n\n\n\n\n\n(b) Fitted vs residual plot\n\n\n\n\n\n\n\nFigure 5.1: Boiling point model and residuals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Squared\n\n\n\n\n\n\n\n\n\n\n\n(b) Square-root\n\n\n\n\n\n\n\n\n\n\n\n(c) Inverse\n\n\n\n\n\n\n\n\n\n\n\n(d) Log\n\n\n\n\n\n\n\nFigure 5.2: Four Tranformations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Log pressure plot with model fit\n\n\n\n\n\n\n\n\n\n\n\n(b) Fitted vs residual plot\n\n\n\n\n\n\n\nFigure 5.3: Boiling point log pressure model\n\n\n\n\n\n\n\nIn R\nWhen transforming data for linear modeling in R you have two options. First, you can create new variables for use in the lm command:\n\nlog_pres&lt;-log(forbes$pres)\nmodel_forbes&lt;-lm(forbes$bp~log_pres)\n\nThis takes an extra line of code but sometimes it’s the cleanest approach if you’re using the transformed x for uses beyond your linear model.\nSecond, you can use the I function inside the lm:\n\nmodel_forbes&lt;-lm(bp~I(pres^2), data=forbes)\n\nThis is a bit simpler as it is all in one line and when coupled with a data statment makes it 100% clear that both your X and Y live within the named data frame.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make it Work</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#log-log-transformation",
    "href": "Chapter5.html#log-log-transformation",
    "title": "5  Make it Work",
    "section": "5.2 Log-Log Transformation",
    "text": "5.2 Log-Log Transformation\nOne transformation that is quite common for violations in linearity and Normality involves taking the log of both your X and your Y term. Consider the plots below in Figure 5.4 showing the body and brain weights of 62 mammals. The raw data in fig (a) shows a hard to distinguish clump of points in the lower left corner because African elephant and Asian elephant dwarf the other mammals, but the relationship does not appear to be linear. Part (b) of Figure 5.4 shows the raw data limited only to those mammals weighing less than 1,000kg. You’ll see a hard to distinguish clump still remains since 39 of the 62 mammals are under 5kg, and the points beyond the clump provide further evidence a simple line isn’t going to fit well. Part (c) shows the result when a log transformation is applied to both body and brain weight of all 62 mammals.\n\n\n\n\n\n\n\n\n\n\n\n(a) All raw data\n\n\n\n\n\n\n\n\n\n\n\n(b) Only mammals &lt;1000kg\n\n\n\n\n\n\n\n\n\n\n\n(c) Log-log with linear fit\n\n\n\n\n\n\n\nFigure 5.4: Mammal body and brain weight\n\n\n\nBelow is the coefficent summary of the log-log model fit.\n\n\n             Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 2.1347887 0.09604339 22.22734 1.183207e-30\nlog_body    0.7516859 0.02846356 26.40871 9.835792e-35\n\n\nThe interpretation of coefficients in a log-log model is a bit special. Whereas in a non-transformed regression fit the slope is interpreted as the mean increase in Y associated with a one-unit increase in X, in the log-log case the slope is the mean percent increase in Y associated with a 1% increase in X. So in the case of mammal brain and body weight, we now see that a 1% increase in a mammal body weight is associated with a roughly 0.75% increase in brain weight.\nAs an example of how the transformation must be considered when making estimates, we turn to black-tailed deer. A 2023 study of dwarfed black-tailed deer on Blakely Island (Geiman and Long 2023) found an average body weight 40.2 kg. So using our model, what would be our best estimate of dwarf black-tailed deer brain weight? First we must take the log of 40.2, and after evaluating the linear fit, we must “un-log” the result to get back to the units of brain weight we care about.\n\n#using predict.lm\nnew_wt&lt;-data.frame(log_body=log(40.2))\npred_log_brain&lt;-predict.lm(mod_mammal, new_wt)\nexp(pred_log_brain)\n\n       1 \n135.8317 \n\n\nWith a 95% confidence interval of:\n\npred.ci&lt;-predict.lm(mod_mammal, new_wt, interval=\"conf\", level=.95)\nexp(pred.ci)\n\n       fit     lwr      upr\n1 135.8317 108.833 169.5281\n\n\nThe Blakely Island researchers estimated a brain weight of 153.8g based on skull cavity measures of found skulls; not far from our estimate of 135.8g and included in our confidence interval for the mean brain weight that ranges from 108.8g to 169.5g.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make it Work</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#box-cox-transformation",
    "href": "Chapter5.html#box-cox-transformation",
    "title": "5  Make it Work",
    "section": "5.3 Box-Cox Transformation",
    "text": "5.3 Box-Cox Transformation\nReal-world data often violate more than just the linearity assumption. Response variables might be skewed, or the variance might increase as the value of the response variable increases (heteroscedasticity). The Box-Cox transformation is a family of mathematical functions used to transform non-Normal dependent variables into a form that more closely follows a Normal distribution. The Box-Cox approach can also help in making variances more consistent to meet our homoskedasticity assmption for regression inference.\n Box-Cox transformations are especially useful when:\n\nYour response variable is strictly positive (\\(y &gt; 0\\)).\nYou observe non-constant variance (heteroscedasticity) in residual plots.\nThe distribution of your response variable is highly skewed.\nResiduals from a fitted model are not normally distributed.\n\nThe down side to Box-Cox is model interpretability - you’ll definitely have some reverse transforming to do once your model is fit so that people can understand it.\nThe Box-Cox transformation is defined as follows for a positive variable y:\n\\[\ny^{(\\lambda)} =\n\\begin{cases}\n\\frac{y^{\\lambda} - 1}{\\lambda} & \\text{if } \\lambda \\ne 0 \\\\\n\\ln(y) & \\text{if } \\lambda = 0\n\\end{cases}\n\\]\nHere, \\(\\lambda\\) (lambda) is a parameter that determines the form and strength of the transformation. The logarithm (\\(\\ln y\\)) is a special case when \\(\\lambda = 0\\).\nSo how do you decide what value to use for \\(\\lambda\\)? The algorithm to determine \\(\\lambda\\) is an iterative one that includes repeated fitting a linear model on transformed data and selecting the \\(\\lambda\\) with the maximum log-likelihood. Good news - R will do heavy repetitive lifting for you. Don’t just take R’s output and run with it though. You should always inspect diagnostic plots before and after applying Box-Cox to ensure it has improved model assumptions.\nFigure 5.5 shows residuals from our CPU performance model first seen in Chapter 4. When CPU performance is modeled as a linear function of estimated performance the variance of residuals is seen to grow substantially from left to right in our fitted vs. residual plot and a QQ plot of the residuals reveals notable deviations from Normality.\n\n\n\n\n\n\n\n\n\n\n\n(a) Fitted vs. Residuals\n\n\n\n\n\n\n\n\n\n\n\n(b) Residual QQ Plot\n\n\n\n\n\n\n\nFigure 5.5: CPU Performance Linear Model\n\n\n\nThe plot shown in Figure 5.6 is produced when R is called on to evaluate values of \\(\\lambda\\) in a Box-Cox tranformation. Dotted lines are included to make reading off the ideal \\(\\lambda\\) easier. Details from R tell us the maximum log-likelihood occurs when \\(\\lambda =0.6262\\).\n\n\n\n\n\n\n\n\nFigure 5.6: Box-Cox \\(\\lambda\\) log-likelihoods\n\n\n\n\n\nWhen the Box-Cox transformation with \\(\\lambda=.6262\\) is then applied to CPU performance something interesting happens though. Yes, the variance becomes much more stable, but we also see that there is significant curvature in the relationship between estimated CPU performance and our new transformed performance outcome. This necessitates a second step: transforming X, then re-running the Box Cox algorithm to find the optimal \\(\\lambda\\) when the transformed X is used to predict Y.\n\n\n\n\n\n\n\n\n\n\n\n(a) Plot of transformed data\n\n\n\n\n\n\n\n\n\n\n\n(b) Add X Transformation\n\n\n\n\n\n\n\n\n\n\n\n(c) New Box-Cox \\(\\lambda\\) plot\n\n\n\n\n\n\n\nFigure 5.7: CPU Model with Box-Cox\n\n\n\nThe curvature after the first Box-Cox round is shown in Figure 5.7 (a), with the fitted line overlaid in blue to make clear just how much curvature is present. Figure 5.7 (b) shows that if the square-root of estimated CPU performance is used instead of the estimated CPU performance directly the relationship becomes much more linear. Part (c) then shows the updated \\(\\lambda\\) estimate is slightly smaller at 0.4242.\nThe revised model model for CPU performance is now shown in Figure 5.8. The curvature is much improved, the variance appears fairly constant across the fitted vs. residual plot, and the QQ plot of residuals is much improved over the original QQ plot of Figure 5.5.\n\n\n\n\n\n\n\n\n\n\n\n(a) Plot of transformed data\n\n\n\n\n\n\n\n\n\n\n\n(b) Fitted vs. Residuals\n\n\n\n\n\n\n\n\n\n\n\n(c) QQ Plot\n\n\n\n\n\n\n\nFigure 5.8: Updated CPU Box-Cox, sqrt(x)\n\n\n\nWork determining a model for CPU performance could certainly end here. Figure 5.8 looks quite reasonable. However, there is still slight curvature in our data shown in plot (a). Maybe we then try a log(estperf) approach to handle curvature. Then a new Box Cox \\(\\lambda\\) search would be needed.\nFigure 5.9 shows the result of using log(estperf) to predict a Box Cox with \\(\\lambda=0.10101\\) applied to CPU performance. The curvature issue is improved over what was seen in Figure 5.8 (a), but the variance in Figure 5.9 (b) is not nearly as consistent as in Figure 5.8 (b) and creates homoskedasticity concerns. Normality is reasonably met with both models\n\n\n\n\n\n\n\n\n\n\n\n(a) Plot of transformed data\n\n\n\n\n\n\n\n\n\n\n\n(b) Fitted vs. Residuals\n\n\n\n\n\n\n\n\n\n\n\n(c) QQ Plot\n\n\n\n\n\n\n\nFigure 5.9: Updated CPU Box-Cox, log(x)\n\n\n\nSo which model is the right model? Box-Cox combined with square-root of estimated performance, or Box-Cox combined with log estimated performance? I prefer the square-root one, but really this is where you have to accept something not all students handle well. There is no one right model. As you gain experience with regression and other data science techniques you will learn this work has an element of art to it. Two different statisticians will quite reasonably come up with two slightly different models and that doesn’t mean either one is wrong.\nStatistician George Box (yup, the Box in Box-Cox) is credited with saying “All models are wrong, but some are useful.” He’s also credited with admonishing that “Statisticians, like artists, have a bad habit of falling in love with their models.” Take these words to heart. If two competing models seem equally useful, pick one and move on with your life. But also, if you discover a flaw in your model, don’t be so wedded to it that you don’t revise it to fix the flaw.\n\nIn R\nTo work with Box-Cox transformations you’ll want to load the MASS library. From there it is as simple as using the boxcox function. The only input the function needs is your basic fitted model object, and it will then output the graph below.\n\nlibrary(MASS)\nsimple_mod&lt;-lm(perf~estperf, data=cpus)\nbc&lt;-boxcox(simple_mod)\n\n\n\n\n\n\n\n\nBy saving the boxcox result as bc (or whatever other name you’d like) you are able to pull the optimal \\(\\lambda\\) parameter by:\n\nbc$x[which.max(bc$y)]\n\n[1] 0.6262626\n\n\nAnd then apply the result:\n\nbc_perf&lt;-((cpus$perf^0.6262626)-1)/0.6262626\nmod_bc&lt;-lm(bc_perf~estperf, data=cpus)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make it Work</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#unit-change",
    "href": "Chapter5.html#unit-change",
    "title": "5  Make it Work",
    "section": "5.4 Unit change",
    "text": "5.4 Unit change\nThere is one last transformation to be aware of and it’s the simplest of all: a change of units. Regardless of the units used to measure \\(x\\) and \\(y\\) the relationship between \\(x\\) and \\(y\\) should remain unchanged. This means if our car fuel efficiency data had km per liter instead of miles per gallon, heavier cars would still be less efficient than lighter cars. It also means if we measured boiling point in degrees Celsius instead of Fahrenheit, the relationship between barometric pressure and boiling point should be the same as we saw back in Figure 5.3 (a).\nThis doesn’t mean the fitted parameters stay the same as units change though. Consider our beer consumption model from Section 2.3. The data is recorded as cans of beer, but could just as easily have been measured in fluid ounces (and that probably would have been better for clarity). Let’s see how the model changes for the two measures of beer consumption:\n\nmod1&lt;-lm(bac~beers, data=bac)\nsummary(mod1)$coef\n\n               Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -0.01270060 0.012637502 -1.004993 3.319551e-01\nbeers        0.01796376 0.002401703  7.479592 2.969480e-06\n\nbeer_oz&lt;-bac$beers*12\nmod2&lt;-lm(bac~beer_oz, data=bac)\nsummary(mod2)$coef\n\n               Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -0.01270060 0.012637502 -1.004993 3.319551e-01\nbeer_oz      0.00149698 0.000200142  7.479592 2.969480e-06\n\n\nThe intercepts of the two models are exactly the same, but the slopes are different. When beer is measure in cans, the slope is 0.01796 but when measured in fluid ounces, 12 times the can count, the slope is 0.0015. Notice that 12 times 0.0015 equals 0.01796. This makes sense because if \\(bac=-0.0127+0.01796\\times cans\\) then the relationship can be equivalently expressed as \\(bac=-0.0127+(0.01796/12)\\times(cans\\times12)\\) (the 12s cancel out to make it all simply equal to multiplying by 1).\nStandard error for our slope estimate changes by the same multiplier so in the end the t-value and corresponding p-value for slope remain unchanged.\nRemember this simple transformation should you find yourself with really tiny slopes you’d like to make bigger, or really large slopes you’d like to make smaller, just for ease of communication. A quick change of measuring in kg to measuring in g will shrink a slope by a factor of 1000 just as going from m to km will increase it by a factor of 1000. Likewise sometimes it makes sense to use measure of 1,000s of people or millions of dollars just because the coefficients are unwieldy when individual people or dollars are the units. Talking about an increase of 2.3 extra points on statewide tests per million dollars in education spending is just easier than an increase of 0.0000023 points per dollar spent.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make it Work</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#on-your-own-all-transformations",
    "href": "Chapter5.html#on-your-own-all-transformations",
    "title": "5  Make it Work",
    "section": "On Your Own, All Transformations",
    "text": "On Your Own, All Transformations\n\nThe Puromycin dataset in the datasets package contains information regarding an experiment investigating instantaneous enzymatic reaction rates of cells as a function of substrate concentrations.\n\nBuild a model for reaction rate as a function of concentration considering only those cells that are untreated. Communicate your model fit and provide evidence major assumptions on residuals are all met.\nBuild a model for reaction rate as a function of concentration considering only those cells that are treated. Communicate your model fit and provide evidence major assumptions on residuals are all met.\nCreate and explain a 90% prediction interval for the reaction rate of an untreated cell with a concentration of 0.4.\nCreate and explain a 90% confidence interval for the mean reaction rate of treated cells with a concentration of 0.4.\nIs there sufficient evidence, at the \\(\\alpha=0.05\\) level, that the reaction rate change associated with changes in concentration is significantly different for the treated and untreated cell groups? Explain.\n\nThe muscle dataset in the MASS library has information on an experiment on rat heart muscle. Using the data, fit a model predicting the change in muscle strip length using the calcium chloride solution concentration.\n\nExplain what transformation you used and interpret your model. Show all assumptions are met.\nCreate a graph of your model, adding the fitted the curve to a plot of un-transformed concentration and length.\nIs the expected length change with zero calcium chloride concentration greater than 0? Explain.\n\nThe mammals dataset in the MASS library contains the brain and body weights of 62 mammals.\n\nApply the appropriate transformation to make a linear model work for predicting brain weight as a function of body weight. Explain your model and demonstrate all assumptions are met.\nAn adult llama weighs approximately 180kg. Create and explain a 90% prediction interval for the weight of a llama brain. After you’ve created your interval, google it and see if your interval contains the answer you find.\nCreate and interpret a 95% confidence interval for the slope of your model.\n\nUsing the pressure dataset in the datasets package, fit an appropriate model for vapor pressure as a function of temperature.\n\nFitting the model requires a transformation. What transformation did you select and why?\nWhat is the equation of your fitted line? Explain how pressure changes for every one degree increase in temperature.\nProvide graphical evidence that all key assumptions are met in your model.\nModify the data to use temperature in degrees Farhenheit. How does your model change? Does this match your expectation? Explain.\n\nUsing the trees dataset from the datasets package,\n\nFit a model for tree volume as a function of tree height. Discuss your chosen model and demonstrate assumptions are all met.\nFit a model for tree volume as a function of tree girth. Discuss your chosen model and demonstrate assumptions are all met.\nWhich model does a better job of predicting tree volume? Explain your choice.\n\nUse the cabbages data in the MASS library. Your goal is to fit a model for the weight of a head of cabbage as a function of the vitamin C content.\n\nWhat assumption or assumptions are not met by a basic linear model? Explain how you know.\nApply a transformation and fit your model. Interpret the fit and give evidence assumptions are all met.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make it Work</span>"
    ]
  },
  {
    "objectID": "Chapter5.html#weighted-least-squares",
    "href": "Chapter5.html#weighted-least-squares",
    "title": "5  Make it Work",
    "section": "5.5 Weighted Least Squares",
    "text": "5.5 Weighted Least Squares\nSometimes your data points should not all be treated equally. Why not? Well consider the babies_crawl data frame in the openintro library. This data provides the average age at which babies began to crawl by birth month. Researchers speculated that babies who are bundled up for cold weather when they are six months old likely learn to crawl later than babies who are six months in warmer seasons and have less clothing inhibiting movement (Benson 1993).\n\n\n\n\n\n\n\n\nFigure 5.10: Crawling Age and Temperature\n\n\n\n\n\nFrom Figure 5.10 it looks like the hypothesis has some merit. A best fit line would reach from the upper left to the lower right indicating babies with warmer temperatures are crawling earlier than babies learning in colder temperatures. However, this plot is hiding some key information about the data. We have just one point for every birth month, but each birth month includes anywhere from 21 to 49 babies. Some months the standard deviation of crawling age was less than 6 weeks, but others it was greater than 8 weeks. If we treat each point equally, those differences of samples sizes and in-month variability are lost.\nEnter Weighted Least Squares (WLS) regression. By applying WLS, you give more weight to observations you trust more, and less weight to those you trust less. Generally in simple linear regression our goal is to fit the \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) model to minimize \\(\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\\), the sum of squared residuals. With WLS we instead aim to minimize weighted residuals \\(\\sum_{i=1}^{n} w_i (Y_i - \\hat{Y}_i)^2\\) where, \\(w_i\\) is the weight assigned to the \\(i^{th}\\) data point. Along with this, our SST and SSR change to incorporate weights as well:\n\\[\nSSError=SSE=\\sum w_i(y_i-\\hat{y}_i)^2\n\\]\n\\[\nSSTotal=SST=\\sum w_i(y_i-\\bar{y})^2\n\\]\n\\[\nSSRegression=SSR=\\sum w_i(\\bar{y}_i-\\hat{y}_i)^2\n\\]\nIt still holds that SST=SSE+SSR. Comparing \\(R^2\\) values of models with different weights is not a fair comparison when evaluation options. Choosing suitable and appropriate weights is crucial. Common approaches include:\n\nInverse Variance Weighting (Most Common)\nIf each observation has a known variance \\(\\sigma_i^2\\) , assign \\(w_i=1/\\sigma_i^2\\). This means observations with smaller variance (higher precision) get more weight.\nBased on Measurement Error\nIf the measurement error for each \\(Y_i\\) is known, assign smaller weight to points with larger error.\nSubjective or Utility Weights\nSometimes weights reflect how important or relevant each data point is for your particular context.\nIteratively Estimated Weights\nWhen heteroskedasticity is present but true variances are unknown, estimate the relationship between variance and the predictor and then assign weights accordingly.\n\nBy understanding when and how to assign weights, you can improve the performance of your regression analysis and draw better conclusions from your data.\nIn the babies_crawl data, the number of babies for each month is given in the n column, and the standard deviation of crawling age observed within each month is given in sd. From the Central Limit Theorem, we know these two components can give us the standard error for the 12 average crawling ages: \\(\\frac{sd}{sqrt(n)}\\). Using the inverse of standard error as weights will give higher weights to low standard errors and low weights to months with high standard errors.\nFigure 5.11 shows the model without weights in a red dashed line, and the model using weights in blue. Each point in the plot is sized relative to it’s weight so we can see the unusually low crawling age at a temperature of 52 is weighted quite low in the weighted model.\n\n\n\n\n\n\n\n\nFigure 5.11: Crawling Age and Temperature, two models\n\n\n\n\n\nNow for an example that doesn’t have standard deviation provided directly. In the openintro library you’ll find the mammals data frame that contains not only brain and body weight as we looked at earlier, but also information on sleep and dreaming.\n\n\n\n\n\n\n\n\n\n\n\n(a) Data with fitted line\n\n\n\n\n\n\n\n\n\n\n\n(b) Fitted vs. Residuals\n\n\n\n\n\n\n\nFigure 5.12: Mammal Sleeping and Non-Dreaming\n\n\n\nIf you split the data into four quartiles and calculate the variance for each group the result is as shown in Figure 5.13 (a). Though close, the relationship between the center of each grouping and the group’s residual variance is not quite linear but a simple square transformation on the group center fixes that. This means the variance of the residuals can be modeled linearly by the square of X. Therefore, to use inverse variance as our weights then, we can use \\(\\frac{1}{X^2}\\).\n\n\n\n\n\n\n\n\n\n\n\n(a) Four group variances\n\n\n\n\n\n\n\n\n\n\n\n(b) Group centers squared vs group variances\n\n\n\n\n\n\n\nFigure 5.13: Mammal Sleeping and Non-Dreaming\n\n\n\nWithout incorporating weights, the model is the dashed orchid line shown in Figure 5.14. With the inverse \\(X^2\\) weights the model is the solid blue line. The models are quite close, but real differences emerge when inference methods are applied under the two models. That is because \\(S_R\\) now includes the weights:\n\\[\nS_{R,w}=\\sqrt\\frac{\\sum w_i(y_i-\\hat{y_i})^2}{n-2}\n\\]\nand weights further play a role in the standard errors for the weighted model coefficients as:\n\\[\nSE_{\\hat{β}_{0,w}}=S_{R,w}\\sqrt{\\frac{1}{n}+\\frac{\\bar{x}_w^2}{\\sum w_i(x-\\bar{x})^2}}\n\\]\n\\[\nSE_{\\hat{β}_{1,w}}=\\frac{S_{R,w}}{\\sqrt{\\sum w_i(x-\\bar{x}_w)^2}}\n\\]\nwhere \\(\\bar{x}_w\\) is the weighted mean: \\[\\bar{x}_w=\\frac{\\sum(w_i \\times x_i)}{\\sum{w_i}}\\]\n\n\n\n\n\n\n\n\nFigure 5.14: Mammal sleep model, Weighted & Unweighted\n\n\n\n\n\nThis means in the unweighted model, \\(S_R=\\) 1.0026 but in the weighted version, \\(S_R=\\) 0.0902.\nFurther, for the unweighted model the 95% confidence interval for slope \\(\\hat \\beta_1\\) ranges from 0.7064 up to 0.8349 but in the weighted model the interval for \\(\\hat{\\beta}_{1,w}\\)is from 0.7717 to 0.8796.\nSimilarly, inference for Y values corresponding to a particular \\(X=x_*\\) changes with revised standard error equations:\n\\[\nSE_{\\bar{Y}|x_*,w}=S_{R,w}\\sqrt{\\frac{1}{\\sum w_i}+\\frac{(x_*-\\bar{x}_w)^2}{\\sum{(x_i-\\bar{x}_w)^2}}}\n\\]\nThe standard error of the next Y at a given X takes on this form:\n\\[\nSE_{y|x_*,w}=S_{R,w}\\sqrt{\\frac{1}{w_*}+ \\frac{1}{\\sum w_i}+\\frac{(x_*-\\bar{x}_w)^2}{\\sum{w_i(x_i-\\bar{x}_w)^2}}}\n\\]\nwhere \\(w_*\\) is the weight associated with \\(x_*\\), the X location of the prediction.\nThis means that while the 95% prediction interval for the amount of non-dreaming sleep that is typical for a mammal that gets nine hours total sleep ranges from 5.32 to 9.4 with the unweighted model, our weighted model produces a narrower interval going from 5.68 to 8.98.\n\nIn R\nThe lm command accommodates weights with minimal change to the code you’ve already learned:\n\nmod_mam_sleep_wt&lt;-lm(non_dreaming~total_sleep, data=mammals, \n                     weights=1/mammals$total_sleep^2)\n\nsummary(mod_mam_sleep_wt)$coef\n\n              Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) -0.1039658 0.20031687 -0.5190066 6.062443e-01\ntotal_sleep  0.8256810 0.02681064 30.7967632 2.383219e-32\n\n\nThe confint and predict.lm functions can still be used on weighted models just as you did with the simpler models without weights. With prediction intervals, you’ll need to input the weight that should be used at the \\(X=x_*\\) of interest.\n\nconfint(mod_mam_sleep_wt)\n\n                 2.5 %    97.5 %\n(Intercept) -0.5071827 0.2992512\ntotal_sleep  0.7717140 0.8796480\n\npredict.lm(mod_mam_sleep_wt, data.frame(total_sleep=c(9,12)), \n           interval=\"prediction\", weights=c(1/81, 1/144))\n\n       fit      lwr       upr\n1 7.327163 5.675607  8.978719\n2 9.804206 7.596301 12.012112\n\n\n\n\nOn Your Own\n\nInstall and load the fivethirtyeight package. Using the murder_2015_final data file, estimate the 2015 murder rate in large US cities using the 2014 murder rates.\n\nBegin by fitting a basic model in the form of murders_2015~murders_2014 regression model. Check all inference assumptions. Which assumptions are met and which need attention?\nDevelop a model for the variance to be used in a weighted model. Explain your approach.\nFit and interpret your weighted least squares model.\n\nExamine the Rabbit data frame in the MASS library.\n\nFind the variance for each dose level and use that in a weighted regression model predicting blood pressure change. Explain what your model shows.\nComplete and interpret a 95% confidence interval for the mean change in blood pressure when given a dose of 80 micrograms of Phenylbiguanide.\n\n\n\n\n\n\nBenson, Janette B. 1993. “Season of Birth and Onset of Locomotion: Theoretical and Methodological Implications.” Infant Behavior and Development 16 (1): 69–81. https://doi.org/10.1016/0163-6383(93)80029-8.\n\n\nGeiman, Claire, and Eric Long. 2023. “Data from: Allometric Brain Reduction in an Insular, Dwarfed Population of Black-Tailed Deer.” Dryad. https://doi.org/10.5061/DRYAD.GQNK98STM.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make it Work</span>"
    ]
  },
  {
    "objectID": "Chapter6.html",
    "href": "Chapter6.html",
    "title": "6  Zeros and Ones",
    "section": "",
    "text": "6.1 Indicator variables\nAn indicator variable, also known as a dummy variable, is a variable used to represent membership in a specific category of a categorical variable. Using only the values 0 and 1, the indicator variable will indicate the presence of a particular attribute or membership in a category of interest with a 1, and use a 0 for everything else.\nIndicator variables are essential when incorporating categorical variables—like treatment group, sex, or employment status—into linear regression models. For example, to represent whether each individual included in a survey is employed or unemployed, an indicator variable could be defined as 1 if the person is employed, and 0 if the person is unemployed.\nFor categorical variables with more than two categories, a separate indicator variable is created for each category except one, which becomes the reference (or baseline) group. This encoding, often called one-hot encoding, ensures that the model can estimate effects relative to the reference group while avoiding perfect collinearity (remember we need \\(X^TX\\) to have an inverse). For instance, if we instead treated employment as having three categories of full-time, part-time, or unemployed we would create two indicator variables: one for full-time and one for part-time. Unemployed would then serve as the reference group.\nThe choice of which group serves as the reference group is arbitrary, but it will affect the interpretation of model coefficients. For this reason, if your categorical variable is ordinal, it is customary to select either the lowest or the highest ordered category as your reference group; Interpretation of coefficients relative to max or min just often makes the most sense.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Zeros and Ones</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#indicator-only-regression-model",
    "href": "Chapter6.html#indicator-only-regression-model",
    "title": "6  Zeros and Ones",
    "section": "6.2 Indicator only Regression Model",
    "text": "6.2 Indicator only Regression Model\nWhile not commonly done, you can create a simple linear regression model using only an indicator variable as a predictor. Consider again the cats data frame in the MASS library. This data contains not only the heart weight and body weight of 144 cats as we’ve seen in earlier models, but also the sex of those 144 cats. From Figure 6.1 below it is clear that the heart weight of female cats is a bit less on average than the heart weight of male cats.\n\n\n\n\n\n\n\n\n\n\n\n(a) Straight data\n\n\n\n\n\n\n\n\n\n\n\n(b) Jittered sex for clarity\n\n\n\n\n\n\n\nFigure 6.1: Cat heart weight by sex, 1=Female\n\n\n\nIf we fit a model of the form \\(y=β_0+β_1x+ε\\) and have \\(x=0\\) for female cats and \\(x=1\\) for male cats what values of \\(β_0\\) and \\(β_1\\) should we expect? Since there are only two possible values of \\(x\\), there will be only two possible fitted values from our model: one corresponding to when \\(x=0\\) and one for when \\(x=1\\). To minimize the sum of squared error, we want the output when \\(x=0\\) to be equal to the mean heart weight for female cats and the output when \\(x=1\\) to be equal to the mean heart weight for male cats. This is achieved when \\(β_0\\) equals the mean heart weight for females and when \\(β_0+β_1\\) mean heart rate for males. Therefore \\(β_1\\) should be the difference between the two group means.\nHere’s the work in R:\n\nmean(cats$Hwt[cats$Sex==\"F\"])\n\n[1] 9.202128\n\nmean(cats$Hwt[cats$Sex==\"M\"])-mean(cats$Hwt[cats$Sex==\"F\"])\n\n[1] 2.120553\n\nmod_cat_sex&lt;-lm(Hwt~Sex, data=cats)\nsummary(mod_cat_sex)$coef\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 9.202128  0.3250734 28.307842 2.959034e-60\nSexM        2.120553  0.3960745  5.353924 3.379786e-07\n\n\nJust as suspected, the result gives us an intercept equal to the mean heart weight of female cats and a slope equal to the difference between male and female heart weights.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Zeros and Ones</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#adding-in-an-indicator",
    "href": "Chapter6.html#adding-in-an-indicator",
    "title": "6  Zeros and Ones",
    "section": "6.3 Adding in an Indicator",
    "text": "6.3 Adding in an Indicator\nYou can also add an indicator term to a model to create a linear equation of the form:\n\\[\ny=β_0+β_1x_1+β_2x_2+ε\n\\]\nContinuing with the cat heart weight example, we could fit this type of model with \\(y=\\) heart weight, \\(x_1=\\) body weight, and \\(x_2=\\) sex (0=female, 1 = male)\nThe resulting model fit is:\n\n\n               Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) -0.41495263  0.7273243 -0.5705194 5.692336e-01\nBwt          4.07576892  0.2947885 13.8260785 5.119676e-28\nSexM        -0.08209684  0.3040474 -0.2700133 7.875448e-01\n\n\nHow can this be interpreted? First, note that the third row begins with SexM this is R’s way of communicating that for the Sex variable level M is the one that is corresponding to the indicator equalling 1. This tells us that the heart weight of cats is, on average, equal to -0.415 plus (4.0758 \\(\\times\\) body weight) for female cats, and (-0.415 plus -0.0821 ) plus (4.0758 \\(\\times\\) body weight) for male cats. Same slope for all cats, but intercepts that differ by -0.0821.\n\n\n\n\n\n\n\n\nFigure 6.2: Cat heart weight as function of body weight\n\n\n\n\n\nHmmm… the two lines look pretty much the same. Why is that? Well take a look again at the model output above. We see that the difference in intercepts is only -0.0821, not a very big number. But scanning across this line in the output we also see that the p-value in the t-test of \\(H_o:β_2=0\\) is 0.788 which means that with any reasonable \\(\\alpha\\) level, we would fail to reject the null. This means the coefficient associated with sex is not significantly different from zero, and the two lines aren’t really needed - one will do just fine.\nOther times the indicator will matter quite a bit. Consider the mtcars dataframe from the datasets library. In a model predicting vehicle fuel efficiency as a function of horse power, does it help if we add in an indicator that is 1 for a manual transmission and 0 for an automatic? Our model will again be in the form of:\\[\ny=β_0+β_1x_1+β_2x_2+ε\n\\]\nwith \\(y=\\)mpg, \\(x_1=\\)horsepower, and \\(x_2=\\)manual transmission.\n\n\n              Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 26.5849137 1.425094292 18.654845 1.073954e-17\nhp          -0.0588878 0.007856745 -7.495191 2.920375e-08\nam           5.2770853 1.079540576  4.888270 3.460318e-05\n\n\nIn this case, the test of \\(H_o:β_2=0\\) has a p-value that is 3^{-5} meaning there is strong evidence this additional term related to transmission type matters. In a plot of the fitted model we see:\n\n\n\n\n\n\n\n\nFigure 6.3: Car MPG as function of HP\n\n\n\n\n\nFigure 6.3 shows us very clearly that cars of equal horsepower tend to get better gas mileage if they are a manual transmission. The difference between the two lines is \\(β_2\\), 5.2771.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Zeros and Ones</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#interacting-with-an-indicator",
    "href": "Chapter6.html#interacting-with-an-indicator",
    "title": "6  Zeros and Ones",
    "section": "6.4 Interacting with an Indicator",
    "text": "6.4 Interacting with an Indicator\nParallel lines are great and all, but there’s no reason to believe that the best model fit for two levels of your indicator should always have the same slope. Ideally we want a model that takes a form more like:\\[\ny=β_0+β_1x_1+β_2x_2+β_3x_1x_2+ε\n\\]\nWith this structure, for our baseline level of \\(x_2=0\\), the \\(β_2\\) and \\(β_3\\) terms disappear since they are multiplied by zero, leaving you with the basic \\(β_0\\) as intercept and \\(β_1\\) as slope. When \\(x_2=1\\) though, the \\(β_2\\) and \\(β_3\\) terms stick around and \\(β_0+β_2\\) becomes the intercept and \\(β_1+β_3\\) becomes the slope. Having a \\(x_1x_2\\) term is called having an interaction between \\(x_1\\) and \\(x_2\\). The \\(x_2\\) indicator value is interacting with the slope associated with your continuous \\(x_1\\).\nRevisiting the cat heart and body weight data, might sex play a significant role in predicting heart weight if we allow slope to change as well as intercept? Here’s the R model summary for the revised model that includes the \\(β_3x_1x_2\\) term:\n\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  2.981312  1.8428394  1.617782 0.1079604636\nBwt          2.636414  0.7759022  3.397869 0.0008845733\nSexM        -4.165400  2.0617552 -2.020318 0.0452578391\nBwt:SexM     1.676265  0.8373255  2.001927 0.0472246471\n\n\nThe last row labeled Bwt:SexM corresponds to our \\(β_3x_1x_2\\) term. Reading across the output summary table shows us that this \\(β_3\\) coefficient is estimated to be significantly different from 0 at the \\(\\alpha=0.05\\) level. The \\(β_2\\) term allowing for different intercepts that was not significant in the limited \\(y=β_0+β_1x_1+β_2x_2\\) model is now also showing a p-value below 0.05 at 0.0453. Plotted, the fits for male and female cats now look like the lines in Figure 6.4.\n\n\n\n\n\n\n\n\nFigure 6.4: Cat heart weight as function of body weight\n\n\n\n\n\n\nIn R\nIn the above example using mtcars, you may have noticed that the reference level wasn’t clear in the model summary output. That is because the transmission type variable, am, is pre-coded as a 0/1 indicator variable. The help menu for mtcars explains that am is a 0 for automatic, and 1 for manual. When read into R, it takes this 0/1 coding as a numeric 0/1 and doesn’t think of it as a factor type needing a reference level at all. Mathematically that is fine - it just makes interpretation a little trickier because you need to remember what is coded as 0 and what is coded as a 1.\nHere is the code for the model described earlier predicting mpg as a function of horsepower and transmission type (am) :\n\n# adding on +am to mpg~hp creates an additive beta for transmission\nmod_mpg&lt;-lm(mpg~hp+am, data=mtcars)\nsummary(mod_mpg)$coef\n\n              Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 26.5849137 1.425094292 18.654845 1.073954e-17\nhp          -0.0588878 0.007856745 -7.495191 2.920375e-08\nam           5.2770853 1.079540576  4.888270 3.460318e-05\n\n\nA simple run of ?mtcars to pull up the help menu will show you that am=0 for automatics and therefore automatic is the baseline reference level for transmission type and the \\(β_2\\) shown for am is the change in intercept associated with a manual transmission.\nIn the cats data set explored this chapter, Sex is coded with F and M levels and is recognized by R as a factor type variable. You can include factors in your lm function input formula as-is and R will take care of the 0/1 coding behind the scenes for you. The first level of your factor will be treated as the 0 reference level. Here is code that first looks at the levels of Sex, then creates and summarizes the cat heart weight model including indicator interaction:\n\n# look at levels to know which will be reference group\nlevels(cats$Sex)\n\n[1] \"F\" \"M\"\n\n# multiplying the indicator factor creates both an additive beta for\n#   different intercepts and the beta for the slope change\nmod_cats_full&lt;-lm(Hwt~Bwt*Sex, data=cats)\nsummary(mod_cats_full)$coef\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  2.981312  1.8428394  1.617782 0.1079604636\nBwt          2.636414  0.7759022  3.397869 0.0008845733\nSexM        -4.165400  2.0617552 -2.020318 0.0452578391\nBwt:SexM     1.676265  0.8373255  2.001927 0.0472246471\n\n\nAs pointed out earlier, the row labels of SexM and Bwt:SexM also make clear level M is the one that is being treated as a 1, meaning level F must be our 0 reference group. If you want that switched, you can use relevel to change the ordering of the levels of the factor variable.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Zeros and Ones</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#sec-mlev_ind",
    "href": "Chapter6.html#sec-mlev_ind",
    "title": "6  Zeros and Ones",
    "section": "6.5 Multi-level factors in Regression",
    "text": "6.5 Multi-level factors in Regression\nFor categorical variables with more than two categories, we create a separate indicator variable for all categories except one, our baseline reference group. The below illustrates this process with an example from the Cars93 data set from the MASS library. Vehicle type is a factor that takes on six different values: Compact, Large, Midsize, Small, Sporty, and Van. To incorporate this one factor with six levels, we create five new variables: Large, Midsize, Small, Sporty, and Van; each coded as a 0 or a 1. We do not need a variable for Compact - the Compact car level is signaled by not being any of the other types. This is often called one-hot encoding.\n\n\n\nOne-hot encoding example\n\n\nThis is how car type can be included in a regression model - through the addition of five indicators. The order of the levels of car Type is, by default, alphabetical:\n\nlevels(Cars93$Type)\n\n[1] \"Compact\" \"Large\"   \"Midsize\" \"Small\"   \"Sporty\"  \"Van\"    \n\n\nTo build a model of vehicle highway mpg as a function of weight, allowing for different intercepts for each type, the code looks much like what we’ve seen with the lm command before:\n\nmod_car93&lt;-lm(MPG.highway~Weight+Type, data=Cars93)\nsummary(mod_car93)$coef\n\n                Estimate  Std. Error     t value     Pr(&gt;|t|)\n(Intercept) 49.532032100 3.308855496 14.96953619 1.126489e-25\nWeight      -0.006736186 0.001104706 -6.09772023 2.969242e-08\nTypeLarge    2.088508950 1.450279960  1.44007296 1.534773e-01\nTypeMidsize  0.098272245 1.115603939  0.08808883 9.300109e-01\nTypeSmall    1.523993750 1.194802165  1.27551974 2.055600e-01\nTypeSporty  -1.213784862 1.092190088 -1.11133115 2.695231e-01\nTypeVan     -1.839809387 1.600556866 -1.14948080 2.535445e-01\n\n\nThe main difference here is that withe the simple +Type we’ve now added five more β estimates; one for each of the new indicator terms. So the given intercept of 49.532 is the intercept for the baseline Compact car type, then we add 2.0885 to that to get the intercept for Large cars, add 0.0983 to 49.532 to get the intercept for Midsize cars, and so on through to adding -1.8398 to 49.532 for Vans.\nNote in these results none of the p-values in the last column for these Type-related βs indicate the β is significantly different from 0. This does NOT mean that no two vehicle types have significantly different intercepts, it just means that none of the vehicle types have an intercept significantly different from Compact cars. If a different car type were selected to be the reference level, these β estimates and their corresponding p-values would change. Below are the results from the same model but using Van as the baseline reference rather than Compact.\n\nType2&lt;-relevel(Cars93$Type, \"Van\")\nmod_car93_Van&lt;-lm(MPG.highway~Weight+Type2, data=Cars93)\nsummary(mod_car93_Van)$coef\n\n                 Estimate  Std. Error    t value     Pr(&gt;|t|)\n(Intercept)  47.692222713 4.346958390 10.9714008 5.017827e-18\nWeight       -0.006736186 0.001104706 -6.0977202 2.969242e-08\nType2Compact  1.839809387 1.600556866  1.1494808 2.535445e-01\nType2Large    3.928318337 1.349446429  2.9110591 4.586149e-03\nType2Midsize  1.938081632 1.272889330  1.5225846 1.315314e-01\nType2Small    3.363803137 2.055312537  1.6366383 1.053606e-01\nType2Sporty   0.626024525 1.637942803  0.3822017 7.032546e-01\n\n\nThe resulting linear equations are exactly the same. Slope is obviously the same as before, and the intercepts are just expressed with relation to a different baseline. Intercept for Midsize cars for example was originally 49.532 + 0.0983=49.6303 and in the newer model the Midsize intercept is 47.6922 + 1.9381=49.6303. So no real change… just a different comparitor in the baseline position. You’ll also see that with Van as the baseline, the β associated with Large cars now has a p-value below 0.05 indicating it is significantly different from zero.\nFor the full model with interactions between car type and weight the addition of *Type leads to the addition of 10 new β estimates: five for unique intercepts plus five for unique slopes. And just as we saw with the additive indicator model, the values of the β estimates will change depending on what factor level is used as the baseline reference level.\nWith the default Compact as the reference level of type:\n\nmod_car93_full&lt;-lm(MPG.highway~Weight*Type, data=Cars93)\nsummary(mod_car93_full)$coef\n\n                        Estimate   Std. Error    t value     Pr(&gt;|t|)\n(Intercept)         4.781871e+01 10.199629443  4.6882791 1.099859e-05\nWeight             -6.149055e-03  0.003486294 -1.7637797 8.153978e-02\nTypeLarge          -7.310074e+00 16.890945386 -0.4327807 6.663243e-01\nTypeMidsize        -2.810940e+00 12.244349583 -0.2295704 8.190043e-01\nTypeSmall           1.774275e+01 11.678767728  1.5192314 1.325981e-01\nTypeSporty         -3.170950e+00 11.741827087 -0.2700559 7.878041e-01\nTypeVan            -1.878289e+01 27.646402307 -0.6793971 4.988232e-01\nWeight:TypeLarge    2.419781e-03  0.005036981  0.4804029 6.322358e-01\nWeight:TypeMidsize  7.724377e-04  0.004011302  0.1925653 8.477814e-01\nWeight:TypeSmall   -6.858783e-03  0.004257682 -1.6109196 1.110875e-01\nWeight:TypeSporty   6.787099e-04  0.004013261  0.1691168 8.661264e-01\nWeight:TypeVan      4.283285e-03  0.007555762  0.5668898 5.723567e-01\n\n\nAnd with Van as the reference level of type:\n\nmod_car93_full2&lt;-lm(MPG.highway~Weight*Type2, data=Cars93)\nsummary(mod_car93_full2)$coef\n\n                        Estimate   Std. Error    t value  Pr(&gt;|t|)\n(Intercept)         29.035823951 25.696130443  1.1299687 0.2618244\nWeight              -0.001865770  0.006703380 -0.2783327 0.7814662\nType2Compact        18.782885727 27.646402307  0.6793971 0.4988232\nType2Large          11.472811300 29.009700360  0.3954819 0.6935270\nType2Midsize        15.971945860 26.574099715  0.6010343 0.5494954\nType2Small          36.525636382 26.318288216  1.3878424 0.1689910\nType2Sporty         15.611936118 26.346331477  0.5925658 0.5551224\nWeight:Type2Compact -0.004283285  0.007555762 -0.5668898 0.5723567\nWeight:Type2Large   -0.001863504  0.007625761 -0.2443696 0.8075625\nWeight:Type2Midsize -0.003510847  0.006990822 -0.5022080 0.6168838\nWeight:Type2Small   -0.011142068  0.007135048 -1.5615968 0.1222818\nWeight:Type2Sporty  -0.003604575  0.006991947 -0.5155324 0.6075853",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Zeros and Ones</span>"
    ]
  },
  {
    "objectID": "Chapter6.html#on-your-own",
    "href": "Chapter6.html#on-your-own",
    "title": "6  Zeros and Ones",
    "section": "On Your Own",
    "text": "On Your Own\n\nContinuing with the examples above in Section 6.5:\n\nWhat is the linear function to predict the highway fuel efficiency of a large car that weighs 3800 lbs using the model with Compact cars as the reference level?\nConfirm the linear function from (a) still applies if you instead use the model fit with Vans as the reference level.\nIf the reference level is changed to Sporty, what do you expect the TypeSmall and Weight:TypeSmall coefficient estimates to be? Explain the reasoning and answer without running the new model.\nCreate a plot showing the six fit lines predicting highway mpg as a function of vehicle weight - one line per vehicle type. You’ll need the Cars93 data set contained in the MASS library.\nAre all assumptions necessary for inference met? Explain.\n\nConsider all cats with a body weight 3.5kg or less in the cats data frame from the MASS library.\n\nFit a simple linear regression model estimating a cat’s heart weight as a function of body weight. What is the equation of your fitted model?\nAre all assumptions for inference met by your model in (a)? Explain.\nIs there sufficient evidence that the slope is greater than 3? Explain.\nNow fit a model estimating heart weight using body weight interacting with cat sex. What is the equation of the fitted model?\nAre all assumptions for inference met by your model in (d)? Explain.\nIs there sufficient evidence that the slope is greater than 3 for male cats? Is there evidence that the slope is less than 3 for female cats? Use an \\(\\alpha=0.05\\) level and explain your findings thoroughly.\nCreate a plot showing both the male and female cat fit lines. Jitter your body weights and use either plotting characters or color to distinguish which sex applies to each data point.\nWhy is it perfectly reasonable to calculate a prediction interval for the heart weight of a male cat weighing 3.3 kg, but not a good idea to make a prediction interval for a female cat weighing 3.3 kg?\n\nThe crabs data set in the MASS library contains a variety of body measurements on two species of crabs.\n\nEstimate a crab’s body depth as a function of carapace width, including crab species information in your model. What type of model makes the most sense: a model with an additive inclusion of a species indicator, or a model with a multiplicative interaction with a species indicator? Explain your answer.\nWhat is the fitted equation of your selected model?\nDo Orange crabs and Blue crabs have a significantly different intercept in a linear model using carapace width to explain body depth? Explain.\nAre all assumptions for inference met in your model? Explain, using an \\(\\alpha=0.01\\) significance level in any tests you run. If your model needs fixing for inference procedures to be valid, fix it.\nUsing a model that meets all assumptions at the \\(\\alpha=0.01\\) level, fit and interpret a 90% confidence interval for the mean body depth of a Blue crab with a carapace width of 40mm. Then fit and interpret a 90% confidence interval for the mean body depth of an Orange crab with a carapace width of 40mm. How do they compare?\nNow complete a model predicting body depth as a function of carapace width using an indicator term for crab sex. What indicator term is more helpful in predicting body depth: sex or species? Explain.\n\nThe openintro library includes a data frame called fastfood that contains nutrition information for 515 fast food items.\n\nFit a model for food item calories as a function of total fat content. Include restaurant as an interaction term in your model. Which two restaurants have the most similar fit lines? What are those two fit lines?\nWhich restaurant has the steepest slope? Which has the least-steep slope? Are the two slopes significantly different from each other?\nFit a new model that does not include restaurant. Which restaurant from your first model comes closest to matching this restaurant-blind overall fit for calories as a function of fat? Explain.\nCreate a plot showing the raw data with three lines: the fit for the restaurant with the steepest slope, the fit for the restaurant with the least-steep slope, and the fit for the restaurant that most closely matched the restaurant neutral fit. Include a legend that provides the name of the restaurant associated with each model fit.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Zeros and Ones</span>"
    ]
  },
  {
    "objectID": "Chapter7.html",
    "href": "Chapter7.html",
    "title": "7  Go Bigger",
    "section": "",
    "text": "7.1 Multiple Regression\nMultiple linear regression is just like simple linear regression, but instead of using only one independent variable \\((X)\\) to predict the outcome \\((Y)\\), it uses two or more independent variables at the same time. Each independent variable gets its own coefficient ( \\(β_1, β_2, β_3...\\) etc.), representing its unique effect on the dependent variable while keeping the others constant. This allows you to see how each factor influences the outcome, controlling for the others.\nSo for a model with \\(p\\) independent predictor terms, the model now takes the form:\n\\[y=β_0+β_1x_1+β_2x_2+β_3x_3+...+β_px_p+ε\\]As you can see, it gets to be a bit much to write out this long-form notation, hence the beauty and value of the alternative matrix approach. Recall that in addition to describing a linear regression model with a simple algebraic linear equation like \\(y=β_0+β_1x_1+ε\\) our model can also be expressed using matrix notation as:\n\\[\nY=Xβ+ε\n\\]\nWhile up until now we’ve only worked in the situation where \\(X\\) is an \\(n\\times 2\\) matrix with the first column full of 1s to correspond to the intercept term, there is no reason \\(X\\) can’t be \\(n\\times (p+1)\\) where \\(p\\) is any number of parameters we’d like to include in our quest to model \\(Y\\). All we need to do is expand \\(β\\) to be \\((p+1)\\times 1\\) to match.\nThe matrix approach to solve for \\(\\hatβ\\) is still calculated by:\n\\[\n\\hatβ=(X^TX)^{-1}X^TY\n\\]\nThis of course means that the solution for \\(β\\) still requires \\(X^TX\\) have an inverse. No inverse, no \\(\\hat{β}\\), no matter the dimensions.\nWhat does this look like in practice? Consider the hills data set in the MASS library that contains the winning times for 35 Scottish hill races in 1984. These races range from a greulling 16km length with a climb of 7500 meters that takes hours to simpler runs of 3km rising 300 meters that are done in under 20 minutes. Since both distance and elevation change obviously play a role in the challenge presented by a race, it would make sense to include both distance and climb in a model predicting the winning time. We want a model that looks like:\\(winning \\space time=β_0+(β_1\\times distance) + (β_2 \\times climb) + ε\\).\nThe matrix form of our model data is:\n\\[\nY=\\begin{bmatrix}\n    16.083 \\\\\n    48.350 \\\\\n33.650\\\\\n    \\vdots \\\\\n159.833\\\\\n    \\end{bmatrix}\nand \\space\nX=\\begin{bmatrix}\n    1 & 2.5  & 650\\\\\n    1 & 6 & 2500\\\\\n1 & 6 & 900 \\\\\n    \\vdots & \\vdots \\\\\n    1 & 20 & 5000\\\\\n\\end{bmatrix}\n\\tag{7.1}\\]\n\\[\n\\hatβ=(X^TX)^{-1}X^TY=\\begin{bmatrix}\n    -8.992 \\\\\n    6.218 \\\\\n0.011\\\\\n    \\end{bmatrix}\n\\]\nThrough R we can obtain this same fit with:\nlibrary(MASS)\nmod_hills&lt;-lm(time~dist+climb, data=hills)\nsummary(mod_hills)$coef\n\n               Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -8.99203896 4.302734388 -2.089843 4.466516e-02\ndist         6.21795571 0.601147884 10.343471 9.859214e-12\nclimb        0.01104791 0.002050892  5.386882 6.445183e-06\nThe approach to using and interpreting this model is similar to if only one predictor term was used. To use it for prediction, we just plug in values of \\(x_1\\) and \\(x_2\\) and note the resulting \\(y\\). For example, to estimate the winning race time for a new race that’s 10km with an elevation change of 3100m, our our model would suggest \\(-8.992+(6.218\\times 10)+(0.011\\times 3100)=87.436\\) minutes. The intercept tells us the winning race time expected for a hypothetical race that is 0km long with 0m climb: an end almost 9 minutes before the race begins. Not a meaningful answer but not a possible real race either. Our slopes are now in two different dimensions making for a fit plane in 3-dimensional space rather than a fit line. It also makes a plot of the data and the fit hard to do. Interpreting the slopes one by one we see that each additional km of distance adds, on average, about 6.2 minutes to the finishing time and each additional meter of climb adds, on average, 0.011 minutes which is less than one second.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Go Bigger</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#variance-and-inference-for-hatbeta",
    "href": "Chapter7.html#variance-and-inference-for-hatbeta",
    "title": "7  Go Bigger",
    "section": "7.2 Variance and Inference for \\(\\hat\\beta\\)",
    "text": "7.2 Variance and Inference for \\(\\hat\\beta\\)\nFor standard errors in multiple regression we’ll continue with the matrix notation for our model and rely more on your linear algebra skills. Start with:\n\\[\n\\hatβ=(X^TX)^{-1}X^TY\n\\]\nSince our model is based on \\(Y=Xβ+ε\\) we can then plug in \\(Xβ+ε\\) for \\(Y\\) to get:\n\\[\\hatβ=(X^TX)^{-1}X^T(Xβ+ε)\\] which reduces to\n\\[\\hatβ=β+(X^TX)^{-1}X^Tε\\]\nthanks to \\((X^TX)^{-1}X^TX=I\\). Unlike our estimate \\(\\hatβ\\), the parameter \\(β\\) is an unknown constant and therefore has zero variance. This means \\[var(\\hatβ)=var((X^TX)^{-1}X^Tε)\\]\nA property of random vectors says that if \\(A\\) is a matrix and \\(v\\) a random vector, then \\(var(Av)=A var(v)A^T\\). Applying that, we now can express \\(var(\\hatβ)\\) as:\n\\[\nvar(\\hatβ)=(X^TX)^{-1}X^Tvar(ε)X(X^TX)^{-1}\n\\tag{7.2}\\]\nRecall one of our big assumptions is that \\(Var(ε)\\) is constant. As part of that, we denote \\(Var(ε)\\) as simply \\(\\sigma^2I\\) where \\(\\sigma\\) is that constant variance. So now:\n\\[\nvar(\\hatβ)=(X^TX)XT^{-1}\\sigma^2IX(X^TX)^{-1}\n\\]\n\\[\nvar(\\hatβ)=\\sigma^2I(X^TX)^{-1}X^TX(X^TX)^{-1}\n\\]\n\\[\nvar(\\hatβ)=\\sigma^2I(X^TX)^{-1}\n\\]\nThis is what R is doing under the hood when it produces your column of standard errors for the model summary output. This holds whether \\(\\hat\\beta\\) is a simple \\(2\\times1\\) vector with an intercept and slope for a single predictor like we saw back in Chapter 3, or whether \\(\\hat\\beta\\) is a much larger \\((p+1)\\times1\\) vector with an intercept and slopes for \\(p\\) predictors.\nAn example demonstrating this in R on the hill race model:\n\nxmat&lt;-matrix(c(rep(1, nrow(hills)), hills$dist, hills$climb), ncol=3)\n\nsigma&lt;-summary(mod_hills)$sigma\n\nvar_b&lt;-(sigma^2)*solve(t(xmat)%*%xmat)\n\n#diagonal is the variance of each beta, off-diagonals are covariances\nvar_b\n\n             [,1]          [,2]          [,3]\n[1,] 18.513523217 -1.2606623775 -1.580489e-03\n[2,] -1.260662378  0.3613787787 -8.042704e-04\n[3,] -0.001580489 -0.0008042704  4.206156e-06\n\n#but standard errors are square-roots of variances\nc(sqrt(var_b[1,1]), sqrt(var_b[2,2]), sqrt(var_b[3,3]))\n\n[1] 4.302734388 0.601147884 0.002050892\n\n#compare to output from model summary\nsummary(mod_hills)$coef\n\n               Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -8.99203896 4.302734388 -2.089843 4.466516e-02\ndist         6.21795571 0.601147884 10.343471 9.859214e-12\nclimb        0.01104791 0.002050892  5.386882 6.445183e-06\n\n\nWhy does variance of \\(\\hat\\beta\\) matter? Inference in multiple regression is just like inference in the single-predictor simple linear regression with confidence intervals that take the form of:\\[\npoint\\: estimate \\pm multiplier\\times standard \\: error\n\\]and hypothesis test statistics have the general format of:\n\\[\n\\frac{observed \\: value - hypothesized\\: value}{standard\\: error}\n\\]\nso obviously standard error is a key element of both. Confidence intervals for each \\(\\beta\\) coefficient can be estimated from \\(\\hat\\beta_i \\pm t_{\\alpha/2}\\times SE_{\\hat\\beta_i}\\) where \\(t\\) has \\(n-(p+1)\\) degrees of freedom and tests of \\(\\beta_i=0\\) have a test statistic of \\(t=\\hat\\beta_i/SE_{\\hat\\beta_i}\\) with p-values calculated from a \\(t_{n-(p+1)}\\) distribution.\nThe easiest way to do confidence intervals for \\(\\beta\\) in R is still the confint function as seen earlier with single predictor models.\n\nmod_hills&lt;-lm(time~dist+climb, data=hills)\nconfint(mod_hills, level=.9)\n\n                      5 %        95 %\n(Intercept) -16.280392325 -1.70368559\ndist          5.199678069  7.23623334\nclimb         0.007573928  0.01452189\n\n\nFor hypothesis tests on \\(\\beta\\), all of the information you need for the most common test of \\(H_0:\\beta_i=0\\) is given in the model summary output. If instead you are interested in testing \\(H_0:\\beta_i=b\\) for some value \\(b\\), you can simply calculate your test statistic from the standard error as given in the model summary table, and the corresponding p-value using the pt function. For example, if we wanted to test the hypothesis that each additional km of distance added an average of more than 5 minutes to the race time, that would be testing \\(H_0:\\beta_{dist}\\leq5\\) with \\(H_A:\\beta_{dist}&gt;5\\). The test statistic would be calculated as:\n\nsummary(mod_hills)$coef\n\n               Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -8.99203896 4.302734388 -2.089843 4.466516e-02\ndist         6.21795571 0.601147884 10.343471 9.859214e-12\nclimb        0.01104791 0.002050892  5.386882 6.445183e-06\n\n#using estimate and std. error for dist:\nt_stat&lt;-(6.217956-5)/0.601148\n\n#find upper tail since alternative is upper tail\npt(t_stat, nrow(hills)-3, lower.tail = FALSE)\n\n[1] 0.02558215\n\n\nWith an \\(\\alpha=0.05\\), this is sufficient evidence that the mean increase in race time for every additional km of distance is indeed greater than 5 minutes.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Go Bigger</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#variance-and-inference-for-fitted-y",
    "href": "Chapter7.html#variance-and-inference-for-fitted-y",
    "title": "7  Go Bigger",
    "section": "7.3 Variance and Inference for fitted Y",
    "text": "7.3 Variance and Inference for fitted Y\nRecall \\(\\hat y=X\\hat\\beta=Hy\\) where \\(H=X(X^TX)^{-1}X^T\\) and is known as the “hat” matrix. Therefore, \\(var(\\hat y)=Hvar(y)H^T\\) using the same property applied in Equation 7.2. Fun fact about \\(H\\): it is both symmetric and idempotent. This means \\(H=H^T\\) and \\(HH=H\\). Assuming \\(Y\\) is distributed \\(N(X\\beta, \\sigma^2)\\) we can then plug in \\(\\sigma^2I\\) for \\(var(y)\\) yielding:\n\\[\nvar(\\hat y)=\\sigma^2H\n\\]\nwhich means the variance for any specific fitted \\(\\hat {y_i}\\) is:\n\\[\nvar(\\hat y)=\\sigma^2h_{ii}\n\\tag{7.3}\\]\nthe variance of y, multiplied by the \\(i^{th}\\) diagonal of the hat matrix.\nThis is great, but more often in regression we aren’t interested in just the variance of specific fitted values for the data set the model trained on. Usually un-observed \\(x^*\\) values are plugged in to our fitted model to estimate additional \\(y\\) outcomes. When that’s the case there is no \\(h_{ii}\\) that applies. This takes us back to \\(\\hat y=X\\hat\\beta\\) but now instead of the full \\(X\\) we only have the vector \\(x_*\\) so:\n\\[var(\\hat y_*)=\\sigma^2 x_*^T(X^TX)^{-1}x_*\\] This is the same as formula you saw for the simple linear case back in chapter 3: \\[\nSE_{\\bar{Y}|x_*}=S_R\\sqrt{\\frac{1}{n}+\\frac{(x_*-\\bar{x})^2}{\\sum{(x_i-\\bar{x})^2}}}\n\\tag{7.4}\\]\nThe matrix algebra is what’s going on behind the scenes in R no matter how many predictors you have. Writing it out longhand as in Equation 7.4 is much harder to do for multiple predictors, but seeing it that way for the simple case makes it easier to see that when \\(x_*\\) values are close to the mean \\(x\\) values the standard error is smaller making the resulting intervals narrower.\nThe predict function used for confidence and prediction intervals for \\(y_*\\) in the simple linear case continues to do the job for multiple regression models.\nConsider the 90% prediction intervals below for a race with a distance of 7km and climb of 1800m (values near the respective means) and a race with a distance 23km and a climb of 6000m (more unusual but within the range observed in the training set).\n\nnew_races&lt;-data.frame(dist=c(7, 23), climb=c(1800, 6000))\npredict(mod_hills, new_races, interval=\"prediction\", level=.9)\n\n        fit       lwr       upr\n1  54.41989  29.20351  79.63627\n2 200.30840 172.08899 228.52782\n\n\nThe race first race has a narrower interval because it involves \\(x_*\\) nearer the center of the training \\(X\\) values.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Go Bigger</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#transformations",
    "href": "Chapter7.html#transformations",
    "title": "7  Go Bigger",
    "section": "7.4 Transformations",
    "text": "7.4 Transformations\n\n7.4.1 Units change on one term\nWe begin with the simplest type of transformation on a single predictor: a unit change. Maybe meters of climb isn’t the unit of measurement we want to use for climb in our hill racing model. Let’s change it so that we can get the amount of time added per km of climb so that the units on climb will match the units on distance. That means instead of what we saw earlier in Equation 7.1 now:\n\\[\nY=\\begin{bmatrix}\n    16.083 \\\\\n    48.350 \\\\\n33.650\\\\\n    \\vdots \\\\\n159.833\\\\\n    \\end{bmatrix}\nand \\space\nX=\\begin{bmatrix}\n    1 & 2.5  & 0.650\\\\\n    1 & 6 & 2.5\\\\\n1 & 6 & 0.9 \\\\\n    \\vdots & \\vdots \\\\\n    1 & 20 & 5.0\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\hatβ=(X^TX)^{-1}X^TY=\\begin{bmatrix}\n    -8.992 \\\\\n    6.218 \\\\\n11.048\\\\\n    \\end{bmatrix}\n\\]\nOr through R code:\n\nclimb_km&lt;-hills$climb/1000\nmod_hills2&lt;-lm(time~dist+climb_km, data=hills)\nsummary(mod_hills2)$coef\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -8.992039  4.3027344 -2.089843 4.466516e-02\ndist         6.217956  0.6011479 10.343471 9.859214e-12\nclimb_km    11.047910  2.0508916  5.386882 6.445183e-06\n\n\nWhich makes perfect sense and behaves just as we saw in the simple linear example from Chapter 5: we divided our \\(x_2\\) by 1000, so to get the same result from \\(β_2 \\times x_2\\) the value of \\({β_2}\\) would need to increase by a factor of 1000. And now our interpretation includes that the winning time increase by about 11 minutes on average for every km of climb added.\nAny time you want to make a units change on a predictor term the \\(β\\) estimate for the associated variable will simply change by the inverse and all other β will be unaffected. Standard errors will change in scale as well which will leave you with parameter test statistics and p-values that are unchanged. The quality of your fit as measured by \\(R^2\\) won’t change at all either so use whatever scales you’d like predictor by predictor.\n\n\n7.4.2 One or more X terms\n\n\n\n\n\n\n\n\n\n\n\n(a) Displacement Squared\n\n\n\n\n\n\n\n\n\n\n\n(b) Square-root Displacement\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Inverse Displacement\n\n\n\n\n\n\n\n\n\n\n\n(d) Log Displacement\n\n\n\n\n\n\n\nFigure 7.1: Displacement Transformations and Residuals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Model with Displacement^2 plus HP, Axle Ratio, and Wt\n\n\n\n\n\n\nFigure 7.2: Motor Trend Car Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Log(dist) vs. residuals of time~climb\n\n\n\n\n\n\n\n\n\n\n\n(b) Log(climb) vs. residuals of time~dist\n\n\n\n\n\n\n\nFigure 7.3: Scottish Hill Races, Log transformations\n\n\n\n\n\n7.4.3 Transforming Y\nWhy would you want to transform Y instead of one or more of your X terms? Well, what if what you really want for the format of your model is \\(y=e^{\\beta_0+x_1\\beta_1+x_2\\beta_2+\\epsilon}\\) ? You aren’t able to fit that type of relationship directly with the least-squares regression methods we’ve covered, but you could use a log transform on y and then fit \\(log(y)=\\beta_0+x_1\\beta_1+x_2\\beta_2+\\epsilon\\) without any trouble. Generally, any time you think the relationship at hand is of the form \\(y=f(\\beta_0+x_1\\beta_1+x_2\\beta_2+\\epsilon)\\) the solution is to transform your \\(y\\) with \\(f'\\) and fit the \\(f'(y)=\\beta_0+x_1\\beta_1+x_2\\beta_2+\\epsilon\\) model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Go Bigger</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#plotting-for-multiple-regression",
    "href": "Chapter7.html#plotting-for-multiple-regression",
    "title": "7  Go Bigger",
    "section": "7.5 Plotting for Multiple Regression",
    "text": "7.5 Plotting for Multiple Regression\nWhile the basics of fitting a model and using it for inference are largely unchanged from our simple linear regression modeling, expanding beyond one independent predictor variable in our model makes plotting the data more complicated (but also more vital). It’s more complicated because we can generally only plot two continuous dimensions at a time (in a way our brains can understand) simply because our screens (and paper) are two-dimensional, but more vital because there are now even more nuances to the relationships of the variables that we need to know about.\nA good first step is to look at the simple pairwise relationships between each indpendent \\(x\\) predictor and the dependent outcome \\(y\\). This means for the hill data we’d consider the plots shown in Figure 7.4.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distance and Time\n\n\n\n\n\n\n\n\n\n\n\n(b) Climb and Time\n\n\n\n\n\n\n\nFigure 7.4: Scottish Hill Race Data\n\n\n\n\nFrom Figure 7.4 it is clearly seen that both the distance of the race and the elevation climb of the race are reasonably linearly related to our outcome of race record time. \\(R^2\\) values for the two simple linear models tell us that distance alone explains 84.6% of the variability in race time and elevation climb explains 64.8% of race time variability. The next step now is to consider how your independent variables relate to each other.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.5: Scottish Hill Race Distance and Elevation Climb\n\n\n\n\n\n\nThis is the relationship shown in Figure 7.5. As one might expect, race distance and elevation change are related to each other. This makes intuitive sense since longer races have greater opportunity for changes in elevation. You aren’t going to have a race that increases 3000 meters in elevation but is only 1 km long - that’s just too steep to be at all safe (or enjoyable). The \\(R^2\\) of a linear model predicting climb as a function of distance is 42.6%. Why is this important when working to predict time? Well if distance gets us 84.6% of the way to explaining time, and climb gets us 64.8% of the way to explaining time, clearly there must be some overlap in the value they offer since 0.846+0.648&gt;1. The 42.6% is a measure of the overlap.\n\n7.5.1 Added Variable Plot (aka Partial Regression Plot)\nThe Added Variable Plot, also sometimes referred to as a Partial Regression Plot, is a tool for capturing how much value an additional independent variable offers in predicting your dependent variable. The plot contains the residuals from two different linear models. First, the x-axis is the residuals from a model predicting \\(x_2\\) using \\(x_1\\). Then, the y-axis is the residuals of a model predicting y using only only \\(x_1\\). To put this another way, the added variable plot shows the relationship between the variability of your possible new predictor term given the existing predictor compared to the variability that remains in your dependent variable after the first predictor is considered.\n\n\n\n\n\n\n\n\n\n\n\n(a) Adding Climb to Distance Only\n\n\n\n\n\n\n\n\n\n\n\n(b) Adding Distance to Climb Only\n\n\n\n\n\n\n\nFigure 7.6: Added Variable Plots\n\n\n\nFigure 7.6 shows two added variable plots. Figure 7.6 (a) shows the plot assessing the value climb offers in predicting time after distance, and Figure 7.6 (b) assesses the value of adding distance if climb was included first. Equations of the least-squares regression lines are given in the top left corners.\nNote that the equations both have an intercept of zero. That’s because, as you should recall, 1) fitted regression lines always pass through the mean of X and the mean of Y, and 2) our X and Y in these cases are both residuals and the mean residual from a linear regression model is always zero. The slopes from these fits are worth noting though.\nWhen the multiple regression model is fit including distance and climb together, notice the slopes for the distance and climb terms exactly match the slopes from the added variable plots.\n\nmhill&lt;-lm(time~dist+climb, data=hills)\nsummary(mhill)$coef\n\n               Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -8.99203896 4.302734388 -2.089843 4.466516e-02\ndist         6.21795571 0.601147884 10.343471 9.859214e-12\nclimb        0.01104791 0.002050892  5.386882 6.445183e-06\n\n\nNow let’s expand beyond our two-predictor hill race model and look at a larger data set with multiple predictors. Figure 7.7 shows a scatterplot matrix for the first 6 columns of the mtcar data frame we’ve worked with earlier.\n\n\n\n\n\n\n\n\nFigure 7.7: Motor Trend Car Data\n\n\n\n\n\nAcross the top row you can see that all of the available independent variables are strong potential predictors of fuel efficiency (MPG). Some terms may warrant some transformation given apparent curvature in the mpg-displacement and mpg-hp plots, but all indicate they could be useful in a model for MPG. Other scatterplots in the matrix show us that many of these terms are closely related to each other as well so we know there is overlap in the value they offer to predict MPG.\nWe start with the model \\(mpg=β_0+β_1wt+β_2cyl+ε\\) because vehicle weight and the number of engine cylinders should have the strongest influence on fuel efficiency. From there, the question is how much would our model improve if drat, the rear axle ratio, were added? From our initial scatterplot matrix drat’s relationship with mpg appears reasonably linear so there shouldn’t be any transformations to worry about.\n\n\n\n\n\n\n\n\nFigure 7.8: Added Variable Plot, Rear Axle Ratio\n\n\n\n\n\nThe added variable plot for drat given in Figure 7.8 shows that there is no real additional value that drat offers when cylinders and weight are already in the model. The relationship between the residuals from drat modeled by cylinders+weight and the residuals of mpg modeled by cylinders+weight appears completely random; has a best fit line that is virtually horizontal, and a correlation that is zero.\nAfter seeing this plot, it should come as no surprise that when a model is fit predicting MPG as a function of cylinders+weight+axle ratio the axle ratio term is not statistically significant.\n\nm_cars&lt;-lm(mpg~cyl+wt+drat, data=mtcars)\nsummary(m_cars)$coef\n\n               Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) 39.76765828  6.8728698  5.7861795 3.258387e-06\ncyl         -1.50957715  0.4464205 -3.3815136 2.142188e-03\nwt          -3.19473392  0.8293065 -3.8522956 6.235168e-04\ndrat        -0.01620074  1.3230926 -0.0122446 9.903173e-01\n\n\n\nIn R\nFigure 7.4 and Figure 7.8 show added variable plots put together manually using basic plotting functionality in R. There’s also the avPlot function in the car library that will produce added variable plots directly with much less effort. All you need to specify in avPlot is the name of the full fitted model object, and the name of the variable you want to consider as the possible add/drop term. The variable name needs to be put in quotation marks. First, an example using the avPlot function on the hill race data:\nmhills&lt;-lm(time~climb+dist, data=hills)\navPlot(mhills, \"climb\")\navPlot(mhills, \"dist\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Climb plot\n\n\n\n\n\n\n\n\n\n\n\n(b) Distance plot\n\n\n\n\n\n\n\nFigure 7.9: Added Variable Plots\n\n\n\nAnd here is the mtcar model example with identification of unusual points removed:\n\nmcars&lt;-lm(mpg~cyl+wt+drat, data=mtcars)\navPlot(mcars, \"drat\", id=FALSE)\n\n\n\n\n\n\n\nFigure 7.10: Added Variable Plot for Axle Ratio\n\n\n\n\n\nIf you want to quickly produce all added variable plots for a model, you can use the avPlots command. Just add that s to the command name and simply providing the model object will create all plots.\n\navPlots(mhills)\n\n\n\n\n\n\n\n\n\n\n\n7.5.2 Partial Residual Plot\nThere’s another useful plot to know about when working with multiple regression: the Partial Residual Plot. A partial residual plot is another way to look at how each predictor plays a role in a model with multiple independent variables.\nTo investigate \\(x_i\\) using a partial residual plot, first fit a model with all independent terms of interest included. The x-axis is then \\(x_i\\), and the y-axis is the residuals from your full model \\(+β_ix_i\\). Figure 7.11 shows these plots for distance and elevation climb as predictors for race time in the hill race data. The partial residual plots produced by R by default have two helpful overlaid lines: a dashed blue line showing the best linear fit, and a more free-form line showing the pattern followed by the points.\n\n\n\n\n\n\n\n\n\n\n\n(a) Distance\n\n\n\n\n\n\n\n\n\n\n\n(b) Elevation Climb\n\n\n\n\n\n\n\nFigure 7.11: Partial residual plots, Hill Races\n\n\n\nIn this example, the pink line for distance follows the blue dashed line fairly well as shown in Figure 7.11 (a). The curvature shown in the pink line for the partial residual plot for climb in Figure 7.11 (b) however, indicates a transformation on climb might make for an even better model. Specifically, it appears that squaring climb might straighten the relationship out.\nTo investigate this, consider the plots in Figure 7.12 showing (a) the partial residual plot when \\(climb^2\\) is used in the model and (b) the partial residual plot when \\(climb^3\\) in the model. The \\(climb^2\\) model is a definite improvement over the original, but still not quite right. The \\(climb^3\\) model straightens the pink curve further to the point that it aligns nearly perfectly with the dashed blue line.\n\n\n\n\n\n\n\n\n\n\n\n(a) Climb Squared\n\n\n\n\n\n\n\n\n\n\n\n(b) Climb Cubed\n\n\n\n\n\n\n\nFigure 7.12: Partial residual plots, Climb transformed\n\n\n\nUsing this, we can update our model for winning time to use \\(climb^3\\), and then evaluate the fit and model assumptions. Figure 7.13 shows the fitted vs. residual plots for both the basic model with \\(climb\\) directly and the approach and the \\(climb^3\\) model.\n\n\n\n\n\n\n\n\n\n\n\n(a) Dist + Climb model\n\n\n\n\n\n\n\n\n\n\n\n(b) Dist + Climb\\(^3\\) model\n\n\n\n\n\n\n\nFigure 7.13: Fitted vs. Residual Plots\n\n\n\nThis shows there is one extreme outlier with a residual greater than 50 that is present in both approaches. We can also see that the basic model does exhibit some curvature that appears fully corrected in the \\(climb^3\\) model. Homoskedasticity appears much improved in the new model as well.\nHow do partial residual plots look for our more complex mtcars example? Consider Figure 7.14. The plot for weight shows slight, but non-consistent deviation from the line. The plot for cylinders suggests perhaps a simple transformation might improve the fit. And the plot for drat, the measure of the rear axle ratio, definitely shows curvature away from a horizontal line. This horizontal line confirms what we saw with the added variable plot: inclusion of the term won’t significantly help predict MPG at all.\n\n\n\n\n\n\n\n\n\n\n\n(a) Cylinders\n\n\n\n\n\n\n\n\n\n\n\n(b) Weight\n\n\n\n\n\n\n\n\n\n\n\n(c) Rear Axle Ratio\n\n\n\n\n\n\n\nFigure 7.14: Partial Residual Plots\n\n\n\nIf cylinders is transformed to be included in the model as \\(1/cylinders\\) then the updated partial residual plot becomes what is shown in Figure 7.15 (a) and the resulting fitted vs. residual plot for the weight+inverse cylinders model is as shown in Figure 7.15 (b). The \\(R^2\\) of the model fit with cylinders inverted is very slightly increased over the initial model with simply weight+cylinders, 0.837 vs 0.830, making it a matter of scientist discretion if the added complexity of inverting cylinders is worth the minimal fit improvement.\n\n\n\n\n\n\n\n\n\n\n\n(a) Inverse Cylinders Partial Residual Plot,Fitted vs. Residual\n\n\n\n\n\n\n\n\n\n\n\n(b) Inverse Cylinders Partial Residual Plot,Fitted vs. Residual\n\n\n\n\n\n\n\nFigure 7.15: Weight + 1/cylinders\n\n\n\n\nmod_car&lt;-lm(mpg~wt+cyl, data=mtcars)\nmod_car2&lt;-lm(mpg~wt+I(1/cyl), data=mtcars)\n\nc(summary(mod_car)$r.square, summary(mod_car2)$r.square)\n\n[1] 0.8302274 0.8372329\n\n\n\nIn R\nLike the avPlot function, the partial residual plot function is in the car library. crPlot creates the partial residual plots with a minimum of two inputs: the full fitted model object, and the name of the variable you want to focus on. Make sure you put the variable name in quotation marks.\nThe code to produce Figure 7.11 was simply:\n\nmodh&lt;-lm(time~dist+climb, data=hills)\ncrPlot(modh, \"dist\")\ncrPlot(modh, \"climb\")\n\nAs seen with avPlot, there is also a plural version of the crPlot command. crPlots will create all possible partial residual plots for a specified model. Where does the name “crPlot” come from? The y-axis is the component of interest plus the model residual: “cr” is short for “component + residual”.\n\n\n\n7.5.3 Inverse Response Plot\nWhen in a situation where transforming Y is your best course of action, an Inverse Response Plot can help. For this we’ll create two dummy examples so the real relationship is fully known. Let’s create 50 rows of three independent columns, “A”, “B”, and “C”, where each is a created from a random sample from a Uniform[0,5]. Then we’ll make Y1 and Y2 where \\(Y_1=(A+2B+3C)^2+\\epsilon\\) and \\(Y_2=e^{A+B+C}+\\epsilon\\) using a Normal(0, 1) for the \\(\\epsilon\\) noise component.\n\nset.seed(123)\nA&lt;-runif(50, 0, 5)\nB&lt;-runif(50, 0, 5)\nC&lt;-runif(50, 0, 5)\n\nY1&lt;-(A+2*B+3*C)^2 + rnorm(50)\nY2&lt;-exp(A+B+C) + rnorm(50)\n\nIf we start by looking at simple plots of A, B, and C plotted against Y1, and against Y2, it is not immediately clear that transforming our Ys is what needs to happen (see Figure 7.16).\n\n\n\n\n\n\n\n\n\n\n\n(a) A vs. Y1\n\n\n\n\n\n\n\n\n\n\n\n(b) B vs. Y1\n\n\n\n\n\n\n\n\n\n\n\n(c) C vs. Y1\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) A vs. Y2\n\n\n\n\n\n\n\n\n\n\n\n(e) B vs. Y2\n\n\n\n\n\n\n\n\n\n\n\n(f) C vs. Y2\n\n\n\n\n\n\n\nFigure 7.16: Simple Scatter Plots\n\n\n\nIf we fit the basic models for Y1 and Y2 as functions of A, B, and C we get fitted vs. residual plots as shown in Figure 7.17 which clearly indicate there is a strong non-linear relationship at play and a transformation somewhere is warranted.\n\n\n\n\n\n\n\n\n\n\n\n(a) Y1~A+B+C Fitted vs. Residual\n\n\n\n\n\n\n\n\n\n\n\n(b) Y2~A+B+C Fitted vs. Residual\n\n\n\n\n\n\n\nFigure 7.17: Fitted vs. Residuals\n\n\n\nSo let’s look at the partial residual plots for A, B, and C to find out where to transform. These are shown in Figure 7.18 and none look to have significant curvature in their pink line fits.\n\n\n\n\n\n\n\n\n\n\n\n(a) A in model of Y1\n\n\n\n\n\n\n\n\n\n\n\n(b) B in model of Y1\n\n\n\n\n\n\n\n\n\n\n\n(c) C in model of Y1\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) A in model of Y2\n\n\n\n\n\n\n\n\n\n\n\n(e) B in model of Y2\n\n\n\n\n\n\n\n\n\n\n\n(f) C in model of Y2\n\n\n\n\n\n\n\nFigure 7.18: Partial Residual Plots\n\n\n\nIf none of the X predictor terms are the source, consider the response Y as needing the transformation by looking at a plot of the fitted value of Y on the x-axis, vs the observed Y values on the y-axis. This is called the Inverse Response Plot and if Y needs a transformation, the shape of the curve in the inverse response plot will tell you which one you need.\n\n\n\n\n\n\n\n\n\n\n\n(a) Inverse Response Y1\n\n\n\n\n\n\n\n\n\n\n\n(b) Inverse Response Y2\n\n\n\n\n\n\n\nFigure 7.19: Inverse Response Plots\n\n\n\nSure enough, the inverse response plot for Y1 is a curve that follows an \\(x^2\\) curve shape which aligns with us needing to apply a square-root to Y1 to make it linear, and the inverse response plot for Y2 is a curve that follows an exponential curve shape indicating to us that a log transformation for Y2 is the way to go. Both just as our data was created.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Go Bigger</span>"
    ]
  },
  {
    "objectID": "Chapter7.html#sec-multico",
    "href": "Chapter7.html#sec-multico",
    "title": "7  Go Bigger",
    "section": "7.6 Multicollinearity",
    "text": "7.6 Multicollinearity\nWhen two or more independent variables are highly correlated with each other, your multiple regression model suffers from a problem known as multicollinearity. At issue is the fact that this correlation undermines the assumption that independent random variables are in fact independent, which is critical for accurately estimating their individual impacts on the dependent variable you’re predicting.\nSometimes multicollinearity arises because of a choice the modeler has made that is easily corrected. For example, in a health study rather than including both the weight of a patient and the patient body mass index, you can just select one as a measure of patient size and move on. Often though the choice isn’t quite so simple as the variables might very naturally move together while at the same time each providing valuable insight.\nThe problem is that having highly correlated independent variables makes the standard errors of the estimated regression coefficients increase. This means your confidence intervals are wider and you’re less able to identify when a term is significantly different from zero or any other key value of interest. Multicollinearity can also lead to coefficients changing notably, even changing signs unexpectedly, with relatively minor changes to the data or the model.\nFor an example, let’s look at the fgl data frame from the MASS library. This data frame considers the refractive index of 214 fragments of six different types of glass and their oxide makeup. We’ll start with the full model that includes all available predictors of refractive index:\n\nmod_glass&lt;-lm(RI~., data=fgl)\nsummary(mod_glass)\n\n\nCall:\nlm(formula = RI ~ ., data = fgl)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2131 -0.3786 -0.0186  0.3697  4.0317 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.68095   67.73022   0.172 0.863248    \nNa           0.60928    0.66840   0.912 0.363100    \nMg           1.32017    0.67269   1.963 0.051086 .  \nAl          -0.92790    0.70416  -1.318 0.189098    \nSi          -0.61637    0.68460  -0.900 0.369022    \nK            0.74007    0.68789   1.076 0.283286    \nCa           2.47150    0.66700   3.705 0.000273 ***\nBa           1.97569    0.70198   2.814 0.005374 ** \nFe           0.36329    0.73879   0.492 0.623442    \ntypeWinNF    0.09996    0.17352   0.576 0.565209    \ntypeVeh     -0.88579    0.26111  -3.392 0.000835 ***\ntypeCon      0.39910    0.39687   1.006 0.315810    \ntypeTabl     0.39263    0.42512   0.924 0.356822    \ntypeHead     1.58234    0.43890   3.605 0.000394 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9503 on 200 degrees of freedom\nMultiple R-squared:  0.9081,    Adjusted R-squared:  0.9021 \nF-statistic:   152 on 13 and 200 DF,  p-value: &lt; 2.2e-16\n\n\nThis model summary tells us a few things. First, you’ll notice that many of our independent variables do not appear significant - many p-values are well above any reasonable \\(\\alpha\\) level. You can also see that two glass types that are most different from each other are vehicle window glass, with a -0.88579 slope and vehicle headlight glass with a 1.58 slope. This kind of makes sense that they would differ from the other types because the thick safety glass of vehicle windows is very different from the type of glass used to make regular home windows or tableware. Similarly, glass for headlights is also very different from your drinking glass but in an entirely different way. These observations aside though, the \\(R^2=0.9081\\) does indicate the model does a fairly good job at predicting the refractive index of glass given the glass components.\nNow consider a reduced model that reduces glass type down to only three levels: vehicle, headlight, or other, and only considers the Al, Si, Ca, and Ba elements.\n\nvehicle&lt;-as.factor(fgl$type==\"Veh\")\nheadlight&lt;-as.factor(fgl$type==\"Head\")\nmod_glass2&lt;-lm(RI~Al+Si+Ca+Ba+vehicle+headlight, data=fgl)\nsummary(mod_glass2)\n\n\nCall:\nlm(formula = RI ~ Al + Si + Ca + Ba + vehicle + headlight, data = fgl)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3816 -0.4509  0.0376  0.4418  3.9111 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   108.21998    7.34898  14.726  &lt; 2e-16 ***\nAl             -2.19413    0.17487 -12.547  &lt; 2e-16 ***\nSi             -1.61520    0.09887 -16.336  &lt; 2e-16 ***\nCa              1.39476    0.05230  26.671  &lt; 2e-16 ***\nBa              0.89572    0.20551   4.359 2.06e-05 ***\nvehicleTRUE    -0.93091    0.26208  -3.552 0.000473 ***\nheadlightTRUE   0.63323    0.31455   2.013 0.045397 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.015 on 207 degrees of freedom\nMultiple R-squared:  0.8914,    Adjusted R-squared:  0.8883 \nF-statistic: 283.3 on 6 and 207 DF,  p-value: &lt; 2.2e-16\n\n\nOur intercept is wildly different now. What do you notice about the coefficients for Al, Si, Ca and Ba for the second model compared to the first? The Al and Si slopes are more than twice what they were in the earlier model while the Ca and Ba ones decreased significantly. And look at the standard errors on those new coefficient estimates. Our large model had standard errors that were all in the .66 to .74 range, but now they are much much smaller. Si went from having a standard error of 0.6846 to one of only .09887. The model coefficient of determination, \\(R^2\\), is only slightly decreased and still strong at 0.8941. What is going on here? Multicollinearity. Here is the correlation matrix (rounded to three decimal places) on our continuous independent variables:\n\nround(cor(fgl[,2:9]),3)\n\n       Na     Mg     Al     Si      K     Ca     Ba     Fe\nNa  1.000 -0.274  0.157 -0.070 -0.266 -0.275  0.327 -0.241\nMg -0.274  1.000 -0.482 -0.166  0.005 -0.444 -0.492  0.083\nAl  0.157 -0.482  1.000 -0.006  0.326 -0.260  0.479 -0.074\nSi -0.070 -0.166 -0.006  1.000 -0.193 -0.209 -0.102 -0.094\nK  -0.266  0.005  0.326 -0.193  1.000 -0.318 -0.043 -0.008\nCa -0.275 -0.444 -0.260 -0.209 -0.318  1.000 -0.113  0.125\nBa  0.327 -0.492  0.479 -0.102 -0.043 -0.113  1.000 -0.059\nFe -0.241  0.083 -0.074 -0.094 -0.008  0.125 -0.059  1.000\n\n\nLooks like Mg is fairly strongly correlated with a few of the other terms. That’s why the model wasn’t hurt much by it’s removal.\nIn addition to checking correlation matrices, the variance inflation factor is a good metric for flagging where multicollinearity is likely an issue. The variance inflation factor is defined as:\n\\[\nVIF_i=\\frac{1}{1-R_i^2}\n\\]\nWhere \\(R_i^2\\) is the \\(R^2\\) from a regression model predicting variable \\(i\\) using all other predictors. So to examine the influence of Mg in our model we would look at how well Mg could be modeled linearly as a function of NA, Al, Si, K, Ca, Ba, Fe, and our two glass types:\n\nmod_Mg&lt;-lm(Mg~Na+Al+Si+K+Ca+Ba+Fe+vehicle+headlight, data=fgl)\nsummary(mod_Mg)$r.square\n\n[1] 0.995202\n\n\nSeems Mg can be almost entirely explained by the other terms. With a high \\(R_{Mg}^2\\), that means \\(VIF_{Mg}\\) will be large. In fact, \\(\\frac{1}{1-.9952}=208.42\\). Values of \\(VIF&gt;5\\) are generally considered to be worth a closer look.\n\nIn R\nFortunately, R will pretty easily provide you with all VIF metrics when provided with a full model under consideration. In the car library lives the vif function. Running all VIFs for our glass model is as simple as:\n\nmod_glass&lt;-lm(RI~Na+Mg+Al+Si+K+Ca+Ba+Fe+vehicle+headlight, data=fgl)\nvif(mod_glass)\n\n        Na         Mg         Al         Si          K         Ca         Ba \n 65.145740 208.421848  27.958365  62.254625  44.602864 200.586359  27.131346 \n        Fe    vehicle  headlight \n  1.218031   1.053694   3.388083 \n\n\nTaking away Mg as the term with the largest VIF, our new model with VIFs looks like:\n\nmod_glass2&lt;-lm(RI~Na+Al+Si+K+Ca+Ba+Fe+vehicle+headlight, data=fgl)\nvif(mod_glass2)\n\n       Na        Al        Si         K        Ca        Ba        Fe   vehicle \n 2.038928  1.882341  1.551700  1.776061  1.648278  2.245932  1.092993  1.047940 \nheadlight \n 3.277242 \n\n\nShowing with that one omission, the bulk of our multicollinearity issue has been cleared up.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Go Bigger</span>"
    ]
  },
  {
    "objectID": "Chapter8.html",
    "href": "Chapter8.html",
    "title": "8  Building and Selection",
    "section": "",
    "text": "8.1 Measures of model strength",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building and Selection</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#measures-of-model-strength",
    "href": "Chapter8.html#measures-of-model-strength",
    "title": "8  Building and Selection",
    "section": "",
    "text": "8.1.1 Test Set and Training Set\nIt may seem counter-intuitive but sometimes the best choice you can make is to NOT use all of your data when fitting your model. If you begin by splitting your data into two, you can fit the model on one set, known as the “training set”, and then test how well the model performs on the remaining “test set”. This helps protect you from creating a model that is so well tuned to your specific data that it isn’t generalizable to the full population and won’t produce helpful predictions.\nDiscretion is allowed when deciding the relative sizes of the training and testing sets. At most you would split the data 50/50 putting half in each set randomly. In some cases with limited data you might shift that to putting as much as 80% in the training set and holding out the remaining 20% for a sanity check.\n\n\nIn R\nBelow is an example using the reduced glass refraction index data just seen at the end of Section 7.6. The fgl glass data frame has 214 rows to start with.\n\nlibrary(MASS)\nlibrary(caret)\n\n\n# set seed for reproducability\nset.seed(1234)\n\n# randomly select 150 rows for the training set\ntrain_index&lt;-sample(1:214, 150)\n\n# put row numbers not selected into the test set\ntest_index&lt;-(1:214)[!is.element(1:214, train_index)]\n\n# create the two data sets\nglass_train&lt;-fgl[train_index,]\nglass_test&lt;-fgl[test_index,]\n\n# fit the model with the training set\nglass_model&lt;-lm(RI~Al+Si+Ca+Ba, data=glass_train)\n\n# find mean squared error in training set\nmean(glass_model$residuals^2)\n\n[1] 1.160116\n\n# find mean squared error in test set\nfit_values&lt;-predict(glass_model, glass_test)\ntest_errors&lt;-fit_values-glass_test$RI\nmean(test_errors^2)\n\n[1] 0.9209567\n\n# if you want a formal test, can do t-test of errors\nt.test(glass_model$residuals^2, test_errors^2)\n\n\n    Welch Two Sample t-test\n\ndata:  glass_model$residuals^2 and test_errors^2\nt = 0.58113, df = 152.37, p-value = 0.562\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5738999  1.0522177\nsample estimates:\nmean of x mean of y \n1.1601156 0.9209567 \n\n\nThis shows that there is no evidence our model fitted using the training data doesn’t perform just as well on our test data. With this we can be confident that future refraction index values predicted using the model are reasonably accurate.\n\n\n8.1.2 K-fold cross-validation\nWhat if we just got lucky with our test set and training set sampling? K-fold cross-validation takes the idea of test/training set validation to the next level. Instead of separating the data into just two parts, a test set and a training set, k-fold cross validation splits the data into k parts. Often k is set to 5 or 10, but there’s no reason you can’t select any other number provided you have enough rows to reasonably split.\nOnce the data is split, the model is then fit k times; each time using k-1 groups as the training set and the held out group as the test set. Figure 8.1 below illustrates what would happen with 4-fold cross validation on a data set with only 8 rows.\nFirst the 8 rows are randomly grouped into four groups of 2. The model is then run 4 times. In the example, rows 1 and 7 are in fold 1. The first time the model is fit, fold 1 points are set aside for testing and the model is trained on rows 2,3,4,5,6, and 8. For model 2, our second fold containing rows 2 and 3 are set aside for testing, and the model is trained using rows 1,4,5,6,7, and 8. In the end you’re left with four examples of how well the model fit using the approach you’ve selected fits data that was not involved in the fitting.\n\n\n\n\n\n\nFigure 8.1: Four-fold cross validation diagram\n\n\n\n\n\nIn R\nContinuing with the glass refractive index scenario, we’ll complete 10-fold cross validation of a model containing Al, Si, Ca, and Ba as linear predictors using the train function in the caret library.\n\n# set seed for reproducability\nset.seed(1234)\n\n# Define the parameters. Method is cross validation, k=10.\ntrain_params &lt;- trainControl(method = \"cv\", number = 10)\n\n# Train your k models.  Specify lm is type of model,\n#     and put in the training parameters set above\nglass_models &lt;- train(RI ~ Al+Si+Ca+Ba, data=fgl,\n  method = \"lm\",trControl = train_params)\n\n# View the results of the cross-validation\nglass_models$resample\n\n        RMSE  Rsquared       MAE Resample\n1  0.4672234 0.9570430 0.3934590   Fold01\n2  0.6537417 0.9465408 0.5239035   Fold02\n3  1.0238911 0.8434257 0.6182354   Fold03\n4  1.4876875 0.7103225 1.0625717   Fold04\n5  1.1690601 0.8168152 0.9380207   Fold05\n6  1.0556602 0.8918505 0.8826193   Fold06\n7  0.5904965 0.9885397 0.4773744   Fold07\n8  1.2919068 0.7462664 0.7501642   Fold08\n9  1.3814389 0.7731487 0.8307277   Fold09\n10 1.0430018 0.8797080 0.7720872   Fold10\n\n\nThese results indicate that over the 10 folds, the worst RMSE (square-root of mean squared error) among test holdouts happened when Fold4 was withheld from training. In this worst-case, the RMSE was 1.487. In the best-case, when Fold 1 was withheld, the RMSE was only 0.467. These are RMSEs calculated on the fold that was used as testing, not the folds that were used in training. If you prefer, the \\(R^2\\) values for the testing holdouts, and the MAE (mean absolute error) for the testing holdouts are also given for each of the k folds. Some variability over the folds is to be expected, and these ten look reasonably consistent indicating the RI~Al+Si+Ba+Ca model approach is not over-fitting and should apply generally to the population.\n\n\n8.1.3 Adjusted \\(R^2\\)\nBack in Chapter 2 we defined \\(R^2\\), the coefficient of determination, as:\n\\[R^2=1-\\frac{SSError}{SSTotal}=\\frac{SSReg}{SSTotal}\\]\nSo whenever the sum of squared residuals, \\(SSError\\), decreases, the \\(R^2\\) goes up. In the world of multiple regression this creates a bit of a problem because every time you add a new term to the regression model the sum of squared residuals will go down whether the new term is really helpful or not. That’s why judging two models against each other using \\(R^2\\) just won’t work. Enter adjusted \\(R^2\\).\n\\[R_{adj}^2=1-\\frac{SSError/(n-p-1)}{SSTotal/(n-1)}\\]\nWhere \\(n\\) is the number of observations and \\(p\\) is the number of predictor variables. This makes it so just adding terms won’t necessarily increase your \\(R^2\\) - it will only improve if the increase in \\(p\\) is adding real value to your model.\nGetting this adjusted \\(R^2\\) from R is simple. The basic model summary output includes this adjusted \\(R^2\\) right after the \\(R^2\\) value you’ve already come to know and love. Below is a highlighted example from the reduced glass refraction index model. The basic \\(R^2\\) with no adjustment is given as 0.8914, and the \\(R_{adj}^2\\) is given as 0.8883. Since the two are close in value, that is an indication that every term in the linear model is providing additional insight into the value of refraction index.\n\n\n\n\n\n\n\n8.1.4 AIC and BIC\nAIC (Akaike’s Information Criterion) and BIC (Bayes Information Criterion) are two metrics used to compare the relative strength of competing models. The specific values of AIC and BIC have no real meaning, but the relative size of AIC and BIC of two or more possible models can help you decide which model to use.\nCalculation of both AIC and BIC includes the log of the likelihood of the model under review. What is a model’s likelihood? Simply put: it’s the probability of seeing the observed data if the fitted model is the truth. The process to figuring out this likelihood is beyond our scope so we’ll rely on R to do the work for us, but it’s good to have a sense of what it’s measuring.\n\\[\nAIC=2(p+2)-2ln(\\hat L)\n\\]\nand\n\\[\nBIC=ln(n)(p+2)-2ln(\\hat L)\n\\]\nwhere \\(\\hat L\\) is the likelihood, \\(p\\) is the number of independent predictors in your model, and \\(n\\) is the number of observations. Notice the two are quite similar; the only difference is AIC has the number of estimated parameters multiplied by 2, while BIC penalizes larger parameter counts by a factor of \\(ln(n)\\).\nFor both AIC and BIC, the model with the lower value is the better choice. The stats library in R (a library usually installed by default) contains both an AIC and a BIC function for calculating these measures.\nHere is an example considering two different models for glass refraction index; one with glass type indicators and one without:\n\nmod_glass2&lt;-lm(RI~Na+Al+Si+K+Ca+Ba+Fe+vehicle+headlight, data=fgl)\n\nmod_glass3&lt;-lm(RI~Na+Al+Si+K+Ca+Ba+Fe, data=fgl)\n\nAIC(mod_glass2)\n\n[1] 598.3572\n\nAIC(mod_glass3)\n\n[1] 625.5007\n\nBIC(mod_glass2)\n\n[1] 635.383\n\nBIC(mod_glass3)\n\n[1] 655.7945\n\n\nIn this case AIC and BIC agree that the model including glass type is better than the model without type. It is not always the case that AIC and BIC will agree though. That’s why for scientific rigor you should select which measure you want to use prior to calculating them.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building and Selection</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#model-building",
    "href": "Chapter8.html#model-building",
    "title": "8  Building and Selection",
    "section": "8.2 Model Building",
    "text": "8.2 Model Building\nWith \\(R_{adj}^2\\), AIC, and BIC we now have three different approaches to measure the quality of a model. This opens the door to ways we can build a multiple regression model piece by piece.\n\n8.2.1 Forward Selection\nThe forward selection process is exactly what is sounds like: adding one variable at a time. At each step of this approach, each potential predictor is considered and one that improves \\(R_{adj}\\) the most, or decreases AIC or BIC the most, is added to the model. The process continues until adding one more term does not improve your performance metric.\n\n\n8.2.2 Backward Elimination\nThis approach to model building is also exactly what it sounds like. With backward selection you begin with a large model including all possible predictors and then remove them one by one. Which one should be removed at each step can be decided based on which has the highest p-value for the test on \\(\\hat\\beta\\), by which one, when removed, increases \\(R_{adj}\\) the most, or by which one, when removed, decreases AIC or BIC the most. The process stops when removing any additional terms will only hurt the model quality.\n\n\n8.2.3 Step-wise\nThe step-wise approach is a combination of forward selection and backward elimination. In this algorithm, the model can either grow or shrink but only ever one term at a time. The modeler uses discretion to add or subtract terms at each iteration, possibly alternating between adding and subtracting, but other paterns are possible as well. The process stops when it’s found that neither adding nor subtracting a term will improve the decision metric (\\(R_{adj}^2\\), AIC or BIC).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building and Selection</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#building-examples-in-r",
    "href": "Chapter8.html#building-examples-in-r",
    "title": "8  Building and Selection",
    "section": "8.3 Building Examples in R",
    "text": "8.3 Building Examples in R\n\n8.3.1 Forward\nIn the openintro library you will find a data frame called gifted that contains information on 36 gifted children. Along with their scores for a test of analytical skills, the data frame contains each child’s mother’s and father’s IQs, the age at which the child first said “mommy” or “daddy”, the age when the child first counted to 10 successfully, and the average number of hours per week the child reads with a parent, watches education TV, and watches cartoons.\nTo build a model working with forward selection and the AIC metric, we can use the step function built into the stats library as follows:\n\n# specify an empty intercept-only model to start\nmod_start&lt;-lm(score~1, data=gifted)\n\n# define the largest model possible as an upper bound on scope \nmod_full&lt;-lm(score~., data=gifted)\n\n# specifying k=2 indicates using AIC.  Each step will be printed out.\nstep(mod_start, scope=list(lower=mod_start, upper=mod_full), \n     direction=\"forward\", k=2)\n\nStart:  AIC=111.33\nscore ~ 1\n\n           Df Sum of Sq    RSS     AIC\n+ motheriq  1   244.838 505.47  99.111\n+ count     1   222.211 528.09 100.687\n+ read      1   206.958 543.35 101.712\n+ edutv     1   102.861 647.44 108.023\n+ speak     1    53.846 696.46 110.650\n+ cartoons  1    45.074 705.23 111.100\n&lt;none&gt;                  750.31 111.331\n+ fatheriq  1    26.542 723.76 112.034\n\nStep:  AIC=99.11\nscore ~ motheriq\n\n           Df Sum of Sq    RSS     AIC\n+ read      1   227.205 278.26  79.622\n+ count     1   211.162 294.31  81.640\n+ speak     1    38.747 466.72  98.240\n+ fatheriq  1    30.712 474.76  98.854\n+ edutv     1    27.814 477.65  99.073\n&lt;none&gt;                  505.47  99.111\n+ cartoons  1     2.272 503.20 100.949\n\nStep:  AIC=79.62\nscore ~ motheriq + read\n\n           Df Sum of Sq    RSS    AIC\n+ fatheriq  1    43.605 234.66 75.486\n&lt;none&gt;                  278.26 79.622\n+ speak     1    11.834 266.43 80.057\n+ edutv     1     5.904 272.36 80.850\n+ count     1     3.617 274.65 81.151\n+ cartoons  1     0.563 277.70 81.549\n\nStep:  AIC=75.49\nscore ~ motheriq + read + fatheriq\n\n           Df Sum of Sq    RSS    AIC\n&lt;none&gt;                  234.66 75.486\n+ speak     1   12.5784 222.08 75.503\n+ edutv     1    9.7910 224.87 75.952\n+ count     1    4.3398 230.32 76.814\n+ cartoons  1    0.8398 233.82 77.357\n\n\n\nCall:\nlm(formula = score ~ motheriq + read + fatheriq, data = gifted)\n\nCoefficients:\n(Intercept)     motheriq         read     fatheriq  \n    44.3656       0.4282      12.7663       0.3215  \n\n\nNow let’s examine each step of the output above. We specified in the step command that we wanted to begin with an empty intercept-only model and that the biggest model that could be considered is the full model with all predictors available in the gifted data frame.\nAt the start, our intercept-only model has an AIC of 111.33, and the first table in the output tells us the RSS (Residual Sum of Squares or SSError) and AIC for models that include the intercept plus each possible term one by one; each term gets a new row in the table. In the first round, R found that adding motheriq to the intercept produced an AIC of 99.111, adding count led to an AIC of 100.687, adding read led to an AIC of 101.712… and so on through to adding fatheriq making for an AIC of 112.034, an actual increase over the baseline start of 111.33. Since lower AIC is better, motheriq gets added to the model, and the second round begins.\nStep two begins with the score~motheriq model. An intercept is included but not explicitly stated here. The table shows the results from adding each possible remaining predictor to the score to the model and finds that adding in read is the most helpful and will decrease the model AIC to 79.622.\nTwo more rounds of forward selection continue. The third round examines adding another term to the score~motheriq+read model, and selects adding in fatheriq. Round four looks to add to the score~motheriq+read+fatheriq model, but discovers adding any of the remaining possible terms will result in a higher AIC so the search ends and the score~motheriq+read+fatheriq is returned as the final model.\n\n\n8.3.2 Backward\nWhat if the backward elimination approach is taken? Same gifted data as used in the forward selection example, but now let’s start with the full model and remove terms one at a time until all \\(\\hat\\beta\\) p-values are below \\(\\alpha=0.05\\).\nWe’ll start by checking the VIFs to determine if high multicollinearity should be taken care of first.\n\nvif(mod_full)\n\nfatheriq motheriq    speak    count     read    edutv cartoons \n1.196214 1.172998 1.182755 6.886769 6.896808 8.214684 8.373075 \n\nsummary(mod_full)$coef\n\n              Estimate  Std. Error    t value     Pr(&gt;|t|)\n(Intercept) 75.5084856 24.02617507  3.1427593 3.934153e-03\nfatheriq     0.2524925  0.13756072  1.8354988 7.707469e-02\nmotheriq     0.4000681  0.07290507  5.4875205 7.328886e-06\nspeak        0.1876429  0.14766697  1.2707168 2.142862e-01\ncount        0.2064897  0.26631214  0.7753673 4.446222e-01\nread         7.5440549  5.58640059  1.3504321 1.876921e-01\nedutv       -4.2024429  2.24503495 -1.8718831 7.170324e-02\ncartoons    -3.3389901  2.01808208 -1.6545363 1.091872e-01\n\n\nSince time spent watching educational tv and time spent watching cartoons are strongly correlated with each other (correlation of -0.9234), it is not surprising that the two have high VIFs. For step one, we’ll remove cartoons because of the two, it is the one with the higher \\(\\hat\\beta\\) p-value.\n\nmod_2&lt;-lm(score~fatheriq+motheriq+speak+count+read+edutv, data=gifted)\nvif(mod_2)\n\nfatheriq motheriq    speak    count     read    edutv \n1.016676 1.154849 1.158823 6.650745 6.759192 1.217195 \n\nsummary(mod_2)$coef\n\n              Estimate  Std. Error    t value     Pr(&gt;|t|)\n(Intercept) 49.8812173 18.90916625  2.6379385 0.0132717059\nfatheriq     0.3406675  0.13056188  2.6092414 0.0142021633\nmotheriq     0.3850639  0.07447438  5.1704205 0.0000157951\nspeak        0.2223968  0.15048032  1.4779129 0.1502106664\ncount        0.2880610  0.26943473  1.0691311 0.2938293264\nread         6.2384283  5.69364778  1.0956822 0.2822341840\nedutv       -0.7741723  0.88969865 -0.8701512 0.3913636419\n\n\nIn our model without cartoons, read and count still have VIFs greater than 5. They are strongly positively correlated (0.9103) so even though edutv has the highest p-value, I’ll drop count this round since it has the higher p-value between count and read.\n\nmod_3&lt;-lm(score~fatheriq+motheriq+speak+read+edutv, data=gifted)\nvif(mod_3)\n\nfatheriq motheriq    speak     read    edutv \n1.016335 1.136772 1.054395 1.072252 1.190811 \n\nsummary(mod_3)$coef\n\n              Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 47.0554351 18.76811965  2.507200 1.781474e-02\nfatheriq     0.3381140  0.13085088  2.583965 1.488079e-02\nmotheriq     0.3950256  0.07406516  5.333487 9.083768e-06\nspeak        0.1741009  0.14388176  1.210028 2.357167e-01\nread        11.8220180  2.27313104  5.200764 1.321418e-05\nedutv       -0.9142163  0.88209867 -1.036411 3.082929e-01\n\n\nHigh multicollinearity is all cleared up now so the focus can be solely on p-values. Now we’ll drop edutv for having the highest p-value.\n\nmod_4&lt;-lm(score~fatheriq+motheriq+speak+read, data=gifted)\nsummary(mod_4)$coef\n\n              Estimate  Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 42.7092528 18.31549501 2.331865 2.637951e-02\nfatheriq     0.3242503  0.13032091 2.488091 1.842563e-02\nmotheriq     0.4207323  0.06987185 6.021485 1.154340e-06\nspeak        0.1898180  0.14325136 1.325070 1.948325e-01\nread        12.2089372  2.24494494 5.438413 6.108775e-06\n\n\nNext to drop is speak.\n\nmod_5&lt;-lm(score~fatheriq+motheriq+read, data=gifted)\nsummary(mod_5)$coef\n\n              Estimate  Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 44.3655659 18.48732491 2.399783 2.239759e-02\nfatheriq     0.3214775  0.13183396 2.438503 2.047870e-02\nmotheriq     0.4282474  0.07045893 6.077973 8.662970e-07\nread        12.7663191  2.23107427 5.722050 2.430603e-06\n\n\nAnd now we’ll stop because all remaining terms have a p-value smaller than our \\(\\alpha=0.05\\) threshold. The final model of score~fatheriq+motheriq+read matches the one obtained using AIC and forward selection, but that need not be the case. In fact, let’s look at what happens if we were to perform our backward elimination without multicollinearity checks along the way.\nOur first step would have us remove count because it’s p-value of 0.4446 is the highest in mod_full. So step two would now be:\n\nmod_2&lt;-lm(score~fatheriq+motheriq+speak+read+edutv+cartoons, data=gifted)\nsummary(mod_2)$coef\n\n              Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 75.7756404 23.85794658  3.176117 3.526914e-03\nfatheriq     0.2430752  0.13607806  1.786292 8.451231e-02\nmotheriq     0.4082659  0.07163665  5.699121 3.646928e-06\nspeak        0.1511945  0.13901910  1.087581 2.857370e-01\nread        11.5226177  2.19353467  5.252991 1.255677e-05\nedutv       -4.5968140  2.17157275 -2.116813 4.297596e-02\ncartoons    -3.6286687  1.96951508 -1.842417 7.566028e-02\n\n\nThen speak is removed…\n\nmod_3&lt;-lm(score~fatheriq+motheriq+read+edutv+cartoons, data=gifted)\nsummary(mod_3)$coef\n\n              Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 79.0467761 23.73960954  3.329742 2.312701e-03\nfatheriq     0.2373610  0.13639031  1.740307 9.205533e-02\nmotheriq     0.4121118  0.07176701  5.742357 2.870557e-06\nread        11.9013189  2.17231039  5.478646 6.030978e-06\nedutv       -4.8881415  2.16154516 -2.261411 3.114315e-02\ncartoons    -3.8202332  1.96759143 -1.941578 6.163171e-02\n\n\nThen fatheriq is removed…\n\nmod_4&lt;-lm(score~motheriq+read+edutv+cartoons, data=gifted)\nsummary(mod_4)$coef\n\n               Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 112.5034019 14.37637560  7.825575 7.855003e-09\nmotheriq      0.4177214  0.07400329  5.644633 3.383735e-06\nread         11.6036137  2.23529988  5.191077 1.241780e-05\nedutv        -6.0534053  2.12140697 -2.853486 7.638249e-03\ncartoons     -5.1125813  1.88075148 -2.718372 1.064514e-02\n\n\nAnd now since all p-values are below our 0.05 threshold we would stop with the score~motheriq+read+edutv+cartoons model. Checking at this stopping point for multicollinearity would make us then question having both edutv and cartoons in the model.\n\nvif(mod_4)\n\nmotheriq     read    edutv cartoons \n1.143219 1.044483 6.938065 6.878850 \n\nmod_5&lt;-lm(score~motheriq+read+edutv, data=gifted)\nsummary(mod_5)\n\n\nCall:\nlm(formula = score ~ motheriq + read + edutv, data = gifted)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6035 -1.7634 -0.0054  1.4456  6.7931 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 87.74186   12.18228   7.202 3.52e-08 ***\nmotheriq     0.40044    0.08076   4.959 2.24e-05 ***\nread        11.99889    2.44313   4.911 2.57e-05 ***\nedutv       -0.79303    0.95214  -0.833    0.411    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.917 on 32 degrees of freedom\nMultiple R-squared:  0.637, Adjusted R-squared:  0.603 \nF-statistic: 18.72 on 3 and 32 DF,  p-value: 3.405e-07\n\n\nRemoving cartoons as having the higher of the two p-values leaves us with score~motheriq+read+edutv, a model that’s a bit different from the one found through forward selection with AIC and backward elimination with multicollinearity considered up front.\n\n\n8.3.3 Step-wise\nLastly we’ll use the same step function used in our forward selection example to perform step-wise model selection using BIC. The scope of models to be considered is\n\n# we begin with the mod_5 at the end of the backward elimination example.\n# scope ranges from intercept-only to the full model as before\n# specifying k=ln(n) indicates using BIC to make decisions\nstepwise&lt;-step(mod_5, scope=list(lower=mod_start, upper=mod_full), \n     direction=\"both\", k=log(nrow(gifted)))\n\nStart:  AIC=87.18\nscore ~ motheriq + read + edutv\n\n           Df Sum of Sq    RSS     AIC\n+ cartoons  1    52.426 219.93  83.070\n+ fatheriq  1    47.491 224.87  83.869\n- edutv     1     5.904 278.26  84.372\n&lt;none&gt;                  272.36  87.184\n+ speak     1    10.237 262.12  89.388\n+ count     1     2.686 269.67  90.410\n- read      1   205.296 477.65 103.824\n- motheriq  1   209.278 481.64 104.123\n\nStep:  AIC=83.07\nscore ~ motheriq + read + edutv + cartoons\n\n           Df Sum of Sq    RSS     AIC\n&lt;none&gt;                  219.93  83.070\n+ fatheriq  1    20.167 199.77  83.192\n+ speak     1     6.877 213.06  85.510\n+ count     1     0.475 219.46  86.576\n- cartoons  1    52.426 272.36  87.184\n- edutv     1    57.767 277.70  87.883\n- read      1   191.180 411.11 102.007\n- motheriq  1   226.047 445.98 104.937\n\n\nThe output looks much like the output created in our forward selection case with one major difference: at each step the table of possible decisions includes both + terms and - terms. In step 1 the function considers adding cartoons, fatheriq, speak, or count, and also taking away edutv, read, or motheriq. Though labeled “AIC” the last column is actually a calculated BIC because we specified k=log(n). In R’s step function, the value of k specifies the multiplier to be put with the (p+2) measure of model size:\n\\[\nstep's \\:AIC=k(p+2)-2ln(\\hat L)\n\\]\nThis means k=2 indicates you want traditional AIC, and k=ln(n) indicates BIC. Remember log(n) in R is using natural log.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building and Selection</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#best-model-of-size-p",
    "href": "Chapter8.html#best-model-of-size-p",
    "title": "8  Building and Selection",
    "section": "8.4 Best model of size p",
    "text": "8.4 Best model of size p\nIn the leaps library you can find a helpful function called regsubsets. The regsubsets function allows you to have R search every possible model with p predictors and tell you which is best.\nHere is an example with our gifted data where regsubsets is asked to consider models ranging from having one predictor (in addition to an intercept) up to four predictors. As inputs we provide the form of the full model, the data set, nbest tells R how many “best” models we want of each size, and nvmax tells R how large a model we want to work up to.\n\nbest_mods&lt;-regsubsets(score~., data=gifted, nbest=1, nvmax=4)\nsummary(best_mods)\n\nSubset selection object\nCall: regsubsets.formula(score ~ ., data = gifted, nbest = 1, nvmax = 4)\n7 Variables  (and intercept)\n         Forced in Forced out\nfatheriq     FALSE      FALSE\nmotheriq     FALSE      FALSE\nspeak        FALSE      FALSE\ncount        FALSE      FALSE\nread         FALSE      FALSE\nedutv        FALSE      FALSE\ncartoons     FALSE      FALSE\n1 subsets of each size up to 4\nSelection Algorithm: exhaustive\n         fatheriq motheriq speak count read edutv cartoons\n1  ( 1 ) \" \"      \"*\"      \" \"   \" \"   \" \"  \" \"   \" \"     \n2  ( 1 ) \" \"      \"*\"      \" \"   \" \"   \"*\"  \" \"   \" \"     \n3  ( 1 ) \"*\"      \"*\"      \" \"   \" \"   \"*\"  \" \"   \" \"     \n4  ( 1 ) \"*\"      \"*\"      \"*\"   \"*\"   \" \"  \" \"   \" \"     \n\n\nFrom the top section of this output you can see there are “forced in” and “forced out” options we did not apply to any of the potential independent variables. The lower section then gives the results. The row labeled “1” has an asterisk under motheriq indicating the best one-predictor model uses motheriq. The row labeled “2” has asterisks for motheriq and read telling us the best two-predictor model includes those terms. The best three-predictor model is shown to be score~fatheriq+motheriq+read, and the best four–predictor model found was score~fatheriq+motheriq+speak+count.\nNotice the best three-predictor model is not simply a subset of the best four-predictor model. While our forward selection, backward elimination, and stepwise models change only one term at a time making each successive model have only one variable different, the work done by regsubsets is exhaustive and doesn’t require one-at-a-time changes.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building and Selection</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#lasso-regression",
    "href": "Chapter8.html#lasso-regression",
    "title": "8  Building and Selection",
    "section": "8.5 Lasso Regression",
    "text": "8.5 Lasso Regression\nRecall from way back in Chapter 2 that our best linear fit solution comes from wanting to set the \\(\\hat\\beta_i\\) values in such a way that \\(\\sum (y_i-\\hat{y_i})^2\\) is minimized. The lasso technique modifies this goal so that instead we minimize:\n\\[\n\\sum_{i=1}^{n} (y_i-\\hat y_i)^2 + \\lambda \\sum_{j=1}^{p}|{\\beta_j}|\n\\tag{8.1}\\]\nWith this change, we are making the decision that minimizing the sum of squared errors is important, but we also want to minimize the sum of the absolute value of our \\(\\beta\\) parameters weighted by a \\(\\lambda\\) tuning parameter. The value in doing this, is that now we can start with all possible independent variables and see which get eliminated through a \\(\\hat\\beta=0\\) fit.\n\nIn R\nWe’ll apply this approach in R to our gifted data using the glmnet function in the glmnet library.\n\n# first need our data in format of x and y matrices\nmod_x&lt;-as.matrix(gifted[,-1])\nmod_y&lt;-as.matrix(gifted[,1])\n\n# use cross validation to explore different values of lambda\nset.seed(1234)\nstep1&lt;-cv.glmnet(mod_x, mod_y, alpha=1)\nplot(step1$glmnet.fit)\n\n\n\n\n\n\n\n\nThis first plot looks at the values of the \\(\\beta\\) coefficients over a wide range of \\(\\lambda\\) values. Each \\(\\beta\\) has one colored line associated with it. The different considered\\(\\lambda\\)s are along the x-axis, but in the form of \\(-log(\\lambda)\\), and the y-axis indicates the value of the \\(\\beta\\)s for each corresponding \\(\\lambda\\). Along the top of the plot the number of non-zero \\(\\beta\\) coefficients is shown.\n\nplot(step1)\n\n\n\n\n\n\n\n\nThis plot shows the mean-squared error for our model under various values of \\(\\lambda\\). Vertical reference lines are drawn at the minimum mean-squared error, and at the minimum plus one standard error. The \\(\\lambda\\)s that correspond to each of these key values are retrievable through step1$lambda.min and step1$lambda.1se.\n\nstep1$lambda.min\n\n[1] 0.2115333\n\n\nThen we can fit the model using the identified best \\(\\lambda\\) and see which coefficients are non-zero:\n\nstep2&lt;-glmnet(mod_x, mod_y, alpha=1, lambda=step1$lambda.min)\nlasso.coef&lt;-predict(step2, type=\"coefficients\")\nlasso.coef\n\n8 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s0\n(Intercept) 63.4282931\nfatheriq     0.2681240\nmotheriq     0.3592196\nspeak        0.1696803\ncount        0.2590080\nread         5.8841220\nedutv       -0.5542266\ncartoons     .        \n\n\nIf not satisfied with only eliminating cartoons, we could change to a larger, non-optimal lambda:\n\nstep2&lt;-glmnet(mod_x, mod_y, alpha=1, lambda=1.5)\nlasso.coef&lt;-predict(step2, type=\"coefficients\")\nlasso.coef\n\n8 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept) 130.0439277\nfatheriq      .        \nmotheriq      0.1734218\nspeak         .        \ncount         0.1340442\nread          2.1009536\nedutv         .        \ncartoons      .        \n\n\nAnd here we see motheriq, count, and read are the three terms selected as having a non-zero coefficient with \\(\\lambda=1.5\\). To use lasso strictly as a model selection tool, from here you would then return to the lm function to fit a model with these three terms are base any predictions on the \\(\\hat\\beta\\)s that result from simply fitting the least-squares criterion, not that seen in Equation 8.1.\n\nmod_result&lt;-lm(score~motheriq+count+read, data=gifted)\nsummary(mod_result)$coef\n\n              Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 85.3970316 11.39507108 7.4942079 1.565612e-08\nmotheriq     0.4157893  0.07710859 5.3922568 6.345091e-06\ncount        0.1820829  0.28048936 0.6491617 5.208663e-01\nread         8.9042595  5.88322934 1.5134986 1.399682e-01",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building and Selection</span>"
    ]
  },
  {
    "objectID": "Chapter8.html#principal-components-analysis-pca",
    "href": "Chapter8.html#principal-components-analysis-pca",
    "title": "8  Building and Selection",
    "section": "8.6 Principal Components Analysis (PCA)",
    "text": "8.6 Principal Components Analysis (PCA)\nNow we’re going to consider a much larger data set: the bdims data frame in the openintro library contains 25 measures on 507 physically active adults. In addition to gender, age, weight, and height, we have access to hip girth, bicep girth, knee diameter, and all kinds of body measurements. It’s great information, but as you might suspect these measurements are often highly correlated with each other. Larger hip girths tend to go with larger waists, small ankles make you more likely to have smaller wrists, and so on.\nPrincipal Components Analysis is a way we can incorporate much of the valuable information offered by these 21 body measurements, without making our model have 21 separate slopes and a host of multicollinearity issues. How?\nLet’s start smaller and focus on two of our body measurements: hip_gi, the hip girth as measured at the same level as bit_di, the bitrochanteric diameter. Bitrochanteric diameter is basically hip width at the top of the femur but below the pelvis. Obviously these two measurements should be correlated and they are. Limiting to only the 247 men in the bdims data frame, the two hip measurements are plotted here in Figure 8.2.\n\n\n\n\n\n\n\n\n\n\n\n(a) Original data\n\n\n\n\n\n\nFigure 8.2: Male Hip Measurements\n\n\n\n\nNow look at the plot with the regression line overlaid and think about rotating the entire scatter plot to right a bit so that regression line was the new x-axis. Go ahead, pick up your book or screen (or turn your head) and rotate the graph so the blue line is our new horizontal. If you knew where along that new x-axis a person lied, you’d know a lot about their hip size. You’d have a combination of hip girth and hip diameter information with just that one blue-axis variable. The vertical variation up and down from that line is the relatively small amount of information you’re giving up by only using one variable instead of the original two.\nWe could repeat this process over and over with pairs of body measurements we know are related, at each point picking the one-dimensional combination of them and effectively cutting our model size in half while giving up relatively little. But 1) that’s time consuming and 2) what if we don’t have the proper knowledge to know which terms to pair?\nStop and think about how that blue line does such a good job simplifying the two dimensions into one main direction of variability and then one minor direction of variability. What if we could take our full 21 dimensions of body measurements and get the computer to tell us the axes that align with the highest variability and the least variability? That is what we can do with principal components analysis, often referred to simply as PCA.\nThe linear algebra that’s driving this change in basis is beyond our scope here, but instead we will focus on how to apply the method in R.\n\nIn R\nFirst, the data must be numeric in nature. For our body measurements, we’ll look only at women, and only at the first 21 columns which cover bia_di (biacromial diameter) through wri_gi (wrist girth). There are a few different functions in R that will perform PCA, but we’ll use the prcomp option that comes in the stats library.\n\nwomens_bdims&lt;-bdims[bdims$sex==0,1:21]\n\n# or if you prefer:\n\nwomens_bdims&lt;-bdims |&gt;\n  filter(sex==0) |&gt;\n  select(bia_di:wri_gi)\n\npca_body&lt;-prcomp(womens_bdims, scale.=TRUE)\n\nNote that when prcomp is called, the scale.=TRUE is added in. What this does is tell R that before finding the axes of highest variability, the data should be scaled. This keeps us from labeling the dimensions that just happen to have bigger numbers from dominating the findings. If all body measurements were taken in cm but we transformed knee diameter into mm, we’d think a difference of 100 in the knee measurement was huge and a difference of 10cm in a waist was tiny. The scale.=TRUE takes each column and converts it to z-scores using the column means and standard deviations. This was a 3-standard deviation high measure on calf girth gets the exact same attention as a 3-standard deviation high measure on ankle girth. Always scale in PCA.\nFigure 8.3 shows the first four principal components. The prcomp function will create as many principal components (new axes) as there are original dimensions to your data. They are ordered based on the amount of variability they can explain. Notice that every plot in the scatter plot matrix looks like just a random blob. That will always happen as these new axes will be uncorrelated with each other by design.\n\n\n\n\n\n\n\n\nFigure 8.3: PCA Results\n\n\n\n\n\nA biplot can be used to get a sense of how the original columns relate to the new principal components. Two biplots showing PC1 vs. PC2 and PC3 vs. PC4 are below in Figure 8.4.\n\npar(mfrow=c(1,2))\npar(mar=c(4,5,2,2))\nbiplot(pca_body)\nbiplot(pca_body, choices=c(3,4))\n\n\n\n\n\n\n\nFigure 8.4: PCA Results\n\n\n\n\n\nBoth of these biplots are a bit hard to read because there are 21 red vectors all on top of each other. The first gives some discernable information though. It shows that PC1 involves a negative multiple of every column, while PC2 looks to be about half positive and half negative. The numeric information for how the original columns map to the principal components is given in the $rotation element of our completed prcomp object.\nHere are the mappings for the first two principal components, rounded to five decimal places. They confirm the all-negative loadings for PC1 and the roughly even positive/negative split for PC2.\n\nround(pca_body$rotation[,1],5)\n\n  bia_di   bii_di   bit_di   che_de   che_di   elb_di   wri_di   kne_di \n-0.16001 -0.15629 -0.21022 -0.17278 -0.20168 -0.20772 -0.19393 -0.23672 \n  ank_di   sho_gi   che_gi   wai_gi   nav_gi   hip_gi   thi_gi   bic_gi \n-0.17357 -0.23440 -0.24326 -0.23461 -0.22822 -0.25381 -0.23532 -0.24366 \n  for_gi   kne_gi   cal_gi   ank_gi   wri_gi \n-0.25137 -0.24002 -0.22807 -0.20682 -0.22877 \n\nround(pca_body$rotation[,2],5)\n\n  bia_di   bii_di   bit_di   che_de   che_di   elb_di   wri_di   kne_di \n 0.31718 -0.05449  0.06264 -0.32902  0.05383  0.29060  0.39074  0.13547 \n  ank_di   sho_gi   che_gi   wai_gi   nav_gi   hip_gi   thi_gi   bic_gi \n 0.31147 -0.03461 -0.18167 -0.32502 -0.29562 -0.18830 -0.20851 -0.19328 \n  for_gi   kne_gi   cal_gi   ank_gi   wri_gi \n 0.00609  0.03437  0.04863  0.16233  0.23538 \n\n\nInterested in how much of the total variability can be seen in each principal component? That information is seen in the $sdev element of the prcomp object and can be seen in a bar chart with the plot command:\n\n# convert standard deviations to variance by squaring\nvariances&lt;-pca_body$sdev^2\n\n# look at variance for first 8 components, rounded\nround(variances[1:8],4)\n\n[1] 12.3825  1.6555  1.2828  1.0451  0.8028  0.5475  0.4608  0.4358\n\n# proportion of total variance for first 8 components, rounded\nround(variances[1:8]/sum(variances),4)\n\n[1] 0.5896 0.0788 0.0611 0.0498 0.0382 0.0261 0.0219 0.0208\n\n# see plot of variance per component\nplot(pca_body)\n\n\n\n\n\n\n\nFigure 8.5: Component Variance Plot\n\n\n\n\n\nBut what does any of this have to do with regression? Well, now that we have our principal components we can use them as predictors in our model to maximize the information from our data set while minimizing the number of \\(\\beta\\) parameters that need estimating.\n\n#create BMI outcome to estimate with the measurements\nbmi&lt;-bdims$wgt/(bdims$hgt/100)^2\n\n#limit to only the women\nbmi_f&lt;-bmi[bdims$sex==0]\n\n#fit model with first seven principal components\nmod_bmi&lt;-lm(bmi_f~pca_body$x[,1:7])\n\n#look at model summary\nsummary(mod_bmi)\n\n\nCall:\nlm(formula = bmi_f ~ pca_body$x[, 1:7])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8904 -0.7937 -0.0327  0.7054  4.1316 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          22.27793    0.07581 293.853  &lt; 2e-16 ***\npca_body$x[, 1:7]PC1 -0.77013    0.02159 -35.677  &lt; 2e-16 ***\npca_body$x[, 1:7]PC2 -0.88505    0.05904 -14.992  &lt; 2e-16 ***\npca_body$x[, 1:7]PC3  0.35202    0.06707   5.249 3.25e-07 ***\npca_body$x[, 1:7]PC4 -0.22522    0.07430  -3.031  0.00269 ** \npca_body$x[, 1:7]PC5 -0.05495    0.08478  -0.648  0.51751    \npca_body$x[, 1:7]PC6  0.24817    0.10266   2.417  0.01634 *  \npca_body$x[, 1:7]PC7 -0.11019    0.11190  -0.985  0.32573    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.222 on 252 degrees of freedom\nMultiple R-squared:  0.8595,    Adjusted R-squared:  0.8556 \nF-statistic: 220.2 on 7 and 252 DF,  p-value: &lt; 2.2e-16\n\n\nThis model summary shows us that just because one component covers more variability than another, doesn’t mean that component is more important when it comes to predicting our outcome. PC6 covers only 2.61% of the total variability compared to PC5’s 3.82%, but when it comes to predicting a person’s BMI PC6 is more helpful than PC5. Since PC1 and PC2 are so strongly related to BMI, it turns out a model with just those two predictors has an \\(R_{adj}^2=0.8337\\).- not bad for a 3-\\(\\beta\\) model on a complex problem.\nYou can run forward selection, backward elimination, or step-wise building algorithms on the principal components just as you would the raw data - but now with the advantage of not having multicollinearity to navigate.\nPCA isn’t without it’s downsides. Using this type of approach and model might be very powerful, but it isn’t nearly as interpretable as a model built on the original data. Depending on the context of the work, interpretability might take a back seat to predictive power which makes PCA a strong option. Also, sometimes looking at the PCA loadings themselves can provide insight into the interdependencies and relationships within your data.\nBottom line: PCA is a powerful tool to have in your tool box, but it shouldn’t be the first tool you turn to when fitting a model.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building and Selection</span>"
    ]
  },
  {
    "objectID": "Chapter9.html",
    "href": "Chapter9.html",
    "title": "9  Well that’s weird",
    "section": "",
    "text": "9.1 Studentized Residuals\nSo you fit a model and all of your residuals are within \\(\\pm 1\\). That’s great! Isn’t it? Well it all depends on the scale of the data you’re working with. If you are trying to model the weight of adults based on waist circumference than being consistently within \\(\\pm 1\\) lb would be great. If instead you are trying to predict a student’s blood alcohol level based on the number of beers they’ve had a \\(\\pm 1\\) error is pretty lousy since no one has a blood alcohol over 1 (or even 0.5).\nSimply put, studentized residuals are a standardized residual measure to help you assess the size of the errors without needing to know detail around the context of the data. A studentized residual is obtained by dividing the raw residual by an estimate of its standard deviation. What is the estimate of its standard deviation? Refer back to Equation 7.3 in Chapter 7 and you’ll see this is something we’ve already worked with and \\(H\\), the hat matrix, plays a key role.\nThe studentized residual for the \\(i\\)th observation is given by:\n\\[t_i = \\frac{e_i}{\\sigma \\sqrt{(1-h_{ii})}}\n\\]\nwhere:\nFundamentally, this approach is no different than the z-score formula you likely saw in intro statistics or the test-statistic format you’ve seen in basic hypothesis testing. The \\(t_i\\) studentized residual is just the observed error, minus the expected error which is always zero by design, then divided by a standard error.\nSo now that you have studentized residuals what can you do with them? Their main use is in being able to quickly identify outlier observations and have a consistent scale in which to think of just how much of an outlier they actually are. An outlier point with a studentized residual of 2-3 isn’t too bad, but an studentized residual of 17 is worth a look no matter what the scale of your y variable is.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Well that's weird</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#studentized-residuals",
    "href": "Chapter9.html#studentized-residuals",
    "title": "9  Well that’s weird",
    "section": "",
    "text": "\\(e_i\\) = residual for the \\(i\\)th observation\n\\(\\sigma\\) = mean squared error estimate as seen before\n\\(h_{ii}\\) = the \\(i\\)th diagonal of the hat matrix, \\(H=X(X^TX)^{-1}X^T\\)\n\n\n\n\nIn R\nTo get the studentized residuals for a fitted model you can use the rstudent function contained in the stats library. The example here explores calorie content of cereals from the UScereal data set in the MASS library.\nmodel_cereal&lt;-lm(calories~fat+sugars+carbo, data=UScereal)\nstud.res&lt;-rstudent(model_cereal)\n\nhist(model_cereal$residuals, main=\"\", xlab=\"Observed Residuals\", breaks=8)\nhist(stud.res, main=\"\", xlab=\"Studentized Residuals\", breaks=8)\n\n\n\n\n\n\n\n\n\n\n\n(a) Observed Residuals\n\n\n\n\n\n\n\n\n\n\n\n(b) Studentized Residuals\n\n\n\n\n\n\n\nFigure 9.1: Residuals for Cereal Model\n\n\n\nA summary of studentized residuals shows the most extreme outlying point has \\(t_i=4.654\\) which is definitely worth a second look. You can refer to a Student’s t-distribution with \\(n-(p+1)\\) degrees of freedom and see that a residual even 2.5 or larger should only happen about 0.76% of the time.\n\npt(2.5, nrow(UScereal)-(3+1), lower.tail = FALSE)\n\n[1] 0.007562028",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Well that's weird</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#leverage-and-cooks-distance",
    "href": "Chapter9.html#leverage-and-cooks-distance",
    "title": "9  Well that’s weird",
    "section": "9.2 Leverage and Cook’s Distance",
    "text": "9.2 Leverage and Cook’s Distance\nOutlier points in regression are not necessarily a problem. Where problems do occur is when the outlier is in a position that causes it to have undue influence over the regression fit. Consider Figure 9.2 below with two distinct outliers on the far right both at an x-value of 40.\nIn the case of the navy outlier point, it is far removed from the typical x values, but lies on the same x-y line that seems to fit the non-outlier points so it doesn’t exert an unusual amount of influence on the blue line fit.\nThe red outlier point however lies much higher than the linear pattern of non-outliers would expect. Because of this, inclusion of that one point when fitting the model moves the line from the blue fit to the one shown in orchid.\n\n\n\n\n\n\n\n\nFigure 9.2: Example of Leverage\n\n\n\n\n\nTo identify outlier points that might be influential, you can use the leverage metric. The leverage for a point is simply that point’s corresponding diagonal element of the “hat matrix”. On average, leverage will be about \\((p+1)/n\\), and any point with a leverage greater than 2.5 times that is worth a closer look.\nAnother popular diagnostic metric to assess when a point might be unduly influential is Cook’s Distance. Cook’s distance combines information from both the residual size and the leverage and is defined as:\n\\[\nD_i = \\frac{e_{i}^2}{(p+1) \\hat\\sigma^2} \\times \\frac{h_{ii}}{(1-h_{ii})^2} =\\frac{t_i^2}{p+1}\\times\\frac{h_{ii}}{1-h_{ii}}\n\\]\nwhere:\n\n\\(e_i\\) = residual for the \\(i^{th}\\) observation\n\\(t_i\\) = studentized residual for the \\(i^{th}\\) observation\n\\(p\\) = the number of independent variables in your model\n\\(\\hat\\sigma^2\\) = the observed standard error of residuals\n\\(h_{ii}\\) = the \\(i^{th}\\) diagonal of the hat matrix, \\(H=X(X^TX)^{-1}X^T\\)\n\nThis highlights that Cook’s distance increases if an observation has a high studentized residual, has a high leverage, or both. A rule of thumb is to examine points with \\(D_i &gt; 4/n\\), where \\(n\\) is the total number of observations. Ultimately though, any value that stands out from most others is worthy of a closer inspection.\n\nIn R\nThe functions for leverage and Cook’s distance are built into the stats library that loads by default. Continuing with the cereal nutrition example we find that there are three points with high leverage values.\n\n\n\n\n\n\n\n\n\n\n\n(a) Fat vs. Calories\n\n\n\n\n\n\n\n\n\n\n\n(b) Sugars vs. Calories\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Carbohydrates vs Calories\n\n\n\n\n\n\n\n\n\n\n\n(d) Model Fitted vs. Residuals\n\n\n\n\n\n\n\nFigure 9.3: Leverage Example\n\n\n\nThe hatvalues function returns the hat matrix diagonal for a fitted model object. From there it is a simple task to identify those that exceed our threshold of two and a half times \\((p+1)/n\\). Figure 9.3 above shows four plots of the cereal data with high leverage points marked in red. Leverage calculations are shown below.\n\nmodel_cereal&lt;-lm(calories~fat+sugars+carbo, data=UScereal)\n\nlev_cereal&lt;-hatvalues(model_cereal)\n((2+1)/nrow(UScereal))*2.5\n\n[1] 0.1153846\n\nsort(lev_cereal, decreasing=TRUE)[1:6]\n\n               Grape-Nuts        Great Grains Pecan        Cracklin' Oat Bran \n               0.61399054                0.42563479                0.14574239 \n            Fruitful Bran Shredded Wheat spoon size      Oatmeal Raisin Crisp \n               0.08625599                0.08324148                0.08255166 \n\n\nThe process is similar for calculating Cook’s distance. There are four cereals flagged with the Cook’s distance metric with Grape-Nuts becoming identified as a very significant standout with a \\(D_i&gt;6\\). Here in Figure 9.4 are the same four cereal plots with red points indicating those with a Cook’s distance greater than the \\(4/n\\) threshold.\n\n\n\n\n\n\n\n\n\n\n\n(a) Fat vs. Calories\n\n\n\n\n\n\n\n\n\n\n\n(b) Sugars vs. Calories\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Carbohydrates vs Calories\n\n\n\n\n\n\n\n\n\n\n\n(d) Model Fitted vs. Residuals\n\n\n\n\n\n\n\nFigure 9.4: Cook’s Distance Example\n\n\n\nBelow shows using the cooks.distance function, and a listing of the top six values.\n\ncd_cereal&lt;-cooks.distance(model_cereal)\n4/nrow(UScereal)\n\n[1] 0.06153846\n\nsort(cd_cereal, decreasing=TRUE)[1:6]\n\n               Grape-Nuts        Great Grains Pecan                 100% Bran \n               6.43414435                0.50046528                0.13898120 \nAll-Bran with Extra Fiber                 Special K Nutri-Grain Almond-Raisin \n               0.12924058                0.06608620                0.04776351",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Well that's weird</span>"
    ]
  },
  {
    "objectID": "Chapter9.html#dffits-and-dfbetas",
    "href": "Chapter9.html#dffits-and-dfbetas",
    "title": "9  Well that’s weird",
    "section": "9.3 DFFits and DFBetas",
    "text": "9.3 DFFits and DFBetas\nTo understand the exact influence yielded by a row of your data, DFFits and DFBetas are used. DFFits will tell you how the fit of your model changes with the inclusion and exclusion of each point. DFBetas tells you how the estimated \\(\\hat\\beta\\) parameters change with the inclusion and exclusion of each point.\nFor example, in our calories~fat+sugars+carbo model in the UScereal data frame, the DFFit for Cinnamon Toast Crunch is -0.237. That means that when the model is fit including Cinnamon Toast Crunch (CTC) in the training set, the fitted value for CTC is 0.237\\(\\sigma_{CTC}\\) lower than when the model is fit excluding CTC from the training set. Note that it is in units of \\(\\sigma\\) specific to the CTC observation, not the broader \\(\\sigma\\) of the model. In general, the formula for DFFits for observation \\(i\\) is:\n\\[\nDFFits_i=\\frac {\\hat y_i - \\hat y_{(i)i}}{\\sqrt{\\hat\\sigma_{(i)}^2h_{ii}}}\n\\]\nwhere:\n\n\\(\\hat y_i\\) = the fitted value for the \\(i^{th}\\) observation when all data trains the model\n\\(\\hat y_{(i)i}\\) = the fitted value for the \\(i^{th}\\) observation when model built without observation \\(i\\)\n\\(\\hat\\sigma_{(i)}^2\\) = the observed mean squared error of residuals in model built without observation \\(i\\)\n\\(h_{ii}\\) = the \\(i^{th}\\) diagonal of the full model hat matrix, \\(H=X(X^TX)^{-1}X^T\\)\n\nDFBetas is the same idea, but looking at the difference in \\(\\beta\\) values with and without each observation rather than at the difference in fits.\n\\[\nDFBetas_j=\\frac{\\hat\\beta_j - \\hat \\beta_{j(i)}}{\\sqrt{\\hat\\sigma^2_{j(i)}(X^TX)^{-1}_{jj}}}\n\\]\nwhere:\n\n\\(\\hat \\beta_j\\) = the estimate for the \\(\\beta_j\\) when all observations train the model\n\\(\\hat\\beta_{j(i)}\\) = the estimate for the \\(\\beta_j\\) when the model is built without the \\(i^{th}\\) observation\n\\(\\hat\\sigma_{j(i)}^2\\) = the observed mean squared error of residuals in model built without observation \\(i\\)\n\\(X\\) = the X matrix of data including the column of 1s for the intercept\n\\((X^TX)^{-1}_{jj}\\) = the \\(j^{th}\\) diagonal of the \\((X^TX)^{-1}\\) matrix\n\nYou can refer back to Chapter 7 to review the derivation of \\(\\hat\\sigma^2(X^TX)^{-1}\\) as the variance matrix for the \\(\\beta\\) vector.\nSimple plots of DFfits and DFBetas are often helpful in spotting influential observations. An example of this is in Figure 9.5. As a general rule, DFfits values greater than \\(2\\sqrt{p/n}\\) or less than \\(-2\\sqrt{p/n}\\) are worthy of a second look so reference lines have been included in Figure 9.5 (a) to show those thresholds. For Figure 9.5 (b) reference lines are included at \\(\\pm 2/\\sqrt{n}\\), the general rule of thumb for evaluating DFBetas.\n\n\n\n\n\n\n\n\n\n\n\n(a) DFFits\n\n\n\n\n\n\n\n\n\n\n\n(b) DFBetas for sugars\n\n\n\n\n\n\n\nFigure 9.5: DFFits and DFBetas\n\n\n\n\nIn R\nHere are the series of steps to calculate out the DFFits value of Cinnamon Toast Crunch, the \\(11^{th}\\) row of USCereals, in R.\n\nmod_cereal&lt;-lm(calories~fat+sugars+carbo, data=UScereal)\nfit1&lt;-mod_cereal$fitted.values[11]\n\nmod_noCTC&lt;-lm(calories~fat+sugars+carbo, data=UScereal[-11,])\nfit2&lt;-predict(mod_noCTC, UScereal[11,]) \n\nCTC_hat&lt;-hatvalues(mod_cereal)[11]\nCTC_sig&lt;-sqrt(summary(mod_noCTC)$sigma^2 * CTC_hat)\n\n(fit1-fit2)/CTC_sig\n\nCinnamon Toast Crunch \n           -0.2372347 \n\n\nThe easier way is to use the dffits function that is part of the stats library in R. The only input needed is the fitted model object, and DFFits for all observations will be returned.\n\nmod_cereal&lt;-lm(calories~fat+sugars+carbo, data=UScereal)\ndfs&lt;-dffits(mod_cereal)\ndfs[11]\n\nCinnamon Toast Crunch \n           -0.2372347 \n\n\nFor DFBetas, here is the DFBeta for the sugars slope considering the Cinnamon Toast Crunch row of data.\n\n  mod_cereal&lt;-lm(calories~fat+sugars+carbo, data=UScereal)\n  fit1&lt;-mod_cereal$coef[3]\n  \n  mod_noCTC&lt;-lm(calories~fat+sugars+carbo, data=UScereal[-11,])\n  fit2&lt;-mod_noCTC$coef[3]\n  \n  sig1&lt;-summary(mod_noCTC)$sigma^2\n  \n  xmat&lt;-as.matrix(cbind(rep(1, 65), UScereal[,c(4,7,8)]))\n  inv&lt;-solve(t(xmat)%*%xmat)\n  siguse&lt;-sqrt(sig1*inv[3,3])\n  \n  (fit1-fit2)/siguse\n\n    sugars \n0.08030943 \n\n\nTo do this automatically in R you can use the dfbetas function. The dfbetas function returns a full matrix of values with each row corresponding to an observation, and each column corresponding to a different \\(\\beta\\). For the sugar slope change associated with omitting Cinnamon Toast Crunch we’ll want the \\(11^{th}\\) row and the \\(3^{rd}\\) column.\n\ndfb&lt;-dfbetas(mod_cereal)\ndfb[11,]\n\n(Intercept)         fat      sugars       carbo \n-0.06653261 -0.19889228  0.05119340  0.07823310 \n\n\nBe aware that in addition to the dfbetas function, there is also a dfbeta function. The dfbeta function without the trailing s, provides the raw difference in \\(\\beta\\) fits without scaling by the standard error of \\(\\beta\\). This can also be useful sometimes, but it is important to know which one you’re looking at.\n\ndfb_raw&lt;-dfbeta(mod_cereal)\ndfb_raw[11,]\n\n(Intercept)         fat      sugars       carbo \n-0.32113477 -0.20768424  0.01484826  0.01446437",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Well that's weird</span>"
    ]
  }
]