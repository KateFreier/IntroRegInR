{
  "hash": "381585f0962bad20d2560a42d8c17c7b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Building and Selection\"\nformat: pdf\neditor: visual\nlabel: sec-chap8\n---\n\n\n\n\n\n\n\n\n## Measures of model strength\n\n### Test Set and Training Set\n\nIt may seem counter-intuitive but sometimes the best choice you can make is to NOT use all of your data when fitting your model. If you begin by splitting your data into two, you can fit the model on one set, known as the \"training set\", and then test how well the model performs on the remaining \"test set\". This helps protect you from creating a model that is so well tuned to your specific data that it isn't generalizable to the full population and won't produce helpful predictions.\n\nDiscretion is allowed when deciding the relative sizes of the training and testing sets. At most you would split the data 50/50 putting half in each set randomly. In some cases with limited data you might shift that to putting as much as 80% in the training set and holding out the remaining 20% for a sanity check.\n\n### In R {.unnumbered}\n\nBelow is an example using the reduced glass refraction index data just seen at the end of @sec-multico. The fgl glass data frame has 214 rows to start with.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(caret)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed for reproducability\nset.seed(1234)\n\n# randomly select 150 rows for the training set\ntrain_index<-sample(1:214, 150)\n\n# put row numbers not selected into the test set\ntest_index<-(1:214)[!is.element(1:214, train_index)]\n\n# create the two data sets\nglass_train<-fgl[train_index,]\nglass_test<-fgl[test_index,]\n\n# fit the model with the training set\nglass_model<-lm(RI~Al+Si+Ca+Ba, data=glass_train)\n\n# find mean squared error in training set\nmean(glass_model$residuals^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.160116\n```\n\n\n:::\n\n```{.r .cell-code}\n# find mean squared error in test set\nfit_values<-predict(glass_model, glass_test)\ntest_errors<-fit_values-glass_test$RI\nmean(test_errors^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9209567\n```\n\n\n:::\n\n```{.r .cell-code}\n# if you want a formal test, can do t-test of errors\nt.test(glass_model$residuals^2, test_errors^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  glass_model$residuals^2 and test_errors^2\nt = 0.58113, df = 152.37, p-value = 0.562\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5738999  1.0522177\nsample estimates:\nmean of x mean of y \n1.1601156 0.9209567 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nThis shows that there is no evidence our model fitted using the training data doesn't perform just as well on our test data. With this we can be confident that future refraction index values predicted using the model are reasonably accurate.\n\n### K-fold cross-validation\n\nWhat if we just got lucky with our test set and training set sampling? K-fold cross-validation takes the idea of test/training set validation to the next level. Instead of separating the data into just two parts, a test set and a training set, k-fold cross validation splits the data into k parts. Often k is set to 5 or 10, but there's no reason you can't select any other number provided you have enough rows to reasonably split.\n\nOnce the data is split, the model is then fit k times; each time using k-1 groups as the training set and the held out group as the test set. @fig-kfold below illustrates what would happen with 4-fold cross validation on a data set with only 8 rows.\n\nFirst the 8 rows are randomly grouped into four groups of 2. The model is then run 4 times. In the example, rows 1 and 7 are in fold 1. The first time the model is fit, fold 1 points are set aside for testing and the model is trained on rows 2,3,4,5,6, and 8. For model 2, our second fold containing rows 2 and 3 are set aside for testing, and the model is trained using rows 1,4,5,6,7, and 8. In the end you're left with four examples of how well the model fit using the approach you've selected fits data that was not involved in the fitting.\n\n![Four-fold cross validation diagram](kfold_table.jpg){#fig-kfold}\n\n### In R {.unnumbered}\n\nContinuing with the glass refractive index scenario, we'll complete 10-fold cross validation of a model containing Al, Si, Ca, and Ba as linear predictors using the `train` function in the `caret` library.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed for reproducability\nset.seed(1234)\n\n# Define the parameters. Method is cross validation, k=10.\ntrain_params <- trainControl(method = \"cv\", number = 10)\n\n# Train your k models.  Specify lm is type of model,\n#     and put in the training parameters set above\nglass_models <- train(RI ~ Al+Si+Ca+Ba, data=fgl,\n  method = \"lm\",trControl = train_params)\n\n# View the results of the cross-validation\nglass_models$resample\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        RMSE  Rsquared       MAE Resample\n1  0.4672234 0.9570430 0.3934590   Fold01\n2  0.6537417 0.9465408 0.5239035   Fold02\n3  1.0238911 0.8434257 0.6182354   Fold03\n4  1.4876875 0.7103225 1.0625717   Fold04\n5  1.1690601 0.8168152 0.9380207   Fold05\n6  1.0556602 0.8918505 0.8826193   Fold06\n7  0.5904965 0.9885397 0.4773744   Fold07\n8  1.2919068 0.7462664 0.7501642   Fold08\n9  1.3814389 0.7731487 0.8307277   Fold09\n10 1.0430018 0.8797080 0.7720872   Fold10\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nThese results indicate that over the 10 folds, the worst RMSE (square-root of mean squared error) among test holdouts happened when Fold4 was withheld from training. In this worst-case, the RMSE was 1.487. In the best-case, when Fold 1 was withheld, the RMSE was only 0.467. These are RMSEs calculated on the fold that was used as testing, not the folds that were used in training. If you prefer, the $R^2$ values for the testing holdouts, and the MAE (mean absolute error) for the testing holdouts are also given for each of the k folds. Some variability over the folds is to be expected, and these ten look reasonably consistent indicating the `RI~Al+Si+Ba+Ca` model approach is not over-fitting and should apply generally to the population.\n\n### Adjusted $R^2$\n\nBack in Chapter 2 we defined $R^2$, the coefficient of determination, as:\n\n$$R^2=1-\\frac{SSError}{SSTotal}=\\frac{SSReg}{SSTotal}$$\n\nSo whenever the sum of squared residuals, $SSError$, decreases, the $R^2$ goes up. In the world of multiple regression this creates a bit of a problem because every time you add a new term to the regression model the sum of squared residuals will go down whether the new term is really helpful or not. That's why judging two models against each other using $R^2$ just won't work. Enter adjusted $R^2$.\n\n$$R_{adj}^2=1-\\frac{SSError/(n-p-1)}{SSTotal/(n-1)}$$\n\nWhere $n$ is the number of observations and $p$ is the number of predictor variables. This makes it so just adding terms won't necessarily increase your $R^2$ - it will only improve if the increase in $p$ is adding real value to your model.\n\nGetting this adjusted $R^2$ from R is simple. The basic model summary output includes this adjusted $R^2$ right after the $R^2$ value you've already come to know and love. Below is a highlighted example from the reduced glass refraction index model. The basic $R^2$ with no adjustment is given as 0.8914, and the $R_{adj}^2$ is given as 0.8883. Since the two are close in value, that is an indication that every term in the linear model is providing additional insight into the value of refraction index.\n\n![](adjR_example.jpg){fig-align=\"center\"}\n\n### AIC and BIC\n\nAIC (Akaike's Information Criterion) and BIC (Bayes Information Criterion) are two metrics used to compare the relative strength of competing models. The specific values of AIC and BIC have no real meaning, but the relative size of AIC and BIC of two or more possible models can help you decide which model to use.\n\nCalculation of both AIC and BIC includes the log of the likelihood of the model under review. What is a model's likelihood? Simply put: it's the probability of seeing the observed data if the fitted model is the truth. The process to figuring out this likelihood is beyond our scope so we'll rely on R to do the work for us, but it's good to have a sense of what it's measuring.\n\n$$\nAIC=2(p+2)-2ln(\\hat L)\n$$\n\nand\n\n$$\nBIC=ln(n)(p+2)-2ln(\\hat L)\n$$\n\nwhere $\\hat L$ is the likelihood, $p$ is the number of independent predictors in your model, and $n$ is the number of observations. Notice the two are quite similar; the only difference is AIC has the number of estimated parameters multiplied by 2, while BIC penalizes larger parameter counts by a factor of $ln(n)$.\n\nFor both AIC and BIC, the model with the lower value is the better choice. The `stats` library in R (a library usually installed by default) contains both an `AIC` and a `BIC` function for calculating these measures.\n\nHere is an example considering two different models for glass refraction index; one with glass type indicators and one without:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_glass2<-lm(RI~Na+Al+Si+K+Ca+Ba+Fe+vehicle+headlight, data=fgl)\n\nmod_glass3<-lm(RI~Na+Al+Si+K+Ca+Ba+Fe, data=fgl)\n\nAIC(mod_glass2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 598.3572\n```\n\n\n:::\n\n```{.r .cell-code}\nAIC(mod_glass3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 625.5007\n```\n\n\n:::\n\n```{.r .cell-code}\nBIC(mod_glass2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 635.383\n```\n\n\n:::\n\n```{.r .cell-code}\nBIC(mod_glass3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 655.7945\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nIn this case AIC and BIC agree that the model including glass type is better than the model without type. It is not always the case that AIC and BIC will agree though. That's why for scientific rigor you should select which measure you want to use prior to calculating them.\n\n## Model Building\n\nWith $R_{adj}^2$, AIC, and BIC we now have three different approaches to measure the quality of a model. This opens the door to ways we can build a multiple regression model piece by piece.\n\n### Forward Selection\n\nThe forward selection process is exactly what is sounds like: adding one variable at a time. At each step of this approach, each potential predictor is considered and one that improves $R_{adj}$ the most, or decreases AIC or BIC the most, is added to the model. The process continues until adding one more term does not improve your performance metric.\n\n### Backward Elimination\n\nThis approach to model building is also exactly what it sounds like. With backward selection you begin with a large model including all possible predictors and then remove them one by one. Which one should be removed at each step can be decided based on which has the highest p-value for the test on $\\hat\\beta$, by which one, when removed, increases $R_{adj}$ the most, or by which one, when removed, decreases AIC or BIC the most. The process stops when removing any additional terms will only hurt the model quality.\n\n### Step-wise\n\nThe step-wise approach is a combination of forward selection and backward elimination. In this algorithm, the model can either grow or shrink but only ever one term at a time. The modeler uses discretion to add or subtract terms at each iteration, possibly alternating between adding and subtracting, but other paterns are possible as well. The process stops when it's found that neither adding nor subtracting a term will improve the decision metric ($R_{adj}^2$, AIC or BIC).\n\n## Building Examples in R\n\n### Forward\n\nIn the openintro library you will find a data frame called gifted that contains information on 36 gifted children. Along with their scores for a test of analytical skills, the data frame contains each child's mother's and father's IQs, the age at which the child first said \"mommy\" or \"daddy\", the age when the child first counted to 10 successfully, and the average number of hours per week the child reads with a parent, watches education TV, and watches cartoons.\n\nTo build a model working with forward selection and the AIC metric, we can use the `step` function built into the `stats` library as follows:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# specify an empty intercept-only model to start\nmod_start<-lm(score~1, data=gifted)\n\n# define the largest model possible as an upper bound on scope \nmod_full<-lm(score~., data=gifted)\n\n# specifying k=2 indicates using AIC.  Each step will be printed out.\nstep(mod_start, scope=list(lower=mod_start, upper=mod_full), \n     direction=\"forward\", k=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStart:  AIC=111.33\nscore ~ 1\n\n           Df Sum of Sq    RSS     AIC\n+ motheriq  1   244.838 505.47  99.111\n+ count     1   222.211 528.09 100.687\n+ read      1   206.958 543.35 101.712\n+ edutv     1   102.861 647.44 108.023\n+ speak     1    53.846 696.46 110.650\n+ cartoons  1    45.074 705.23 111.100\n<none>                  750.31 111.331\n+ fatheriq  1    26.542 723.76 112.034\n\nStep:  AIC=99.11\nscore ~ motheriq\n\n           Df Sum of Sq    RSS     AIC\n+ read      1   227.205 278.26  79.622\n+ count     1   211.162 294.31  81.640\n+ speak     1    38.747 466.72  98.240\n+ fatheriq  1    30.712 474.76  98.854\n+ edutv     1    27.814 477.65  99.073\n<none>                  505.47  99.111\n+ cartoons  1     2.272 503.20 100.949\n\nStep:  AIC=79.62\nscore ~ motheriq + read\n\n           Df Sum of Sq    RSS    AIC\n+ fatheriq  1    43.605 234.66 75.486\n<none>                  278.26 79.622\n+ speak     1    11.834 266.43 80.057\n+ edutv     1     5.904 272.36 80.850\n+ count     1     3.617 274.65 81.151\n+ cartoons  1     0.563 277.70 81.549\n\nStep:  AIC=75.49\nscore ~ motheriq + read + fatheriq\n\n           Df Sum of Sq    RSS    AIC\n<none>                  234.66 75.486\n+ speak     1   12.5784 222.08 75.503\n+ edutv     1    9.7910 224.87 75.952\n+ count     1    4.3398 230.32 76.814\n+ cartoons  1    0.8398 233.82 77.357\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = score ~ motheriq + read + fatheriq, data = gifted)\n\nCoefficients:\n(Intercept)     motheriq         read     fatheriq  \n    44.3656       0.4282      12.7663       0.3215  \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nNow let's examine each step of the output above. We specified in the `step` command that we wanted to begin with an empty intercept-only model and that the biggest model that could be considered is the full model with all predictors available in the `gifted` data frame.\n\nAt the start, our intercept-only model has an AIC of 111.33, and the first table in the output tells us the RSS (Residual Sum of Squares or SSError) and AIC for models that include the intercept plus each possible term one by one; each term gets a new row in the table. In the first round, R found that adding `motheriq` to the intercept produced an AIC of 99.111, adding `count` led to an AIC of 100.687, adding `read` led to an AIC of 101.712... and so on through to adding `fatheriq` making for an AIC of 112.034, an actual increase over the baseline start of 111.33. Since lower AIC is better, `motheriq` gets added to the model, and the second round begins.\n\nStep two begins with the `score~motheriq` model. An intercept is included but not explicitly stated here. The table shows the results from adding each possible remaining predictor to the score to the model and finds that adding in `read` is the most helpful and will decrease the model AIC to 79.622.\n\nTwo more rounds of forward selection continue. The third round examines adding another term to the `score~motheriq+read` model, and selects adding in `fatheriq`. Round four looks to add to the `score~motheriq+read+fatheriq` model, but discovers adding any of the remaining possible terms will result in a higher AIC so the search ends and the `score~motheriq+read+fatheriq` is returned as the final model.\n\n### Backward\n\nWhat if the backward elimination approach is taken? Same `gifted` data as used in the forward selection example, but now let's start with the full model and remove terms one at a time until all $\\hat\\beta$ p-values are below $\\alpha=0.05$.\n\nWe'll start by checking the VIFs to determine if high multicollinearity should be taken care of first.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvif(mod_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfatheriq motheriq    speak    count     read    edutv cartoons \n1.196214 1.172998 1.182755 6.886769 6.896808 8.214684 8.373075 \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(mod_full)$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Estimate  Std. Error    t value     Pr(>|t|)\n(Intercept) 75.5084856 24.02617507  3.1427593 3.934153e-03\nfatheriq     0.2524925  0.13756072  1.8354988 7.707469e-02\nmotheriq     0.4000681  0.07290507  5.4875205 7.328886e-06\nspeak        0.1876429  0.14766697  1.2707168 2.142862e-01\ncount        0.2064897  0.26631214  0.7753673 4.446222e-01\nread         7.5440549  5.58640059  1.3504321 1.876921e-01\nedutv       -4.2024429  2.24503495 -1.8718831 7.170324e-02\ncartoons    -3.3389901  2.01808208 -1.6545363 1.091872e-01\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nSince time spent watching educational tv and time spent watching cartoons are strongly correlated with each other (correlation of -0.9234), it is not surprising that the two have high VIFs. For step one, we'll remove `cartoons` because of the two, it is the one with the higher $\\hat\\beta$ p-value.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_2<-lm(score~fatheriq+motheriq+speak+count+read+edutv, data=gifted)\nvif(mod_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfatheriq motheriq    speak    count     read    edutv \n1.016676 1.154849 1.158823 6.650745 6.759192 1.217195 \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(mod_2)$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Estimate  Std. Error    t value     Pr(>|t|)\n(Intercept) 49.8812173 18.90916625  2.6379385 0.0132717059\nfatheriq     0.3406675  0.13056188  2.6092414 0.0142021633\nmotheriq     0.3850639  0.07447438  5.1704205 0.0000157951\nspeak        0.2223968  0.15048032  1.4779129 0.1502106664\ncount        0.2880610  0.26943473  1.0691311 0.2938293264\nread         6.2384283  5.69364778  1.0956822 0.2822341840\nedutv       -0.7741723  0.88969865 -0.8701512 0.3913636419\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nIn our model without `cartoons`, `read` and `count` still have VIFs greater than 5. They are strongly positively correlated (0.9103) so even though `edutv` has the highest p-value, I'll drop `count` this round since it has the higher p-value between `count` and `read`.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_3<-lm(score~fatheriq+motheriq+speak+read+edutv, data=gifted)\nvif(mod_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nfatheriq motheriq    speak     read    edutv \n1.016335 1.136772 1.054395 1.072252 1.190811 \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(mod_3)$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) 47.0554351 18.76811965  2.507200 1.781474e-02\nfatheriq     0.3381140  0.13085088  2.583965 1.488079e-02\nmotheriq     0.3950256  0.07406516  5.333487 9.083768e-06\nspeak        0.1741009  0.14388176  1.210028 2.357167e-01\nread        11.8220180  2.27313104  5.200764 1.321418e-05\nedutv       -0.9142163  0.88209867 -1.036411 3.082929e-01\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nHigh multicollinearity is all cleared up now so the focus can be solely on p-values. Now we'll drop `edutv` for having the highest p-value.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_4<-lm(score~fatheriq+motheriq+speak+read, data=gifted)\nsummary(mod_4)$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Estimate  Std. Error  t value     Pr(>|t|)\n(Intercept) 42.7092528 18.31549501 2.331865 2.637951e-02\nfatheriq     0.3242503  0.13032091 2.488091 1.842563e-02\nmotheriq     0.4207323  0.06987185 6.021485 1.154340e-06\nspeak        0.1898180  0.14325136 1.325070 1.948325e-01\nread        12.2089372  2.24494494 5.438413 6.108775e-06\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nNext to drop is speak.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_5<-lm(score~fatheriq+motheriq+read, data=gifted)\nsummary(mod_5)$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Estimate  Std. Error  t value     Pr(>|t|)\n(Intercept) 44.3655659 18.48732491 2.399783 2.239759e-02\nfatheriq     0.3214775  0.13183396 2.438503 2.047870e-02\nmotheriq     0.4282474  0.07045893 6.077973 8.662970e-07\nread        12.7663191  2.23107427 5.722050 2.430603e-06\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nAnd now we'll stop because all remaining terms have a p-value smaller than our $\\alpha=0.05$ threshold. The final model of `score~fatheriq+motheriq+read` matches the one obtained using AIC and forward selection, but that need not be the case. In fact, let's look at what happens if we were to perform our backward elimination without multicollinearity checks along the way.\n\nOur first step would have us remove `count` because it's p-value of 0.4446 is the highest in `mod_full`. So step two would now be:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_2<-lm(score~fatheriq+motheriq+speak+read+edutv+cartoons, data=gifted)\nsummary(mod_2)$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) 75.7756404 23.85794658  3.176117 3.526914e-03\nfatheriq     0.2430752  0.13607806  1.786292 8.451231e-02\nmotheriq     0.4082659  0.07163665  5.699121 3.646928e-06\nspeak        0.1511945  0.13901910  1.087581 2.857370e-01\nread        11.5226177  2.19353467  5.252991 1.255677e-05\nedutv       -4.5968140  2.17157275 -2.116813 4.297596e-02\ncartoons    -3.6286687  1.96951508 -1.842417 7.566028e-02\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nThen `speak` is removed...\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_3<-lm(score~fatheriq+motheriq+read+edutv+cartoons, data=gifted)\nsummary(mod_3)$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) 79.0467761 23.73960954  3.329742 2.312701e-03\nfatheriq     0.2373610  0.13639031  1.740307 9.205533e-02\nmotheriq     0.4121118  0.07176701  5.742357 2.870557e-06\nread        11.9013189  2.17231039  5.478646 6.030978e-06\nedutv       -4.8881415  2.16154516 -2.261411 3.114315e-02\ncartoons    -3.8202332  1.96759143 -1.941578 6.163171e-02\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nThen `fatheriq` is removed...\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_4<-lm(score~motheriq+read+edutv+cartoons, data=gifted)\nsummary(mod_4)$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) 112.5034019 14.37637560  7.825575 7.855003e-09\nmotheriq      0.4177214  0.07400329  5.644633 3.383735e-06\nread         11.6036137  2.23529988  5.191077 1.241780e-05\nedutv        -6.0534053  2.12140697 -2.853486 7.638249e-03\ncartoons     -5.1125813  1.88075148 -2.718372 1.064514e-02\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nAnd now since all p-values are below our 0.05 threshold we would stop with the `score~motheriq+read+edutv+cartoons` model. Checking at this stopping point for multicollinearity would make us then question having both `edutv` and `cartoons` in the model.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvif(mod_4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nmotheriq     read    edutv cartoons \n1.143219 1.044483 6.938065 6.878850 \n```\n\n\n:::\n\n```{.r .cell-code}\nmod_5<-lm(score~motheriq+read+edutv, data=gifted)\nsummary(mod_5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = score ~ motheriq + read + edutv, data = gifted)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6035 -1.7634 -0.0054  1.4456  6.7931 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 87.74186   12.18228   7.202 3.52e-08 ***\nmotheriq     0.40044    0.08076   4.959 2.24e-05 ***\nread        11.99889    2.44313   4.911 2.57e-05 ***\nedutv       -0.79303    0.95214  -0.833    0.411    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.917 on 32 degrees of freedom\nMultiple R-squared:  0.637,\tAdjusted R-squared:  0.603 \nF-statistic: 18.72 on 3 and 32 DF,  p-value: 3.405e-07\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nRemoving cartoons as having the higher of the two p-values leaves us with `score~motheriq+read+edutv`, a model that's a bit different from the one found through forward selection with AIC and backward elimination with multicollinearity considered up front.\n\n### Step-wise\n\nLastly we'll use the same step function used in our forward selection example to perform step-wise model selection using BIC. The scope of models to be considered is\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# we begin with the mod_5 at the end of the backward elimination example.\n# scope ranges from intercept-only to the full model as before\n# specifying k=ln(n) indicates using BIC to make decisions\nstepwise<-step(mod_5, scope=list(lower=mod_start, upper=mod_full), \n     direction=\"both\", k=log(nrow(gifted)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStart:  AIC=87.18\nscore ~ motheriq + read + edutv\n\n           Df Sum of Sq    RSS     AIC\n+ cartoons  1    52.426 219.93  83.070\n+ fatheriq  1    47.491 224.87  83.869\n- edutv     1     5.904 278.26  84.372\n<none>                  272.36  87.184\n+ speak     1    10.237 262.12  89.388\n+ count     1     2.686 269.67  90.410\n- read      1   205.296 477.65 103.824\n- motheriq  1   209.278 481.64 104.123\n\nStep:  AIC=83.07\nscore ~ motheriq + read + edutv + cartoons\n\n           Df Sum of Sq    RSS     AIC\n<none>                  219.93  83.070\n+ fatheriq  1    20.167 199.77  83.192\n+ speak     1     6.877 213.06  85.510\n+ count     1     0.475 219.46  86.576\n- cartoons  1    52.426 272.36  87.184\n- edutv     1    57.767 277.70  87.883\n- read      1   191.180 411.11 102.007\n- motheriq  1   226.047 445.98 104.937\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nThe output looks much like the output created in our forward selection case with one major difference: at each step the table of possible decisions includes both + terms and - terms. In step 1 the function considers adding `cartoons`, `fatheriq`, `speak`, or `count`, and also taking away `edutv`, `read`, or `motheriq`. Though labeled \"AIC\" the last column is actually a calculated BIC because we specified `k=log(n)`. In R's `step` function, the value of k specifies the multiplier to be put with the (p+2) measure of model size:\n\n$$\nstep's \\:AIC=k(p+2)-2ln(\\hat L)\n$$\n\nThis means *k=2* indicates you want traditional AIC, and *k=ln(n)* indicates BIC. Remember `log(n)` in R is using natural log.\n\n## Best model of size p\n\nIn the `leaps` library you can find a helpful function called `regsubsets`. The `regsubsets` function allows you to have R search every possible model with p predictors and tell you which is best.\n\nHere is an example with our gifted data where regsubsets is asked to consider models ranging from having one predictor (in addition to an intercept) up to four predictors. As inputs we provide the form of the full model, the data set, `nbest` tells R how many \"best\" models we want of each size, and `nvmax` tells R how large a model we want to work up to.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_mods<-regsubsets(score~., data=gifted, nbest=1, nvmax=4)\nsummary(best_mods)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSubset selection object\nCall: regsubsets.formula(score ~ ., data = gifted, nbest = 1, nvmax = 4)\n7 Variables  (and intercept)\n         Forced in Forced out\nfatheriq     FALSE      FALSE\nmotheriq     FALSE      FALSE\nspeak        FALSE      FALSE\ncount        FALSE      FALSE\nread         FALSE      FALSE\nedutv        FALSE      FALSE\ncartoons     FALSE      FALSE\n1 subsets of each size up to 4\nSelection Algorithm: exhaustive\n         fatheriq motheriq speak count read edutv cartoons\n1  ( 1 ) \" \"      \"*\"      \" \"   \" \"   \" \"  \" \"   \" \"     \n2  ( 1 ) \" \"      \"*\"      \" \"   \" \"   \"*\"  \" \"   \" \"     \n3  ( 1 ) \"*\"      \"*\"      \" \"   \" \"   \"*\"  \" \"   \" \"     \n4  ( 1 ) \"*\"      \"*\"      \"*\"   \"*\"   \" \"  \" \"   \" \"     \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nFrom the top section of this output you can see there are \"forced in\" and \"forced out\" options we did not apply to any of the potential independent variables. The lower section then gives the results. The row labeled \"1\" has an asterisk under `motheriq` indicating the best one-predictor model uses `motheriq`. The row labeled \"2\" has asterisks for `motheriq` and `read` telling us the best two-predictor model includes those terms. The best three-predictor model is shown to be `score~fatheriq+motheriq+read`, and the best four–predictor model found was `score~fatheriq+motheriq+speak+count`.\n\nNotice the best three-predictor model is not simply a subset of the best four-predictor model. While our forward selection, backward elimination, and stepwise models change only one term at a time making each successive model have only one variable different, the work done by `regsubsets` is exhaustive and doesn't require one-at-a-time changes.\n\n## Lasso Regression\n\nRecall from way back in Chapter 2 that our best linear fit solution comes from wanting to set the $\\hat\\beta_i$ values in such a way that $\\sum (y_i-\\hat{y_i})^2$ is minimized. The lasso technique modifies this goal so that instead we minimize:\n\n$$\n\\sum_{i=1}^{n} (y_i-\\hat y_i)^2 + \\lambda \\sum_{j=1}^{p}|{\\beta_j}|\n$$ {#eq-lasso}\n\nWith this change, we are making the decision that minimizing the sum of squared errors is important, but we also want to minimize the sum of the absolute value of our $\\beta$ parameters weighted by a $\\lambda$ tuning parameter. The value in doing this, is that now we can start with all possible independent variables and see which get eliminated through a $\\hat\\beta=0$ fit.\n\n### In R {.unnumbered}\n\nWe'll apply this approach in R to our gifted data using the `glmnet` function in the `glmnet` library.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# first need our data in format of x and y matrices\nmod_x<-as.matrix(gifted[,-1])\nmod_y<-as.matrix(gifted[,1])\n\n# use cross validation to explore different values of lambda\nset.seed(1234)\nstep1<-cv.glmnet(mod_x, mod_y, alpha=1)\nplot(step1$glmnet.fit)\n```\n\n::: {.cell-output-display}\n![](Chapter8_files/figure-pdf/unnamed-chunk-19-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\nThis first plot looks at the values of the $\\beta$ coefficients over a wide range of $\\lambda$ values. Each $\\beta$ has one colored line associated with it. The different considered$\\lambda$s are along the x-axis, but in the form of $-log(\\lambda)$, and the y-axis indicates the value of the $\\beta$s for each corresponding $\\lambda$. Along the top of the plot the number of non-zero $\\beta$ coefficients is shown.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(step1)\n```\n\n::: {.cell-output-display}\n![](Chapter8_files/figure-pdf/unnamed-chunk-20-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\nThis plot shows the mean-squared error for our model under various values of $\\lambda$. Vertical reference lines are drawn at the minimum mean-squared error, and at the minimum plus one standard error. The $\\lambda$s that correspond to each of these key values are retrievable through `step1$lambda.min` and `step1$lambda.1se`.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstep1$lambda.min\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2115333\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nThen we can fit the model using the identified best $\\lambda$ and see which coefficients are non-zero:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstep2<-glmnet(mod_x, mod_y, alpha=1, lambda=step1$lambda.min)\nlasso.coef<-predict(step2, type=\"coefficients\")\nlasso.coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n8 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s0\n(Intercept) 63.4282931\nfatheriq     0.2681240\nmotheriq     0.3592196\nspeak        0.1696803\ncount        0.2590080\nread         5.8841220\nedutv       -0.5542266\ncartoons     .        \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nIf not satisfied with only eliminating cartoons, we could change to a larger, non-optimal lambda:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstep2<-glmnet(mod_x, mod_y, alpha=1, lambda=1.5)\nlasso.coef<-predict(step2, type=\"coefficients\")\nlasso.coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n8 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s0\n(Intercept) 130.0439277\nfatheriq      .        \nmotheriq      0.1734218\nspeak         .        \ncount         0.1340442\nread          2.1009536\nedutv         .        \ncartoons      .        \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nAnd here we see `motheriq`, `count`, and `read` are the three terms selected as having a non-zero coefficient with $\\lambda=1.5$. To use lasso strictly as a model selection tool, from here you would then return to the lm function to fit a model with these three terms are base any predictions on the $\\hat\\beta$s that result from simply fitting the least-squares criterion, not that seen in @eq-lasso.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_result<-lm(score~motheriq+count+read, data=gifted)\nsummary(mod_result)$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept) 85.3970316 11.39507108 7.4942079 1.565612e-08\nmotheriq     0.4157893  0.07710859 5.3922568 6.345091e-06\ncount        0.1820829  0.28048936 0.6491617 5.208663e-01\nread         8.9042595  5.88322934 1.5134986 1.399682e-01\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n## Principal Components Analysis (PCA)\n\nNow we're going to consider a much larger data set: the `bdims` data frame in the `openintro` library contains 25 measures on 507 physically active adults. In addition to gender, age, weight, and height, we have access to hip girth, bicep girth, knee diameter, and all kinds of body measurements. It's great information, but as you might suspect these measurements are often highly correlated with each other. Larger hip girths tend to go with larger waists, small ankles make you more likely to have smaller wrists, and so on.\n\nPrincipal Components Analysis is a way we can incorporate much of the valuable information offered by these 21 body measurements, without making our model have 21 separate slopes and a host of multicollinearity issues. How?\n\nLet's start smaller and focus on two of our body measurements: `hip_gi`, the hip girth as measured at the same level as `bit_di`, the bitrochanteric diameter. Bitrochanteric diameter is basically hip width at the top of the femur but below the pelvis. Obviously these two measurements should be correlated and they are. Limiting to only the 247 men in the `bdims` data frame, the two hip measurements are plotted here in @fig-hips.\n\n\n\n\n\n\n\n\n::: {#fig-hips .cell}\n::: {.cell-output-display}\n![Original data](Chapter8_files/figure-pdf/fig-hips-1.pdf){#fig-hips}\n:::\n\nMale Hip Measurements\n:::\n\n\n\n\n\n\n\n\nNow look at the plot with the regression line overlaid and think about rotating the entire scatter plot to right a bit so that regression line was the new x-axis. Go ahead, pick up your book or screen (or turn your head) and rotate the graph so the blue line is our new horizontal. If you knew where along that new x-axis a person lied, you'd know a lot about their hip size. You'd have a combination of hip girth and hip diameter information with just that one blue-axis variable. The vertical variation up and down from that line is the relatively small amount of information you're giving up by only using one variable instead of the original two.\n\nWe could repeat this process over and over with pairs of body measurements we know are related, at each point picking the one-dimensional combination of them and effectively cutting our model size in half while giving up relatively little. But 1) that's time consuming and 2) what if we don't have the proper knowledge to know which terms to pair?\n\nStop and think about how that blue line does such a good job simplifying the two dimensions into one main direction of variability and then one minor direction of variability. What if we could take our full 21 dimensions of body measurements and get the computer to tell us the axes that align with the highest variability and the least variability? That is what we can do with ***principal components analysis***, often referred to simply as ***PCA***.\n\nThe linear algebra that's driving this change in basis is beyond our scope here, but instead we will focus on how to apply the method in R.\n\n### In R {.unnumbered}\n\nFirst, the data must be numeric in nature. For our body measurements, we'll look only at women, and only at the first 21 columns which cover `bia_di` (biacromial diameter) through `wri_gi` (wrist girth). There are a few different functions in R that will perform PCA, but we'll use the `prcomp` option that comes in the `stats` library.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwomens_bdims<-bdims[bdims$sex==0,1:21]\n\n# or if you prefer:\n\nwomens_bdims<-bdims |>\n  filter(sex==0) |>\n  select(bia_di:wri_gi)\n\npca_body<-prcomp(womens_bdims, scale.=TRUE)\n```\n:::\n\n\n\n\n\n\n\n\nNote that when `prcomp` is called, the `scale.=TRUE` is added in. What this does is tell R that before finding the axes of highest variability, the data should be scaled. This keeps us from labeling the dimensions that just happen to have bigger numbers from dominating the findings. If all body measurements were taken in cm but we transformed knee diameter into mm, we'd think a difference of 100 in the knee measurement was huge and a difference of 10cm in a waist was tiny. The `scale.=TRUE` takes each column and converts it to z-scores using the column means and standard deviations. This was a 3-standard deviation high measure on calf girth gets the exact same attention as a 3-standard deviation high measure on ankle girth. Always scale in PCA.\n\n@fig-pcares1 shows the first four principal components. The `prcomp` function will create as many principal components (new axes) as there are original dimensions to your data. They are ordered based on the amount of variability they can explain. Notice that every plot in the scatter plot matrix looks like just a random blob. That will always happen as these new axes will be uncorrelated with each other by design.\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![PCA Results](Chapter8_files/figure-pdf/fig-pcares1-1.pdf){#fig-pcares1}\n:::\n:::\n\n\n\n\n\n\n\n\nA `biplot` can be used to get a sense of how the original columns relate to the new principal components. Two biplots showing PC1 vs. PC2 and PC3 vs. PC4 are below in @fig-pcares2.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\npar(mar=c(4,5,2,2))\nbiplot(pca_body)\nbiplot(pca_body, choices=c(3,4))\n```\n\n::: {.cell-output-display}\n![PCA Results](Chapter8_files/figure-pdf/fig-pcares2-1.pdf){#fig-pcares2 fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\nBoth of these `biplots` are a bit hard to read because there are 21 red vectors all on top of each other. The first gives some discernable information though. It shows that PC1 involves a negative multiple of every column, while PC2 looks to be about half positive and half negative. The numeric information for how the original columns map to the principal components is given in the `$rotation` element of our completed `prcomp` object.\n\nHere are the mappings for the first two principal components, rounded to five decimal places. They confirm the all-negative loadings for PC1 and the roughly even positive/negative split for PC2.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(pca_body$rotation[,1],5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  bia_di   bii_di   bit_di   che_de   che_di   elb_di   wri_di   kne_di \n-0.16001 -0.15629 -0.21022 -0.17278 -0.20168 -0.20772 -0.19393 -0.23672 \n  ank_di   sho_gi   che_gi   wai_gi   nav_gi   hip_gi   thi_gi   bic_gi \n-0.17357 -0.23440 -0.24326 -0.23461 -0.22822 -0.25381 -0.23532 -0.24366 \n  for_gi   kne_gi   cal_gi   ank_gi   wri_gi \n-0.25137 -0.24002 -0.22807 -0.20682 -0.22877 \n```\n\n\n:::\n\n```{.r .cell-code}\nround(pca_body$rotation[,2],5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  bia_di   bii_di   bit_di   che_de   che_di   elb_di   wri_di   kne_di \n 0.31718 -0.05449  0.06264 -0.32902  0.05383  0.29060  0.39074  0.13547 \n  ank_di   sho_gi   che_gi   wai_gi   nav_gi   hip_gi   thi_gi   bic_gi \n 0.31147 -0.03461 -0.18167 -0.32502 -0.29562 -0.18830 -0.20851 -0.19328 \n  for_gi   kne_gi   cal_gi   ank_gi   wri_gi \n 0.00609  0.03437  0.04863  0.16233  0.23538 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nInterested in how much of the total variability can be seen in each principal component? That information is seen in the `$sdev` element of the `prcomp` object and can be seen in a bar chart with the plot command:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert standard deviations to variance by squaring\nvariances<-pca_body$sdev^2\n\n# look at variance for first 8 components, rounded\nround(variances[1:8],4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12.3825  1.6555  1.2828  1.0451  0.8028  0.5475  0.4608  0.4358\n```\n\n\n:::\n\n```{.r .cell-code}\n# proportion of total variance for first 8 components, rounded\nround(variances[1:8]/sum(variances),4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5896 0.0788 0.0611 0.0498 0.0382 0.0261 0.0219 0.0208\n```\n\n\n:::\n\n```{.r .cell-code}\n# see plot of variance per component\nplot(pca_body)\n```\n\n::: {.cell-output-display}\n![Component Variance Plot](Chapter8_files/figure-pdf/fig-pcares3-1.pdf){#fig-pcares3 fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\n\nBut what does any of this have to do with regression? Well, now that we have our principal components we can use them as predictors in our model to maximize the information from our data set while minimizing the number of $\\beta$ parameters that need estimating.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create BMI outcome to estimate with the measurements\nbmi<-bdims$wgt/(bdims$hgt/100)^2\n\n#limit to only the women\nbmi_f<-bmi[bdims$sex==0]\n\n#fit model with first seven principal components\nmod_bmi<-lm(bmi_f~pca_body$x[,1:7])\n\n#look at model summary\nsummary(mod_bmi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = bmi_f ~ pca_body$x[, 1:7])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8904 -0.7937 -0.0327  0.7054  4.1316 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          22.27793    0.07581 293.853  < 2e-16 ***\npca_body$x[, 1:7]PC1 -0.77013    0.02159 -35.677  < 2e-16 ***\npca_body$x[, 1:7]PC2 -0.88505    0.05904 -14.992  < 2e-16 ***\npca_body$x[, 1:7]PC3  0.35202    0.06707   5.249 3.25e-07 ***\npca_body$x[, 1:7]PC4 -0.22522    0.07430  -3.031  0.00269 ** \npca_body$x[, 1:7]PC5 -0.05495    0.08478  -0.648  0.51751    \npca_body$x[, 1:7]PC6  0.24817    0.10266   2.417  0.01634 *  \npca_body$x[, 1:7]PC7 -0.11019    0.11190  -0.985  0.32573    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.222 on 252 degrees of freedom\nMultiple R-squared:  0.8595,\tAdjusted R-squared:  0.8556 \nF-statistic: 220.2 on 7 and 252 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\nThis model summary shows us that just because one component covers more variability than another, doesn't mean that component is more important when it comes to predicting our outcome. PC6 covers only 2.61% of the total variability compared to PC5's 3.82%, but when it comes to predicting a person's BMI PC6 is more helpful than PC5. Since PC1 and PC2 are so strongly related to BMI, it turns out a model with just those two predictors has an $R_{adj}^2=0.8337$.- not bad for a 3-$\\beta$ model on a complex problem.\n\nYou can run forward selection, backward elimination, or step-wise building algorithms on the principal components just as you would the raw data - but now with the advantage of not having multicollinearity to navigate.\n\nPCA isn't without it's downsides. Using this type of approach and model might be very powerful, but it isn't nearly as interpretable as a model built on the original data. Depending on the context of the work, interpretability might take a back seat to predictive power which makes PCA a strong option. Also, sometimes looking at the PCA loadings themselves can provide insight into the interdependencies and relationships within your data.\n\nBottom line: PCA is a powerful tool to have in your tool box, but it shouldn't be the first tool you turn to when fitting a model.\n",
    "supporting": [
      "Chapter8_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}